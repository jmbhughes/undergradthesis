%
% Thesis template conforming to Williams College rules.
% Thanks to Ben Wood '08 and other contributors.
%
\documentclass[twoside]{report}
\usepackage[top=1.0in, bottom=1in, left=1.5in, right=1in, includehead]{geometry}
\pagestyle{headings}
\usepackage{setspace}
%% Special math fonts and symbols
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{minted}
%% Rotate tables and figures
\usepackage{rotating}
%% Used for TODO items
\usepackage[table]{xcolor}
%% used for code listings.
\usepackage{float} 
%% Used to replace LaTeX's ugly emptyset with diameter, which looks nicer.
\usepackage{wasysym}
%% Nicely formatted algorithms.
\usepackage{algorithmicx}
\usepackage[chapter]{algorithm}
\usepackage{algpseudocode}
%% Nicely formatted listings.
\usepackage{listings}
%% More kinds of arrow with stuff

\usepackage{empheq}
\usepackage{multicol}
\usepackage{subfigure}
\usepackage{indentfirst}
%% 
%% Hughes additions
%%
%% set table width
\usepackage{array}
%% graphics path
\graphicspath{{figs/}}
%% hyperlinks
\definecolor{color1}{RGB}{0,0,90} % Color of the article title and sections
\definecolor{color2}{RGB}{0,20,20} % Color of the boxes behind the abstract and headings
\usepackage{hyperref} % Required for hyperlinks
\hypersetup{hidelinks,colorlinks,breaklinks=true,urlcolor=color2,citecolor=color1,linkcolor=color1,bookmarksopen=false,pdftitle={Solar Thematic Map Generation via Machine Learning},pdfauthor={J. Marcus Hughes}}
%% Used to set up better TODO items
% \usepackage[paperwidth=210mm,
%             paperheight=297mm,
%             left=50pt,
%             top=50pt,
%             textwidth=345pt,
%             marginparsep=25pt,
%             marginparwidth=124pt,
%             textheight=692pt,
%             footskip=50pt]
%            {geometry}
\usepackage{xargs}
\usepackage{listings}
\lstset{
   breaklines=true,
   basicstyle=\ttfamily}
\newcommand{\halpha}{H$\alpha$\,}
\newcommand{\todo}[1]{{\color{red}{\textbf{#1}}}}
\newcommand{\code}[1]{\mintinline{python}|#1|}

\newcolumntype{b}{>{\columncolor[HTML]{D0D1ED}} c}
\newcolumntype{r}{>{\columncolor[HTML]{CCCCCC}} c} 
\newcolumntype{n}{>{\columncolor[HTML]{FFFFFF}} c} 

%\usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}
%\newcommandx{\unsure}[2][1=]{\todo[linecolor=red,backgroundcolor=red!25,bordercolor=red,#1]{#2}}
%\newcommandx{\change}[2][1=]{\todo[linecolor=blue,backgroundcolor=blue!25,bordercolor=blue,#1]{#2}}
%\newcommandx{\info}[2][1=]{\todo[linecolor=OliveGreen,backgroundcolor=OliveGreen!25,bordercolor=OliveGreen,#1]{#2}}
%\newcommandx{\improvement}[2][1=]{\todo[linecolor=Plum,backgroundcolor=Plum!25,bordercolor=Plum,#1]{#2}}
%\newcommandx{\noshow}[2][1=]{\todo[disable,#1]{#2}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Thesis body %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Title page %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{titlepage}
$\;$
\vskip1.5in
\onehalfspacing
\begin{center}
{\LARGE
Solar Thematic Map Generation via Machine Learning
}
\large
\vskip.25in
by\\
J. Marcus Hughes\\
\vskip.125in
Professor Joonsuk Park, Advisor\\
\singlespacing
\vskip.5in
\small
A thesis submitted in partial fulfillment\\
 of the requirements for the\\
Degree of Bachelor of Arts with Honors\\
in Computer Science\\
\vskip.5in
Williams College\\
Williamstown, Massachusetts\\
\vskip.5in
\today
%%\vskip.5in
%%{\Huge \textbf{DRAFT}}
\end{center}
\end{titlepage}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\tableofcontents
\listoffigures
\listoftables
\onehalfspacing

\chapter*{Abstract}
The new Solar Ultraviolet Imager (SUVI) instruments aboard NOAA’s GOES-R series satellites collect continuous, high-quality imagery of the Sun in six wavelengths. SUVI imagers produce at least one image every 10 seconds, or 8,640 images per day, considerably more data than observers can digest in real time. Over the projected 20-year lifetime of the four GOES-R series spacecraft, SUVI will provide critical imagery for space weather forecasters and produce an extensive but unwieldy archive. In order to condense the database into a dynamic and searchable form I have developed solar thematic maps, maps of the Sun with key features, such as coronal holes, flares, bright regions, quiet corona, and filaments, identified. Thematic maps will be used in NOAA’s Space Weather Prediction Center to improve forecaster response time to solar events and generate several derivative products. Likewise, scientists use thematic maps to find observations of interest more easily and guide experiments. 

This thesis presents machine learning approaches to automatically generate thematic maps in real-tie. In order to train these classifiers, I created software to collect expert classifications of solar features based on SUVI images. Using this software, I compiled a database of expert classifications, which will be made available for future use. These are used to establish experimental limits on human performance. Here, I describe the software to collect expert training and the successes and limitations of the classifier. These results are promising and encourage future research into an ensemble classification approach and solar weather prediction with similar methods.

\chapter*{Acknowledgments}
I'd like to thank Dan Seaton and Jon Darnel for guiding me during this project and supplying data and annotations.  I'd like to thank Jay Pasachoff for his support in solar physics and discovery of this project. And, I'd like to thank Jon Park for helping me organize and embark on an independent thesis. I'd like to thank all the aforementioned individuals again for reading the thesis and providing feedback as well as the rest of the Williams College computer science faculty. 

\chapter{Introduction}
\section{Motivation}
From August 28th to September 5th, 1859, the night sky nearly all over the world blazed with auroral displays for hours. Sources reported, ``there was another display of the Aurora last night so brilliant that at about one o’clock ordinary print could be read by light'' (The New York Times, New York Herald, Washington Daily National Intelligencer, September 2, 3, 5, 1859) \cite{green:2006}. Normally, aurora, a visible consequence of material streaming (because of the substorms caused) in from the Sun and interacting Earth's magnetic field, are confined to polar regions. However, there were observations in New Orleans and even as far south as Honolulu, shown in Figure \ref{fig:carrington-spatial}  \cite{cliver:2004}. These displays were symptomatic of a massive solar event impacting Earth's magnetic fields. Consequently, telegraphs ceased to work, and human operators reported burns and other injuries as small fires started \cite{green:2006}. Events like this, while rare, are fairly periodic, with a 12\% probability of another within the decade \cite{riley:2012}. 

\begin{figure}[ht]
  \begin{center}
    \includegraphics[scale=1]{carrington-spatial.png}
    \caption{{\bf Spatial extent of Carrington Event}: As noted by Cliver and Svalgaard (2004) the Carrington event was observed to very low latitudes in the Americas. Closed circles represent overhead aurora; open circles represent visible aurora. The heavy curved line denotes the geomagnetic equator and the $\bigoplus$ symbol indicates the anti-Sun point. The lowest geomagnetic latitude at which the storm was observed was Honolulu (not shown).}
    \label{fig:carrington-spatial}
 \end{center}
\end{figure}

On March 13th, 1989, another geomagnetic storm, less than a third of the strength of the 1859 Carrington event, disrupted power in Canada and the United States, resulting in blackouts for the majority of Quebec for nine hours; the cost for repair was \$6 billion \cite{cervantes:2013a}. A much smaller event occurred in 2003, known now as the Halloween event \cite{muller:2014, viljanen:2014}. In preparation, satellites were placed into a stable stand-by mode. Ultimately, only one satellite was damaged with most satellites unscathed and exceeding their nominal lifespan. An extreme Carrington event was observed pointed away from Earth in 2012 \cite{eastwood:2017}. It is difficult to quantify the total cost; estimates of the damage a modern Carrington event would cause range from \$140 billion to \$3.4 trillion \cite{eastwood:2017}.

There are many much more frequent but less devastating solar events, discussed in Section \ref{sec:space-weather-events} that can cause damage to power grids, satellites, Earth communications, astronauts, and many other sensitive systems. While they cannot be prevented, advanced warning allows for preparation that can mitigate the damage. Coronal mass ejections, one danger resulting when the Sun spews large amounts of charged material sometimes towards Earth, have been recorded to reach speeds of up to 2000 miles per hour, reaching Earth within the day \cite{fastcme}. Within minutes to hours of a solar flare, the ionospheric disturbance can interrupt radio communications \cite{swap}. For proper safety protocols to be enacted, fast warning of an event is necessary.  This thesis explores solar feature classification through modern machine learning approaches, specifically the creation of a computer vision systems utilizing satellite imagery to identify solar activity in real time for quick response. These classification systems allow for real-time warning of space weather events.

\section{Contributions}

This thesis has several key contributions as outlined here:
\begin{itemize}
\item Prior to this work, there did not exist a curated database of human annotated solar images designed for solar machine learning representing multiple experts input. This thesis presents the first of its kind, comparing it to existing automated databases. In addition, this thesis analyzes the human labeling to understand agreement/disagreement between different annotators and consistency for each annotator. In order to create this database, the necessary labeling software was created and is described here
\item This thesis provides a suite of modern machine learning approaches to solar image classification with a random forest, Bayesian, and neural network implementation. These are compared to existing solar classification approaches as well as each other to characterize their strengths, weaknesses, and overall performance.
\item Ultimately, high-quality solar image classification opens up avenues of research for solar physics. Thus, a prototype solar feature database is compiled from the images, indexing the images and allowing solar physicists to easily find interesting events. Further, a research application estimating the fractal dimension of active regions and properties of coronal hole is presented.
\end{itemize}

\section{Organization}
Chapter \ref{ch:background} introduces the background information for solar physics. This includes a definition of the relevant solar structures to classify.  Then, Chapter \ref{ch:relatedwork} documents prior work in solar imge classification including both the unsupervised systems in Section \ref{sec:unsupervised} and superverised systems in Section \ref{sec:supervised}. The author's original contributions begin in Chapter \ref{ch:data}, after a description of the solar imagery, with an overview and analysis of the human annotated images. Chapter \ref{ch:classifiers} details the classifiers tested in this approach through experiments in Chapter \ref{ch:experiments}. Two applications of the solar classification are explored in Chapter \ref{ch:application}: the labeled solar database in Section \ref{sec:database} and fractal dimension estimastion for solar features in Section \ref{sec:fractal}. Finally, Chapter \ref{ch:conclusion} outlines the results of the entire project and potential future work. 

\chapter{Solar Physics Background} \label{ch:background}

As mentioned, space weather has dangerous and expensive consequences including harm to astronauts and satellites, destruction of power grids, and routine rerouting of intercontinental flights over the north pole. These different effects stem from a variety of solar events. These are outlined in this section with a description of the structure of the varying phenomena.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[scale=0.4]{sunparts}
    \caption{{\bf Parts of the Sun}: This image indicates many of the parts of the Sun. The top cutaway portion shows the internal structure of the Sun, not discussed in this thesis. The bottom portion shows many of the features discussed here which comprise the solar atmosphere. The solar corona is the uppermost portion of the Sun which is millions of degrees Celsius. From this and the cooler and lower photosphere and chromosphere, the space weather emanates. Several different features discussed in Section \ref{sec:solarstruct} are shown, for example prominences, flares, and coronal holes \cite{sunparts}.}
    \label{fig:sunparts}
 \end{center}
\end{figure}


\section{Space Weather Phenomenon} \label{sec:space-weather-events}
The Sun is responsible for many phenomena in daily life providing basic fundamentals such as heat and light to sustain life on Earth. In 1733, Jean Jacque d'Ortous de Mairan proposed an explanation for aurora: it is from solar gas entering Earth's atmosphere \cite[p. 51]{langbook}. Later in the 20th century, Birkeland argued that it is from Earth's magnetic field focusing electrons around Earth's poles \cite[p. 51]{langbook}. Today, the Sun's magnetic field is a recognized as the driver of many of its internal and visible properties: the cycle of sunspots and the coronal temperature. In 1958, Eugene Parker advanced a model for the Sun's impact on Earth via the solar wind, the pressure caused from outflowing solar material, with a theortical model where the wind propogates as a spiral into interplanetary space from the Sun's rotation and radial flow \cite[p. 62]{langbook}. This is now referred to as the Parker Spiral in Figure \ref{fig:parkerspiral}. 

\begin{figure}[ht]
  \begin{center}
    \includegraphics[scale=0.6]{parkerspiral}
    \caption{{\bf Parker Spiral}: The Parker Spiral explains how events on the Sun propogate outward. Events on one region of the Sun have a direct line along the Parker Spiral toward Earth while events on another portion will miss Earth \cite{spiralimage}}
    \label{fig:parkerspiral}
 \end{center}
\end{figure}

The solar wind continuously is blowing millions of tons of material away from the Sun as a stream of rarefied plasma. When it reaches Earth's orbit, it is traveling at roughly 600 km/s  with 10 million particles per cubic meter \cite[p. 67]{langbook}. As the Sun goes through an 11-year cycle, this stream of material shifts from stronger to weaker and back again. During the most energetic portion of the cycle, there are frequent sudden and rapid outpbursts called solar flares. These release $10^{32}$ ergs \cite{solarenergy}, the equivalent to million of 100-megaton bombs exploding at the same time over less than ten minutes \cite[p. 89]{langbook}. These solar flares hurl high energy particles along the Parker Spiral into interplanetary space at nearly the speed of light. The strongest of these flares can cause "planet-wide radio blackouts and longlasting radiation storms" \cite[p. 255]{langbook}. 

Flares are often accompanied by coronal mass ejections (CMEs), although CMEs and flares can occur independently of each other. These eruptions send roughly 10 billion tons of solar material into interplanetary space. These have been reported to reach speeds in excess of 2000 miles per hour, reaching Earth within the day \cite{fastcme}. Smaller outbursts called filament eruptions can also release material towards Earth \cite[p. 285]{langbook}. Beyond these eruptions, there is variable strength solar wind constantly baragging Earth. Wind from coronal holes, discussed shortly, is faster and can be more of a constant threat. 

\section{Solar Structures}\label{sec:solarstruct}
Figure \ref{fig:sunparts} showed some of the different parts of the Sun. In Table \ref{tab:solarfeatures} there is a listing of some of the more prominent solar features with a brief description of them. 

\begin{table}[ht!]
\centering
 \begin{tabular}{||c m{0.5\textwidth}||} 
 \hline
 Name & Description \\
   \hline\hline
   Active regions & Complexes of brighter and darker regions in UV observations caused by the solar magnetic field piercing through the solar atmosphere \\ \hline \\
   Filaments & A suspension of cool material high in the solar atmosphere by the magnetic field\\ \hline \\
   Prominences & A filament observed off the disk of the Sun \\ \hline \\
   Coronal holes & Regions that appear dark in EUV observations where the magnetic field is open with no clear reconnection back into the Sun which allow fast outflow of material \\ \hline \\
   Flares & A sudden brightening on the Sun \\ \hline \\
   Coronal mass ejection & An event when the Sun dispels mass and charged particles, sometimes towards Earth \\ \hline \\
   Sigmoids & S-shaped structures, a special subclass of filaments, on the surface of the Sun thought to be precursors to flares because of their instability from high shear  \\ \hline \\
   Quiet corona & Parts of the Sun devoid of strong magnetic fields \\ 
 \hline
 \end{tabular}
 \caption{Solar phenomena: This is a short description of some of the solar events related to space weather. }
 \label{tab:solarfeatures}
\end{table}

Active regions will often be called bright regions in this work. Bright regions is a broader term used in space weather forecasting while active regions is the more technical description. 

\chapter{Related work} \label{ch:relatedwork}
This chapter surveys some of the underlying work on solar image classification. 

\section{Synoptic Charts and Thematic Maps}
When classifying space weather phenomena, it is important to understand its context: where it is on the solar disk, when it developed, and its strength. The Sun has a consistent radial outflow of material at approximately 400 km/s except for regions of fast 700 km/s wind largely caused by coronal holes, as it rotates on its axis every 27.5 days. Unlike Earth's rotation, the Sun undergoes differential rotation with a slower 25 day rotational period at the equator. Differential rotation tangles magnetic field lines that can lead in sudden reconnection and release of energy. The rotation causes magentic field lines to propogate outward from the Sun in a spiral pattern called the ``Parker Spiral.'' As such, solar wind from coronal holes or other energetic releases on the western side of the Sun will reach Earth quicker than from the eastern side of the Sun. Thus, knowing the both the location of the event and the time of the event is very important when considering when the impacts will be felt on Earth. A detailed understanding of the strength of the event and exigent conditions is required to estimate the impact on Earth when it does arrive.

The necessary information can be summarized in a solar synoptic chart. Solar synoptic charts (also referred to as solar thematic maps) detail the solar activity at any given moment using a labeled image of the Sun. While not reponsible for synoptic maps' invention Song et al. (2015)  describe the necessary components of synoptic charts \cite{song:2015}. The synoptic chart must be produced in real-time so that space weather forecasters can read and respond as needed. It must be quantitative when describing observations and object boundaries so that the information can be used in other follow-up systems such as expert validation and database generation. Finally, it must be comprehensive, providing more than sufficient information and easy to examine images of the Sun at various important solar atmospheric heights and temperatures. Based upon a literature study, Song et al. (2015) argued that magnetogram and extreme ultraviolet (EUV) imagery are most valuable for general solar event classification \cite{song:2015}. They created a database of 1586 space weather papers and investigated which types of solar phenomena are correlated to different wavelengths in modern research. After analyzing these trends for active regions, coronal holes, filaments/prominences, flares, and coronal mass ejections, they found that for all the categories 87.4 \% used magnetograms and 59 \% used extreme ultraviolet images. Thus, these two types of data should be featured most prominently on any synoptic charts.

At the moment, reliable synoptic charts used in forecasting are predominantly human drawn. There are existing automatic classifiers, but they often only detail one type of feature. Space weather forecasters at NOAA's Space Weather Prediction Center (SWPC) still hand draw synoptic maps daily, outlining magnetic field lines, coronal holes, flares, filaments/prominences, and plages. Historical maps are \href{http://www.swpc.noaa.gov/products/solar-synoptic-map}{available in PDF format} back until 1972. In a future project, this could serve as an interesting source of labeled data, especially for the difficult task of finding magnetic neutral lines. Zheng et al. (2016) utilized similar synoptic drawings from Yunnan Observatory to extract text annotations about sunspots with a convolutional neural network \cite{zheng:2016}. Some observatories are moving toward automated feature classification. For example, when the person in charge of synoptic maps at the Meudon Observatory was set to retire, they implemented a filament classifier and tracker \cite{meudon:2007}. 

\begin{figure}[ht]
  \begin{center}
    \includegraphics[scale=0.25]{ssc_example-song-2015.png}
    \caption{{\bf Synoptic map example}: Song et al. (2015) propose the solar synoptic map to include a labeled composite image with different wavelength images accompanying to provide a full image of solar activity \cite{song:2015}. }
    \label{fig:sccexample}
 \end{center}
\end{figure}

\section{Data sources}
\subsection{Raw imagery to classify} 
For solar image classification many types and sources of images can be utilized. While brand new imagers like the Solar Ultraviolet Imager (SUVI) do not yet have public data repositories, Sunpy, a python package for solar physics, provides an interface for the Virtual Solar Observatory's large database of solar images \cite{sunpy}. (SUVI's data release is May 4th, 2018, and will subsequently be available through the VSO.) Unsupervised approaches to the problem use this tool and others like it to fetch \halpha, ultraviolet, x-ray, white light, magnetic, and other forms of solar images. They utilize from the Solar Dynamics Observatory \cite{sdoinstrument, lemen:2012}, the Solar and Heliospheric Observatory \cite{sohoinstrument, eit}, Solar Terrestrial Relations Observatory \cite{stereoinstrument, secchipaper}, Transition Region And Coronal Explorer \cite{traceinstrument}, the Global Oscillation Network Group\cite{gong1, gong2}, and many other sources. There is continuous imagery of the Sun at many wavelengths and heights; raw data is generally not a limitation. Instead, the limitation is incorporating raw imagery into a classifier or obtaining labeled data for a supervised trainer. 

\subsection{Labeled data for training} 
The supervised techniques discussed in Section \ref{sec:supervised} require labeled data to train their classifiers. A research group formerly at Montana State University and now at Georgia State University has collated large amounts of images with labeling from unsupervised classifiers. The original dataset in 2013 comprised over 15,000 images with 24,000 events observed in the first half of 2012 by the Solar Dynamics Observatory (SDO). Using the SDO unsupervised classifiers, small grid regions of the image were labeled active region, coronal hole, filament, flare, sigmoid, and sunspot. Each grid region is also statistically analyzed and assigned an entropy, mean, standard deviation, fractal dimension, skewness, kurtosis, uniformity, relative smoothness, contrast, and directionality measures \cite{schuh:2013}. This data was later incorporated into a database tool that allows the user to identify an example image and query the full dataset for similar events \cite{banda:2014}. This dataset and tool was later expanded to the full SDO observing database \cite{schuh:2016}. 

\section{Solar machine learning}
Solar machine learning can be divided into two categories: unsupervised and supervised. Unsupervised techniques do not required human input of labeled images but instead often run on rules; this approach is much more common in astronomy. Supervised techniques are often more flexible and outperform unsupervised techniques in other related fields \cite{anzanello:2014, yu:2013,guerra:2011, huang:2017}.

\subsection{Unsupervised solar segmentation} \label{sec:unsupervised}
Unsupervised solar segmentation can be broken into many approaches: edge-based algorithms, region-based algorithms, hybrid algorithms, and artificial intelligence approaches. The first three categories are more image-processing techniques solely while artificial intelligence approaches are more generic examples of clustering, support-vector machines, and other tools. These can be used in tandem with the pure image processing techniques. 

\subsubsection{Edge-based algorithms}
Edge-based techniques utilize discontinuities and identify different features utilizing boundaries. Curto, Blanca, \& Martinez (2008) employed edge-based unsupervised detection when identifying sunpots in \halpha images \cite{curto2008automatic}. Since sunpots have crisp boundaries their edges can be used to quickly identify them. Curto, Blanca, \& Martinez (2008) used morphological operations to emphasize these boundaries: erosion, dilation, opening, closing, and the top hat transformation. Erosion shrinks bright regions by removing boundary pixels while dilation grows them. Dilation will also fill holes in features. Opening is an erosion followed by a dilation while closing is dilation followed by erosion. Both closing and opening smooth the image: opening fills shape holes, whereas closing breaks wide lines and erases thin lines \cite{curto2008automatic}. The top hat transformation subtracts the original image by the closing of the image. It results in an image showing only the erased parts. By stringing together an empirically determined set of operations, they were able to reliably identify sunpots. Qu et al.(2005) ave a similar system that identifies filaments \cite{qu2005automatic}. 

\subsubsection{Region-based methods}
Region based approaches include histogram segmentations, clustering/thresholding, and region-growing approaches. Fuller, Aboudarham, and Bentley (2005) implemented a filament classifier using region growing \cite{fuller:2005}. This work is based off more generic computer vision region growing by Gonzalez \& Woods (2002) \cite{gonzalez:2002}. After calibrating, removing dust, and sharpening the solar \halpha images \cite{standardization}, seed pixels are chosen for region growing using a thresholding technique. Only the dimmest pixels are chosen since they should be at the center of filaments. For multiple iterations, the region grows adding new pixels that are connected to seeds and follow the mean and standard deviation of the neighborhood and consequently are similar to the seed. Finally, a morphological closing operation is applied to remove any holes and make the filaments smoother. After this, the center line of the filament can be determined and characterized using a combination of convolutions, dilations, and erosions. This characterization makes it easier to track filament evolution and measure their length. Ultimately, this technique produced 1149 filaments compared to a human labeled 1232 filaments \cite{fuller:2005}. This resulted in missing 10\% of the filaments in an image. Roughly 5\% of the detections were false positives, keying on sunspots instead of filaments because there was no spatial requirement for filaments to be long and skinny. Thus, they could be confused for sunspots which are also dark in \halpha images. Other region growing methods include: Benkhalil et al.(2006) which used ionized calcium, \halpha, and extreme ultraviolet imagery to grow active region boundaries \cite{benkhalil}, Higgins et al. (2011) which combined magnetograms, image differencing, and region growing to identify and track emerging active regions \cite{higgins}, and McAteer et al. (2005) which used full-disk magnetograms to identify magnetically significant regions and characterize their flare potential \cite{mcateer}. 

Instead of growing regions, one can identify significant features by looking at the histogram of their intensities in various wavelengths. Olmedo et al. (2008) designed such a system to identify coronal mass ejections. The intensity in solar images can be plotted as a histogram as a function of position angle as shown in Figure \ref{fig:olmedo}. A threshold is used to determine what is a significant event in the histogram. If portions of the histogram exceed this, they are declared a region in the image and grouped together. Some region growing is also used in this approach. Ultimately, they were able to recover about 75\% of the human identified coronal mass ejections in a 12-month period. Interestingly, they found an equal number of small coronal events that had been overlooked by humans, often weaker but creating an interesting new population for scientific research and space weather awareness \cite{olmedo2008automatic}. 

\begin{figure}[ht]
  \begin{center}
    \includegraphics[scale=1]{olmedo.png}
    \caption{{\bf Histogram segmentation}: Olmedo et al.(2008) utilized histogram segmentation to identify coronal mass ejections. ``The intensity profile along the angular axis showing the 1D projection of the CME image. Only positive pixels along the radial axis are used. This profile effectively indicates the angular positions of a CME when it is present.'' \cite{olmedo2008automatic}}
    \label{fig:olmedo}
 \end{center}
\end{figure}

Other histogram based methods include Preminger et al.(2001) which used contrast ratios between ionized calcium and magnetograms to identify sunspots and faculae \cite{preminger}.

\subsubsection{Hybrid}\label{sec:hybridspoca}
Some approaches bridge between region-based and edge-based techinques. One key example is Spatial Possibilistic Classification Algorithm (SPoCA), a fuzzy classifier that uses multiwavelength extreme ultraviolet observations to assign multiple classes: quiet Sun, active region, and coronal hole. SPoCA is more accurately a suite that implements three types of fuzzy clustering algorithms tailored to the segmentation of solar coronal EUV images: Fuzzy C-means (FCM),  Possibilistic C-means (PCM) algorithm, and Spatial Possibilistic Clustering Algorithm (SPoCA) \cite{verbeeck2014spoca}. It differentiates itself from other techniques because of its reliance on fuzzy logic. Barra et al. (2008) note that the use of fuzzy logic allows the algorithm to overcome noise in the images and the scientific definitions of solar features while meeting many needs. Often, unsupervised solar segmentation techniques are developed with a specific research question and thus are very restricted. For example, one classifier may focus only on the brightest cores of active regions to study energy transfer while another may be designed to catch active regions as they form and thus catch many weak active regions. SPoCA is generic enough to overcome this conflict.

SPoCA works by utilizing the assumptions of fuzzy logic. Each pixel has membership in all of the different classes used in the thematic map. The sum of all the memberships for a pixel must be one, i.e. the pixel membership indicates how much the pixel matches that theme. In fuzzy c-means \cite{cmeans}, a generalization of K-means clustering to fuzzy logic, the variance within a cluster, all the pixels labeled a theme, is minimized. This approaches is very susceptible to the noise in astronomical images though \cite{krishnapuram1993possibilistic}. Thus, the fuzzy c-means requirement of membership summing to one is relaxed to form of possibilistic c-means \cite{krishnapuram1996possibilistic}. SPoCA is a further modification that incorporates weighting by spatial extent such that neighboring pixels should be assigned similar labels \cite{barra:2008, barra:2009}. 

Barra et al. (2009) used SPoCA to both segment and track features on the Sun \cite{barra:2009}. They exercised the power of SPoCA to perform two very different experiments. First, they tracked the biggest active region for a month, quantifying its size, average intensity, fractal dimension, and other parameters for scientific inquiry. Then, they identified and tracked coronal bright points, a feature not initially intended in SPoCA. These short lived ($< 2$ days) bright regions have some spectral similarity to active regions but are much smaller and can appear within coronal holes. They impact the structure and dynamics of the solar corona. During the study, Barra et al. (2009) tracked their counts, north/south asymmetry, intensity fluctuations, and other parameters \cite{barra:2009}.  

There are other types of unsupervised classifiers that do not fit nicely into the two main categories. For example, Bratsolis \& Sigelle (1998) utilized mean field fast annealing to segment sunspots \cite{bratsolis1998solar}. The approach uses simulated annealing to minimize the classification into $q$ classes. Each pixel is assigned a label. This labeling has an energy described by mean field theory and the Potts interaction between pixels. Essentially, this approach attempts to find the most meaningful classification. It excels over histogram methods which often are not granular enough to separate regions of different activity in the sunspot.

\subsubsection{Example system: Solar Dynamics Observatory}
Many unsupervised approaches that only deal with one class at a time can be chained together to create all the necessary data for a thematic map. The Solar Dynamics Observatory (SDO) satellite mission produces 1.5 TB of imagery per day in multiple ultraviolet wavelengths, a magnetogram, and other data channels. To deal with this influx of data, teams of researchers developed classifiers that identified specific classes of features \cite{sdo}. All of the component parts can be seen in Figures \ref{fig:sdo1} and \ref{fig:sdo2}.

\begin{figure}[H]
  \begin{center}
    \includegraphics[scale=0.4]{sdo1.png}
    \caption{{\bf First half of SDO classifiers} \cite{sdo}}
    \label{fig:sdo1}
 \end{center}
\end{figure}

\begin{figure}[H]
  \begin{center}
    \includegraphics[scale=0.4]{sdo2.png}
    \caption{{\bf Second half of SDO classifiers} \cite{sdo}}
    \label{fig:sdo2}
 \end{center}
\end{figure}

The SDO suite suite is a comprehensive approach to classification that utilizes all of the techniques mentioned thus far and more. For features that are beyond the individual classifiers' scope, a trainable module is employed. While this is a supervised system, it is mentioned here to emphasize the difference between supervised and unsupervised approaches. A user can identify a specific type of feature they are interested in by identifing them within an image. These are placed in feature vectors of 12 texture paramters (for example mean, entropy, uniformity). These train either a support vector machine or a C4.5 decision tree and will then identify similar features from the rest of the database \cite{lamb2008example}. The SDO suite lacks the ability to combine all the classifications into a single thematic map. 

\subsubsection{Comparisons}

As shown in Figure \ref{fig:revathy}, different methods produce often similar but still different results. By computing the fractal dimension over the segmented image, they were able to characterize the difference between fuzzy clustering, region growing, iterative thresholding, and histogram thresholding. They found that depending on the height of the active region in the solar atmosphere it was segmented differently. In general, the fuzzy-based and histogram approaches outperformed the others. They propose that using longer wavelength ultraviolet images tends to larger area active regions.

\begin{figure}[H]
  \begin{center}
    \includegraphics[scale=1]{revathy.png}
    \caption{{\bf Performance of different methods}: Revathy, Lekshmi, \& Nayar (2005) compared the performance of different segmentation techniques in identifying active regions \cite{revathy}. At left is the result of a histogram thresholding approach while at the right is fuzzy-based segmentation. }
    \label{fig:revathy}
 \end{center}
\end{figure}

Caballero \& Aranda (2013) conducted an independent comparison of unsupervised techniques for active regions \cite{caballero}. Using 6000 images from SOHO in 195 angstroms, they initially segmented the images using region growing techinques. Then, the different independent regions were clustered together into units using either partition approaches or a hierarchial classification. They found that the hiearchial classification, the idea that nearby regions should be more strongly connected, produced more human-like clusters. However, this approach requires exponential time complexity and results in a hierarchy instead of a simple division into clusters.

For a much longer comparison of many different techniques see Aschwanden (2010) \cite{aschwanden:2010}. 

\subsection{Supervised solar segmentation} \label{sec:supervised}
There are relatively few supervised solar segmentation approaches but the existing approaches are very promising and tend to perform on a broader set of classes and image types. Thus, a bit more detail is provided in describing each approach. 

\subsubsection{Maximum likelihood estimation}\label{sec:riglermaxlikelihood}

Rigler et al. (2012) proposed a preliminary naive Bayesian classifier for SUVI images using the Solar Dyanmics Observatory (SDO) Advanced Imaging Array (AIA) observations for testing \cite{rigler:2012}. Their work focused on eight classes: flare, prominence, active region, quiet corona (off-disk), quiet corona (on-disk), coronal hole (off-disk), coronal hole (on-disk), and outer space. They were able to achieve an average accuracy of 86\%. This was calculated by training the classifier on a set of data then testing it on a classified image that was not used for training. The tabulation was done strictly on a pixel by pixel basis, not checking if the error was coherent or random noise. Prominence was the most problematic class with only 41\% of prominence pixels being classified correctly. They were often misclassified as off-disk quiet corona or on-disk coronal hole.

The naive Bayesian approach works by classifying every pixel into one of $n$ classes using multispectral ultraviolet images. Therefore, a pixel, a spatial element at $(i,j)$ corresponds to $h$ channels and can be described as a vector:
\[ x_{(i,j)} = \begin{bmatrix} x_1 & x_2 & \hdots & x_h \end{bmatrix}^T \]
The approach is assign each $x_{(i,j)}$ pixel a label $w_k$ from the set of classes $W$. This approach employs Bayes' Theorem:
\[ P(w_k | x_{(i,j)}) = \frac{P(x_{(i,j)} | w_k) P(w_k)}{P(x_{(i,j)})} \]
Since $P(x_{(i,j)})$ is not a function of the label classification, it can be ignored.
\[ P(w_j | x_{(i,j)}) \propto P(x_{(i,j)}) P(w_{k}) \].
Rigler et al. (2012) simplify this even further by stating that ``if there is no a priori reason to believe a pixel should be assigned label $w_k$, $P(w_k)$ can be assumed to be drawn from a uniform distribution''. Thus,
\[ P(w_k | x_{(i,j)}) \propto P(x_{(i,j)} | w_k) \]
This approach is the maximum likelihood solution to this problem.

For training they simplify each class into a multivariate normal, i.e. for each potential label there is a archetypal example pixel and all pixels with that label should be distributed normally about it. This is overly constraining if any given class has multiple distinct modes with respect to the selected data. The multivariate distribution for class $w_k$ is characterized by a mean vector $\mu_k$ and covariance matrix $C_k$ which are calculated as:
\[ \mu_k = \frac{\sum_{x \in W_k} x}{|W_k|} \]
where $W_k$ is the collection of pixels with label $w_k$. Similarly,
\[ C_k = \frac{\sum_{x \in W_k} \begin{bmatrix} x - \mu_k \end{bmatrix} \times \begin{bmatrix} x - \mu_k \end{bmatrix}^T}{|W_k|} \]
These mean vectors and covariance matrices characterize the class. Given this characterization for class $w_k$ one can calculate the conditional probability of a pixel $x_{(i,j)}$ having label $w_k$:
\[ P(x_{(i,j)} | w_k) = \frac{1}{\sqrt{(2 \pi)^h} \sqrt{|C_k|}} \mathrm{exp} \left( \frac{-1}{2} \times \left(x_{(i,j)} - \mu_k \right)^T \times C_k^{-1} \times \left(x_{(i,j)} - \mu_k \right) \right) \]
Thus, the pixel is assigned the class that maximizes this probability.

Since each pixel is treated separately, any noise in single pixels or in the image as a whole can result in a noisy classification where a pixel class does not agree with its neighboring pixels as expected. Therefore, Rigler et al. (2012) propose a smoothness prior be enforced so that a pixel's labeling relies on its neighbors This can be enforced by iteratively calculating the thematic map, calculating a smoothed map relying on neighbor probabilities, and repeating until convergence using simulated annealing, maximizing posterior marginals, or iterated conditional modes as proposed by Tso and Mather (2009) \cite{tso:2009}. 

Their results were promising with high accuracies and maps that generally coherent. However, their results are concerning because some statistics and accuracy measurements come from running the classifier on training data, providing no indication on how the classifier would perform on unfamiliar, real-world examples. 

Rigler et al.(2012) built upon earlier work by de Wit (2006) who suggested the Bayesian approach \cite{dewit:2006}. After decreasing the noise and normalizing the intensity in each image, de Wit (2006) instead used only four ultraviolet wavelengths and projected them into a three-dimensional parameter space using singular-value decomposition \cite{dewit:2006}. Thus, de Wit (2006) ran a naive Bayesian classifier on these transformed parameters instead of the higher dimensional wavelengths. Figure \ref{fig:dewit} shows an example result by de Wit (2006), emphasizing the coherence of this segmentation without any forced smoothness \cite{dewit:2006}. This approach ran in near real-time, taking only a few minutes to classify every pixel. 

\begin{figure}[H]
  \begin{center}
    \includegraphics[scale=1]{dewit.png}
    \caption{{\bf de Wit Segmentation}: This figure indicates the power of quick, multi-spectral, supervised segmentation done by de Wit (2006), very similar to other work by Rigler et al.(2012) and De Visscher et al.(2015) \cite{dewit:2006}. The classes for this study are: ``(1) Tenuous corona outside of the disk, in regions with open magnetic-field lines.(2) Dense corona outside of the disk. (3) Coronal holes. (4) Quiet sun, including the chromospheric network and regions inside the network boundaries. (5) Active regions on the disk'' \cite{dewit:2006}. }
    \label{fig:dewit}
 \end{center}
\end{figure}

\subsubsection{Maximum A Posteriori}
De Visscher et al. (2015) improve upon Rigler et al. (2012) by recognizing that the classes are not equally likely (for example the majority of the Sun is covered by quiet corona at any given time) and that crisp segmentation procedures are too limiting compared to fuzzy segmentation \cite{visscher:2015}. Further, they incorporate spatial information by letting the probability of a class rely on both the intensity and latitude, assuming these are statistically independent variables so $P((x, L(x)) | w_k) \approx P(x|w_k) P(L(x)|w_k)$ where $L(x)$ is the latitude of the pixel and $w_k$ is a class. This addresses the observable fact that coronal holes tend to form at solar poles, high latitudes, while active regions form near the equator. Unlike Rigler et al. (2009), De Visscher et al. (2015) use only one wavelength of light in their dataset: the 19.3 nm SDO-AIA channel. Additionally, they only assign three classes: active region, coronal hole, and quiet sun. This makes it difficult to directly compare their results because the troubling classes Rigler et al.(2009) observed are not included.  Therefore, the probability they are maximizing is instead: 
\[P(x|w_k)P(L(x) | w_k) P(w_k) \]
That is the product of the probability of pixel x given its class, the probability of the latitude given the class, and the probability of the class. While doing this, they have assumed a fuzzy segmentation that allows for degrees of membership in each class. This approach allows estimation of how certain different classes are under different conditions. For example, they confirmed that wrongly classified pixels were classified as half one class and half another class and often on the object boundaries. In adition, they were able to pinpoint that there is disagreement within the gold standard database. Upon second examination, they find that the human assigned labels may actually be wrong in this region, highlighting a key problem in solar segmentation: there is no clear universally agreed upon definition of some of the classes so no human classification can be accepted as completely correct. Even if a human standard does exist, it is sometimes still impossible to discern between classes due to degeneracy in the observational parameters. Ultimately, they report average 94\% accuracy over all classes.

In addition to accuracy with a single frame, De Visscher et al. (2015) establish a criteria for accurate segmentation into large-scale features \cite{visscher:2015}:
\begin{itemize}
\item ``Stable segmentations on short timescales in the absence of major solar activity''
\item Consistent and smooth trends and classifications over longer periods of time
\item Consistency with human drawn maps
\end{itemize}
While there is no quantified method for the first two criteria, they are highly relevant and often not addressed in other approaches explicitly. 

\subsubsection{Neural Networks}
Deep convolutional neural networks have proven to be very skilled in classifying and segmenting in various contexts \cite{szegedy:2015, tso:2009, krizhevsky:2012}. Convolutional neural networks are specifically designed for image data such as solar images. Each layer extracts local features from an image using a kernel which are combined in intermediate pooling layers. This allows for a robust classification with respect to distortions or noise in the images. Activation functions allow only significant features to influence the final classification. Deep learning approaches are advantageous because they automate feature selection by weighting input data according to their training algorithms instead of having a scientist develop detailed rules about what data components indicate which classes.

Kucuk et al.(2017) applied the first convolutional neural network to solar imagery for classification \cite{kucuk:2017}. While classifying over a finer granularity of classes they were able to achieve an average 70\% accuracy across each class. This convolutional neural network approach outperformed the only other published neural network solar segmentation found during this review. Zharkova \& Schetinin (2005) employed a feed-forward neural network with two hidden neurons and one output neuron to identify solar filaments at 82\% accuracy \cite{zharkova:2005}. This result is not directly comparable since it only classified one type of feature. However, it illustrated the power of neural networks in solar images. Filament classification by classical image processing techniques is often confused by the highly variable background between different parts of a filament and from filament to filament. The artificial neural network was able to flexibly learn many patterns and more accurately identify filaments. At the time, it was only outperformed by a region-growing approach \cite{fuller:2005}. 

Up until now, classification has only been discussed in a spatial domain. However, flares and coronal mass ejections have a temporal components. They are by definition changing features. Borda et al. (2002) implemented a simple neural network consisting of two layers (not including input): a hidden layer of nonlinear neurons and
an output layer of one linear neuron \cite{borda2002automatic}. Given optical \halpha images, it identifies solar flares in real-time. It operates on 7 input features: mean image brightness, standard deviation of the brightness, the pixel of maximum brightness change between images, absolute brightness of pixel with maximum change, radial position of that pixel, variation of mean brightness between two images (to characterize possible weather influences), and the contrast between the pixel with largest change in brightness and its neighbors. Given 124 test events, fewer than 5\% were misidentified. (The paper does clarify false positives versus false negatives.) Accounting for normal operations, this would be a misclassification every few days. There has been limited solar time series neural networks beyond this, but it establishes a baseline system for future architectures and generalizations to other feature types.

\section{Earth Remote Sensing}
Earth remote sensing of multiwavelength features has many more applications and a longer availability of data and thus has advanced further than solar machine learning techniques. 

I could not find an example of random forests for solar image classification. However, they are routinely used in Earth remote sensing classifications. Random forests are an ensemble of tree classifiers. To classify a new feature vector, the input vector is classified with each tree in the forest, and the forest chooses the classification having the most votes over all the trees in the forest. Random forests have many advantages: high accuracy compared to current algorithms, efficient implementation on large data
sets, and an easily storable data structure for future use \cite{ghose2010decision}. Lowe \& Kulkarni (2015) used a random forest to identify terrain type in hyperspectral images, nearly simultaneous images of Earth composed of tens to hundreds of narrow passpands for each pixel \cite{lowe:2015}. For this application, the random forest had 96.25 \% accuracy compared to neural network's 76.87\%, support vector machines 86.88\%, and maximum likelihood's 83.11\% \cite{lowe:2015}. This high performance for random forests with this type of problem is not uncommon \cite{puissant2014object, salas2016multispectral, clark2016mapping, kulkarni2017multispectral}.

Similarly, neural networks have a rich tradition in Earth remote sensing. Lee \& Kwon (2017) developed a 9 layer convolutional neural network, both wider and deeper than state-of-the-art methods for this problem, to classify land types in Earth remote sensing \cite{lee2017going}. This network achieves over 95\% accuracy in nearly every class. It is specially designed for spectral-spatial data and explores neighborhood relationships in a more optimized fashion that previous networks by allowing for multi-scale examination.

Li et al. (2014) present a comprehensive review of Earth remote sensing classification techinques based on spatial techniques \cite{li2014review}. They detail the usage of K-means, ISODATA, SOM,
hierarchical clustering, Maximum likelihood, Minimum distance-to-means, Mahalanobis distance, Parallelepiped, k-nearest Neighbors,  artificial neural network, classification tree, random forests, support vector
machine, genetic algorithms, Fuzzy classification, neural networks, regression modeling, regression tree analysis, spectral mixture analysis, fuzzy-spectral mixture analysis,  and image segmentation and object-based image
analysis techniques in Earth remote sensing. 

\section{General Computer Vision}
Given sufficiently large datasets, cutting edge computer vision research can be applied to the solar segmentation problem. The solar segmentation problem and producing thematic maps is an example of semantic segmentation, a well studied problem in computer vision. However, it is difficult to apply these techniques to solar image segmentation because they require large labeled datasets that are currently non-existent in solar imaging. 

\subsection{Fully convolutional neural networks}
The notion of extending convolutional neural networks to do dense prediction, effectively creating a thematic map, was first proposed by Matan et al. (1991) \cite{matan} to extend the LeNet convolutional neural network \cite{lenet} for handwritten digit recognition. Shelhamer, Long, \& Darrell (2016) presented a new implementation, the fully convolutional neural network (FCN), that takes arbitrarily sized input and creates a similarly sized semantic segmentation \cite{fcnn}. During their development, they thoroughly describe convolutional neural network for semantic segmentation up until 2016 \cite{fcnn}.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[scale=1]{fcnn.png}
    \caption{{\bf Fully convolutional neural network}: This is an example architecture for a fully convolutional neural network (figure 1 from Shelhamer, Long, \& Darrell (2016) \cite{fcnn}).  }
    \label{fig:fcnn}
 \end{center}
\end{figure}

A FCN works by using existing full neural networks in a convolutional fashion. Then, the last steps of the existing network are removed so that it cannot make a classification for the entire input image at that time. Instead, a pixelwise prediction layer is added using deconvolution and striding over the input image. Without specialized refinement, the FCN can then create a dense output map. At the time, the FCN gave 20\% improvement over state-of-the-art semantic segmentation in a shorter inference time.

\subsection{Mask R-CNN}
He, Gkioxari, Doll{\'{a}}r, \& Girschick developed Mask Regional Convolutional Neural Network (Mask R-CNN), an extension of Faster R-CNN \cite{ren:2017} that creates another form of a semantic segmentation \cite{he:2017}. Technically, Mask R-CNN is a type of FCN \cite{fcnn}. Faster R-CNN worked by quckly determining bounding boxes for various objects in a scene. Mask R-CNN extends this by determining a pixel mask for each bounding box in parallel. Thus, it can distinguish both the type of an object and between neighboring objects in a scene. 

\begin{figure}[ht]
  \begin{center}
    \includegraphics[scale=0.5]{mrcnn.png}
    \caption{{\bf Mask regional convolutional neural networks}: The top row was the existing state-of-the-art instance segmentation \cite{li2016fully}, an example of a FCN, compared to the bottom row of Mask R-CNN performance on the same scene. The overlaid coloration indicates the segmentation while bounding boxes indicate where Mask R-CNN evaluated these masks. Clearlly, the Mask R-CNN produces more coherence classifications. In addition, it runs in less time with higher accuracy than the existing state-of-the-art systems and was consequently awarded the 2017 International Conference on Computer Vision best paper award. Figure from He et al. (2017) \cite{he:2017}.}
    \label{fig:mrcnn}
 \end{center}
\end{figure}

The Mask R-CNN design was tested in several domains: cityscapes, human pose estimation, common objects. In all cases, it outperformed existing systems. With modification, this type of architecture solves the problem of solar segmentation. 

\section{Summary}
Solar image segmentation is well motivated by space weather and archival concerns. Up until now, there have been numerous unsupervised approaches, too many to completely document in this paper. These approaches employ a variety of computer vision techniques but are often limited to only determining membership in one class, for example determining which pixels are filament. They can be used in tandem to create solar thematic maps that label all classes on the Sun in an ensemble classifier. Settling disputes between the independent classifiers can be difficult and running them all in parallel can be costly. Instead, machine learning techniques can be applied to label the entire image at one time. A few supervised approaches have been developed and perform well. However, there is limited quantification and even more limited consistent quantification of that performance making it difficult to compare systems. By looking to Earth remote sensing and state-of-the-art computer vision machine learning approaches, solar image segmentation can be advanced in a systematic and measurable fashion. 

\chapter{Data} \label{ch:data}

This chapter first describes the raw solar data utilized in this work. It then details the processing done to use the images in the project. Finally, it describes the annotation process for the images and analyzes the results. 

\section{Solar Imagery}
Ultraviolet observations of the Sun are optimal for seeing both dynamic, high temperature solar eruptions and cooler, more stable coronal holes. There are a few ultraviolet solar imagers potentially suitable for this investigation: the Atmospheric Imaging Assembly (AIA) aboard the \textit{Solar Dynamics Observatory} (SDO) spacecraft \cite{lemen:2012}, the Extreme-ultraviolet Imaging telescope aboard the \textit{Solar and Heliospheric Observatory} (SOHO) spacecraft \cite{eit}, and the Sun Watcher with Active Pixels and Image Processing (SWAP) aboard the \textit{Project for On-Board Autonomy 2} spacecraft \cite{halain:2013, seaton:2013}. This project instead uses data from the new Solar Ultraviolet Imager (SUVI) aboard the \textit{ Geostationary Operational Environmental Satellite 16} (GOES-16) and GOES-17 operated by the United States National Oceanic and Atmospheric Administration (NOAA) \cite{suvibg, suvibg2}. SDO, SOHO, and SUVI share many of the same wavelength passbands, but SUVI has a wider field of view allowing for classification of events further off the Sun. SWAP has the same field of view as SUVI but only one passband that is very similar to SUVI's 17.1 nm band. SUVI has a slightly higher spatial resolution than SWAP. In addition, GOES-16 is the first four commissioned satellites in the R series, all of which will have SUVI instruments. Thus, the classification system and database built here will be used for at least 20 years. The classification techniques applied here could be applied, with modification, to any of the aforementioned satellites, including to archived data. 

\subsection{Solar Ultraviolet Imager (SUVI) Data}
SUVI observes in six different wavelength passbands (and their corresponding coronal EUV emission line) using filters and multilayer mirrors sensitive to specfic wavelengths: 9.4 nm (Fe \textsc{XVIII}), 13.1 nm (Fe \textsc{XXI}), 17.1 nm (Fe \textsc{IX/X}), 19.5 nm, (Fe \textsc{XII}), 28.4 nm (Fe \textsc{XV}), and 30.4 nm (He \textsc{II}) Each wavelength is most sensitive to a specific wavelength and solar feature as detailed in Figure \ref{fig:suviwavelength}. 

\begin{figure}[ht]
  \begin{center}
    \includegraphics[scale=0.4]{suvi-wavelength-log.jpg}
    \caption{{\bf Wavelengths}: This diagram indicates which SUVI wavelengths are most helpful in identifying different space weather events. Figure from the SUVI website \cite{suviwebsite}.}
    \label{fig:suviwavelength}
 \end{center}
\end{figure}

However, these passbands overlap with other spectral lines that form at different temperatures than the predominant one listed above. This can cause potential confusion between channels during classification. The qualitatively apparent in the first-light images from SUVI in Figure \ref{fig:suviimages}.

\begin{figure}[hbt]
  \begin{center}
    \includegraphics[scale=0.8]{suvi-images.jpg}
    \caption{{\bf Example SUVI images}: These first-light imag-es taken on January 29, 2017 exemplify SUVI's response to different solar features in different passbands. For example, the large coronal hole (the darker boot-like shape in the lower portion of the Sun) has higher contrast from the background in 19.5 nm than 304 nm. Figure from the SUVI website \cite{suviwebsite}.}
    \label{fig:suviimages}
 \end{center}
\end{figure}


Each image is composed of $1280\times1280$ 32-bit pixels. The individual pixels are $2.5\times2.5$ arcseconds on a side resulting in a $53.3 \times 53.3$ arcminute total field of view horizontally \cite{suvibg2}. The field of view is slightly larger along the diagonal but vignetting in some of the image corners in some passbands renders these regions unusable. Ultimately, SUVI can observe out to 1.6-2.3 solar radii (depending whether on the horizontal or diagonal). 

One image, in a single passband, is taken every 10 seconds. The observation sequence insures that every passband is observed at least once every four minutes. In addition, SUVI captures a short exposure time and a long exposure time image in each passband. During energetic and bright events like flares, the detector would saturate at the longer exposure time needed for high contrast images during times without energetic events. These two exposure times are combined to create high-dynamic-range composite images, which a two to three orders of magnitude increased dyanmic range than a single exposure time. In addition, SUVI has anti-blooming circuitry so that saturation in the long exposure image does not deteriorate quality in the composite image. This compositing will allow better classification of very bright active regions and flares than if another data set were used. Finally, the composite images are aligned so that the Sun is centered in each. 

\subsection{Noise-gater procedures}
The SUVI 94 angstroms channel is riddled with shot noise that degrades the signal quality. Shot noise is statistical noise caused by uncertainty in counting photons received by a camera sensor that follows the Poisson distribution and is proportional to the square root of the number of received photons. Since the 94 angstrom channel is so short and consequently high energy, there are many fewer photons emitted by the Sun at this wavelength resulting in a larger shot noise problem in this channel than others. The shot noise buries the signal and could potentially make it more difficult for the machine learning classifiers to cleanly identify the solar classes. Deforest (2017) proposed a method using localized Fourier transforms to characterize and remove the noise from images with specific application to extreme ultraviolet images. The effectiveness of noise-gating is shown in Figure \ref{fig:noise-gate-example}.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[scale=0.8]{noise-gate.png}
    \caption{{\bf Effectiveness of noise-gating} The upper left is a good image, no cleaning necessary. However, images like the upper right, dominated by shot noise, are typical for the 94 angstrom channel. This image is created by taking the image on the upper left and adding Poisson noise with a signal-to-noise ratio of 2. The algorithm still performs even if it is worse, although some artifacts appear. DeForest's algorithm was applied to create the cleaned image on the bottom left. This can be compared to simply smoothing the image to decrease the noise as in the bottom right, a typical alternative procedure.}
    \label{fig:noise-gate-example}
 \end{center}
\end{figure}

The noise-gating algorithm was developed by \cite{deforest:2017} for use in extreme ultraviolet solar images, medical imaging, and other applications. The term noise-gating stems from the approach used in audio noise reduction and functions similarly: local adaptive filters in Fourier domain distinguish between coherent spatial structure and background noise. 

Beginning with an image sequence, described as a mapping from coordinates in the horizontal and vertical spatial dimensions and a time dimension, i.e. $\mathbb{Z}^3 \rightarrow \mathbb{R}$, the noise is modeled as:
\[Im(x,y,t) = Im_0(x,y,t) + N_a(x,y,t) + N_s(x,y,t) + N_{\text{other}}(x,y,t) \]
The observed image sequence $Im(x,y,t)$ is a combination of $Im_0$, the idealized, noise-free image; $N_a$, background noise that is independent of the signal of $Im_0$; $N_s$, shot noise that is dependent on the signal of $Im_0$; and $N_{\text{other}}$, all other noise sources. For this application, $N_a$ and $N_{\text{other}}$ are ignored because $N_s$ dominates them.  $N_s$ is then approximated with a normal distribution $G(x,y,t)$ and instrumental constant $\alpha$:
\[N_s(x,y,t) \approx \alpha G(x,y,t) \sqrt{\text{Im}_0(x,y,t)} \]
This is transformed into Fourier space as:
\[|N_s'(k_x, k_y, \omega)| \approx \beta(k_x, k_y, \omega) \sum_{x,y,t}\sqrt{\text{Im}(x,y,t)}\]
$\beta$ is spectrum that is characteristic of the imaging instrument and remains constant across images. The image is broken into many small neighborhoods $\text{Im}_i (x,y,t)$. Since the image is dominated by shot noise, which should be fairly uniform in Fourier space, the noise distribution is approximated by: \[\beta_{\text{approx}}(k_x,k_y,\omega) = \text{median}_i \left(\frac{|\text{Im}_i'(k_x,k_y,\omega)|}{\overline{\text{Im}_i}} \right)\] Then, the threshold to filter out noise is set for each image section independently by:
\[T_i(k_x, k_y, \omega) = 3 N_i'(k_x, k_y, \omega)\] with a gate filter:
\[F_{i, \text{gate}}' (k_x, k_y, \omega) = \begin{cases} 0 & \text{if Im}_i'(k_x, k_y, \omega) < T_i(k_x, k_y, \omega) \\ 1 & \text{otherwise} \end{cases}\]. 
This results in a cleaned image $Im_i'(k_x, k_y, k_\omega) F_i'(k_x, k_y, k_z)$ which can be mapped back into image space. The Fourier space is apodized by a Hanning windowo to avoid any vignetting. The full details can be found in the original paper \cite{deforest:2017}. 

I experimented with noise-gating and implemented it in Python, which I have made available \href{https://github.com/jmbhughes/noise_gating}{on GitHub}. Unfortunately, the temporal sparsity of the images in this thesis means it does not provide significant improvement and was not applied to the images. However, it could provide improved classification certainty in near-real-time application. 

\subsection{H-alpha imagery} 
As discussed in Section \ref{sec:halphainclusion}, it was impossible to differentiate between all labeled classes using only ultraviolet imagery. Thus, \halpha images, a narrow red passband at 656.28 nm caused by a hydrogen electron falling from the third energy level to the second, were included. They are particularly helpful in observing solar prominences and filaments. These images were gathered using the Virtual Solar Observatory \cite{vso}, an online compilation of images of the Sun from various sources, using SunPy \cite{sunpy, sunpyweb}, a Python toolkit for solar physics. In particular, \halpha images from the Global Oscillation Network Group (GONG) were used since they have continuous time coverage because of their seven globally distributed observing sites in Learmonth, Australia; Udaipur, India; El Teide, Spain; Cerro Tololo, Chile; Tuscon, Arizona, United States; Big Bear, California, United States; and Mauna Loa, Hawaii, United States. Each image is $2048\times2048$ pixels with each pixel being one arcsecond on a side. These are rescaled to SUVI using a linear transformation that scales the 1~arcsecond per pixel \halpha down to SUVI's plate scale of 2.5 arcseconds per pixel. The image was shifted so that the Sun remained centered.

\begin{figure}[H]
  \begin{center}
    \includegraphics[scale=0.1]{example-halpha.jpg}
    \caption{{\bf Example \halpha image}: This image is a typical \halpha view of the Sun from the GONG network. A prominence is visible on the bottom right, and a dark filament is visible in the center of the center of the Sun.}
    \label{fig:halphaimage}
 \end{center}
\end{figure}


\section{Labeled Imagery} \label{sec:labeling}
Since this study is using supervised machine learning, labeled images of the Sun are needed. There does not exist a verified sample of human labeled solar events for more than one category of event. The closest system is the Heliophysics Event Knowledgebase (HEK) \cite{hurlburt:2012} which combines data mining and computer vision with data visualization techniques to create a database of labeled events. These are sometimes verified by humans but can be problematic, especially with multiple entries per event to sift through. Each event is given one of their designated labels as shown in Table \ref{tab:heklabels}.  

\begin{table}[ht!]
  \centering
  \begin{tabular}{|p{4cm} p{10cm}|}
    \hline
    Event Class & Description \\ \hline
    Active Region & Solar Active Region\\
    Coronal Mass Ejection & Ejection of material from the solar corona \\
    Coronal Dimming & A large-scale reduction in EUV emission \\
    Coronal Jet & A jet-like object observed in the low corona \\
    Coronal Wave & EIT or Morton waves spanning a large fraction of the solar disk \\
    Emerging Flux & Regions of new magnetic flux in the solar photosphere \\
    Filament & Solar Filament or Prominence \\
    Filament Eruption & A sudden launching of a filament into the corona  \\
    Filament Activation & A sudden change in a filament without launching \\
    Flare & Solar Flare \\
    Loop & Magnetic loops typically traced out using coronal imagery \\
    Oscillation &  A region with oscillating coronal field lines \\
    Sigmoid & S-shaped regions seen in soft X rays; indicator for flares \\
    Spray & Surge Sudden or sustained intrusion of chromospheric material well into the corona \\
    Sunspot & Sunspots on the solar disk \\
    Plage &  Bright areas associated with active regions\\
    Other & Something that could not be classified – good candidate for further research \\
    Nothing & Reported Used to indicate that the particular data were examined, but had nothing noteworthy to the observer \\
    \hline             
  \end{tabular}
  \caption{List of event labels for HEK \cite{hurlburt:2012}.}
  \label{tab:heklabels}
\end{table}

For this project, a small curated set of solar events was produced using three solar physics experts.  Four annotators were asked however only one (Annotator B) provided a complete annotation of every image. One (Annotator A) provided a nearly complete sample. Another (Annotator C) provided only an image. The last withdrew from the project. They used an abbreviated set of solar event categories as detailed in Table \ref{tab:mylabels}. 


\begin{table}[ht!]
  \centering
  \begin{tabular}{|p{4cm} p{10cm}|}
    \hline
    Event Class & Description \\ \hline
    Bright Region & Solar Active Region\\
    Coronal Hole & Dimmer region in EUV where magnetic field lines are open\\
    Filament & Solar Filament \\
    Flare & Solar Flare \\
    Prominence & Solar prominence \\
    Limb & Edge of solar disk in EUV \\
    Structured outer space & region off the disk with structure \\
    Unstructured outer space & region off the disk with no structure \\
    Unlabeled & region where no label was given or confidence was especially low \\
    Quiet Sun & region on disk with no particularly interesting structures \\
    \hline             
  \end{tabular}
  \caption{List of event labels for curated data gathered in this study.}
  \label{tab:mylabels}
\end{table}

Twenty-seven image groupings of the SUVI six-band imagery were used for the labeled data as shown in Table \ref{tab:labelingtimes}. Each group consists of one image from each SUVI band, an \halpha image, and any needed derived images used as features in the machine learning. These images were selected because they were spread over the entirety of SUVI's operational lifetime and capture a variety of solar phenomena. 

\begin{table}[ht!]
  \centering
  \begin{tabular}{|c c c c|}
    \hline
    Group Number & Date & Time & Span of time (seconds) \\
    \hline
    0 & 2017-04-01 & 00:02:19 & 123 \\
    1 & 2017-04-15 & 00:01:49 & 150 \\
    2 & 2017-05-15 & 00:01:20 & 220 \\
    3 & 2017-05-20 & 00:02:07 & 121 \\
    4 & 2017-06-01 & 00:03:09 & 210 \\
    5 & 2017-06-15 & 00:02:57 & 120 \\
    6 & 2017-06-19 & 06:02:09 & 220 \\
    7 & 2017-07-01 & 00:02:18 & 120 \\
    8 & 2017-07-15 & 00:02:07 & 120 \\
    9 & 2017-07-28 & 05:02:17 & 122 \\
    10 & 2017-08-01 & 00:02:36 & 120 \\
    11 & 2017-08-20 & 00:01:07 & 160 \\
    12 & 2017-09-01 & 00:01:59 & 150 \\
    13 & 2017-09-08 & 00:01:58 & 180 \\
    14 & 2017-09-15 & 00:02:31 & 130 \\
    15 & 2017-10-01 & 12:01:56 & 150 \\
    16 & 2017-10-15 & 00:02:02 & 200 \\
    17 & 2017-11-02 & 00:02:07 & 210 \\
    18 & 2017-11-15 & 00:01:47 & 190 \\
    19 & 2017-11-30 & 00:01:51 & 150 \\
    20 & 2017-12-15 & 00:02:31 & 220 \\
    21 & 2018-01-01 & 00:03:04 & 210 \\
    22 & 2018-01-15 & 00:02:57 & 130 \\
    23 & 2018-02-01 & 00:02:54 & 200 \\
    24 & 2018-02-15 & 00:01:35 & 180 \\
    25 & 2018-03-01 & 00:01:57 & 220 \\
    26 & 2018-03-03 & 00:02:15 & 200 \\
    \hline
  \end{tabular}
  \caption{Times for images used in the labeling image set with the number of seconds between the first and last image in the grouping. }
  \label{tab:labelingtimes}
\end{table}


\subsection{Annotating Software}\label{sec:annotatesoftware}
In general, a solar annotation tool requires the following features:
\begin{itemize}
\item automatic solar coordinate alignment 
\item an interface to select a labeling class
\item utility to indicate which pixels should be labeled
\item ability to edit already annotated images
\item navigation tools to move throughout the image
\end{itemize}

To aid annotators, I created a very simple annotation tool in Tkinter that met these requirements and more. This tool is very much a work in progress and has iteratively been improved based on user feedback. There are currently plans to move the annotation tool into a web interface to avoid a cumbersome install process. (This is a response to users having problems installing necessary dependencies because of the wide variety of machine operating systems and environments.) 

A user of the tool must first configure a reference database. A reference database is an HDF5 file that coordinates the image paths and groups. This database contains meta-data about when the database was compiled, what channels it includes, and the master path to the images. Its main purpose is to coordinate image groupings. For every image grouping, there is an entry which contains a name and paths to each passband image. This structuring allows users to classify based on a shared data repository instead of having individual copies. The database can automatically be generated from a directory containing SUVI images with a provided script. In the case of this thesis, the reference database and passband images were compiled and given to the users to minimize their work. 

With this database set up, the annotator calls the software from the commandline by specifying where the reference database is located and where to save labeled images. To make this easier, the users were given a parameterless version of this call which was configured to run based on the zipped reference database and images given to them. 

Upon startup, the software will look at its configuration file and determine how many times each image should be classified. As discussed in Section \ref{sec:consistency}, each annotator was asked to label every group twice to determine how consistent they were. If there are still images left to label, the software loads one using the reference database. Using meta-data in the images, it solves for where the Sun should be located in each image. It is configured with an option to automatically classify the entire disk of the Sun as quiet Sun, the limb of the Sun, and everything else as unstructured outer space. The location of these classes is known and do not need classification. The annotator can then draw on top of these labels by dragging their mouse. A lasso tool is used to determine the region enclosed by their drawing and given their selected label.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[scale=0.25]{labeling-initial}
    \caption{{\bf Startup of annotation software:} The region on the left shows a preview image of the Sun. Initially, it is configured to show a three-color composite of the Sun, with channels scaled to show most features. The right region shows the labeling of the Sun.}
    \label{fig:label-initial}
 \end{center}
\end{figure}

In a single static image, it is difficult to label all the features because they are on different spatial, brightness, and spectral scales. Pan and zoom tools were included to allow the user to easily navigate about an image. In the configure panel, the annotator can adjust the left, preview solar image. They can choose to either see an RGB image that combines three different passbands or look at a single passband in a greyscale image. If they select the RGB they can assign the colors to their desired passband as well as what scaling factor to use. The scaling factor is included because the brightness values between different passbands differ. Without the adjustable scaling factor, the brightest passband will always dominate and more subtle detail cannot be seen. A single color option is included because some features may only be definitively visible in one passband, for example filaments in \halpha. 

\subsubsection{Iterated improvements}
While this software is not complicated, it was not the first approach. In the very first approach, the image was automatically segmented into super-pixels using the Simple Linear Iterative Clustering (SLIC) segmentation algorithm \cite{SLIC}. Drawing boundaries is more time consuming than simply clicking an region of the image. SLIC was used in the hopes of producing more labeled images. However, it systematically over/under segmented specific classes, making their boundaries consistently too large or small, and did not consistently segment filaments. It instead lumped the structure into a larger superpixel with other classifications. 

This approach would render the annotated database full of biases that would be difficult to correct. Effectively, the machine learning approaches used here would approximate the segmentation algorithm instead a gold-standard human definition. Thus, SLIC was abandoned and annotators denoted boundaries with a set of points that were individually clicked. Eventually, the current drawing approach was reached. It most effectively simulates the act of hand-drawing labeled maps by space weather forecasters used at NOAA today. The current version of the annotation software is available at the \href{https://github.com/jmbhughes/smachy}{author's Github}. 
\subsection{Analysis of labeled data}
Since this is the first digitally collected dataset of human solar annotations for multiple classes, it brings the opportunity to establish baseline metrics for human performance on the task of solar image segmentation. 

\subsubsection{Self-consistency of annotators} \label{sec:consistency}

Each annotator was asked to label each image group twice. They were not informed when they were relabeling but instead shown the images in a random order. This allows for a check on human self-consistency. If the task were trivial, then the annotators would be expected to have perfect consistency. However, this is not the case. 

\begin{figure}[H]
  \begin{center}
    \includegraphics[scale=0.1]{g20171015000202-0}
    \includegraphics[scale=0.35]{g20171015000202}
    \includegraphics[scale=0.1]{g20171015000202-1}
    \caption{{\bf Mistake for annotator} Left and right are annotations for Group 16, at center, by Annotator A. As you can see the annotator appears to have forgotten to label bright regions in one case. Such errors could be identified manually and cleaned from the labeled set. } 
    \label{fig:annotatorconsistency}
 \end{center}
\end{figure}

\begin{figure}[H]
  \begin{center}
    \includegraphics[scale=0.1]{g20171102000206-0}
    \includegraphics[scale=0.35]{g20171102000206}
    \includegraphics[scale=0.1]{g20171102000206-1}
    \caption{{\bf Typical inconsistency for annotator} Left and right are annotations for Group 17, at center, by Annotator A. In this case, the annotator makes a typical kind of disagreement in the extent of boundaries. It is unclear which boundary is correct so both must be included in the labeled set.} 
    \label{fig:annotatorconsistency2}
 \end{center}
\end{figure}

The agreement can be quantified using Cohen's kappa or Fleiss's kappa \cite{landiskoch}. Cohen's kappa can be used when there are two raters or in this case two annotations of one solar image group by the same annotator. Fleiss's kappa is used when more than two raters exist or the raters are not deliberately chosen but selected from a population at random. 

Cohen's kappa is defined as 
\[\kappa = \frac{p_o - p_e}{1-p_e} \] where $p_o$ is the relative agreement among raters and $p_e$ is the hypothetical chance agreement. $p_o$ is thus analogous to accuracy and in this case is interpreted as the number of pixels the annotator labeled the same between sessions. $p_e$ provides context for $p_o$ is calculated in the binary label class as the sum of the random chance of labeling true $p_t$ and the random chance of labeling false $p_f$. In turn, $p_t$ is the percentage of pixels labeled with a given class out of the total for each trial and $p_f$ is similarly defined. With more categories, it is the sum of chance of labeling a category as something other than agreed between sessions. Cohen's kappa can be interpreted as shown in Table \ref{tab:cohens} \cite{cohens}.

\begin{table}[ht!]
  \centering
  \begin{tabular}{|c c c|}
  \hline    
$\kappa$ range &  Agreement interpretation & \% of data that are reliable \\
\hline \hline
0.00-0.20	& None &	0-4\% \\
0.21-0.39	& Minimal &	4-15\% \\
0.40-0.59 & Weak & 15-35\% \\
0.60-0.79	& Moderate & 35-63\% \\
0.80-0.90 & Strong &64-81\% \\
$>0.90$  & Almost Perfect	& 82-100\% \\
\hline
  \end{tabular}
  \caption{Interpretation of Cohen's kappa coefficient for agreement \cite{cohens}}
  \label{tab:cohens}
\end{table}

Fleiss's kappa is defined as \[ \kappa = \frac{\overline{P} -    \overline{P_e}}{1-\overline{P_e}} \] where $\overline{P} - \overline{P_e}$ quantifies actual observed agreement above chance and $1 - \overline{P_e}$ quantifies attainable agreement above chance. These terms are calculated where $N$ is the number of pixels, $n$ is the number of ratings per pixel, and $k$ is the number of classes each pixel can be assigned. Each pixel is index $i = 1, 2, ..., N$ and each class is indexed $j = 1, 2, ..., k$. Then, $n_{ij}$ is the number of annotators who labeled the $i$-th pixel with class $j$. 

\begin{align*}
p_j &= \frac{1}{Nn} \sum_{i=1}^{N} n_{ij} \\
P_i &= \frac{1}{n(n-1)} \left(\sum_{j=1}^{k} n_{ij}^2 - n_{ij} \right) \\
\overline{P} &= \frac{1}{N} \sum_{i=1}^N P_i \\
\overline{P_e} &= \sum_{j=1}^k p_j^2
\end{align*}

Landis and Koch (1977) proposed the scale in Table \ref{tab:fleiss} for Fleiss's kappa \cite{landiskoch}. 

\begin{table}[ht!]
  \centering
  \begin{tabular}{|c c|}
  \hline    
$\kappa$ range &  Interpretation \\
\hline \hline
$<$ 0 & No agreement \\ 
0.0 - 0.19 & Poor agreement \\ 
0.20 - 0.39 & Fair agreement \\
0.40 - 0.59 & Moderate agreement \\
0.60 - 0.79 & Substantial agreement \\ 
0.80 - 1.00 & Almost perfect agreement \\
\hline
  \end{tabular}
  \caption{Interpretation of Fleiss's kappa coefficient for agreement}
  \label{tab:fleiss}
\end{table}

Since each annotator labeled twice, Cohen's kappa was used here as shown in Table \ref{tab:annotatorconsistency}. 

\begin{table}[ht!]
\centering
 \begin{tabular}{|c |c c c c c c c c c c | c|} 
 \hline
\textbf{A} & BR & CH & EOS & FIL & FLA & limb & PR & QS & SOS & UL & Sum \\ \hline
BR & 115923 & 0 & 4051 & 550 & 351 & 2363 & 1051 & 25379 & 0 & 0 & 149668\\
CH & 25 & 138006 & 222 & 4526 & 0 & 12361 & 0 & 44810 & 0 & 0 & 199950\\
EOS & 5758 & 11 & 6829470 & 0 & 0 & 0 & 2369 & 0 & 0 & 0 & 6837608\\
FIL & 815 & 6400 & 183 & 8415 & 0 & 548 & 1412 & 4105 & 0 & 0 & 21878\\
FLA & 567 & 0 & 0 & 0 & 231 & 19 & 0 & 0 & 0 & 0 & 817\\
limb & 3585 & 6258 & 0 & 100 & 0 & 229637 & 2441 & 334 & 0 & 0 & 242355\\
PR & 98 & 0 & 6627 & 0 & 0 & 6311 & 13417 & 4 & 0 & 0 & 26457\\
QS & 58507 & 47494 & 0 & 9850 & 14 & 0 & 0 & 2229489 & 0 & 0 & 2345354\\
SOS & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
UL & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 6312 & 0 & 0 & 6313\\ \hline
Sum & 185279 & 198169 & 6840553 & 23441 & 596 & 251239 & 20690 & 2310433 & 0 & 0 & \\
 \hline
 \end{tabular}
 \caption{Annotator A's consistency: This table details Annotator A's labelings for each class. The rows indicate one session and the columns another. Thus, column CH and row FIL with a value of 6400 indicate that in one session Annotator A labeled 6400 pixels as coronal hole and in the other session as filament. The classes have been abbreviated as follows: bright region is BR, coronal hole is CH, empty outer space is EOS, filament is FIL, flare is FLA, limb is limb, prominence is PR, quiet Sun is QS, structured outer space is SOS, and unlabeled is UL. As seen here, Annotator A did not use the structured outer space label and did leave some pixels unlabeled.}
 \label{tab:annotatora}
\end{table}

\begin{table}[ht!]
\centering
 \begin{tabular}{|c |c c c c c c c c c c | c|} 
 \hline
\textbf{B} & BR & CH & EOS & FIL & FLA & limb & PR & QS & SOS & UL & Sum \\ \hline
BR & 80107 & 605 & 3094 & 280 & 94 & 2474 & 141 & 41576 & 0 & 0 & 128371\\
CH & 227 & 319404 & 2613 & 0 & 0 & 19069 & 0 & 58172 & 0 & 0 & 399485\\
EOS & 1954 & 447 & 10256024 & 0 & 0 & 0 & 8498 & 0 & 150 & 0 & 10267073\\
FIL & 3101 & 0 & 0 & 12366 & 0 & 8 & 186 & 13238 & 0 & 0 & 28899\\
FLA & 570 & 0 & 0 & 0 & 573 & 0 & 0 & 31 & 0 & 0 & 1174\\
limb & 1012 & 15889 & 0 & 0 & 0 & 331215 & 10019 & 0 & 0 & 0 & 358135\\
PR & 0 & 92 & 3677 & 0 & 0 & 5229 & 29701 & 273 & 0 & 0 & 38972\\
QS & 30045 & 73408 & 0 & 7160 & 0 & 0 & 518 & 3411629 & 0 & 0 & 3522760\\
SOS & 0 & 0 & 141 & 0 & 0 & 0 & 0 & 0 & 590 & 0 & 731\\
UL & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\ \hline
Sum & 117016 & 409845 & 10265549 & 19806 & 667 & 357995 & 49063 & 3524919 & 740 & 0 &\\
 \hline
 \end{tabular}
 \caption{Annotator B's consistency: This table details Annotator B's labelings in the same format as Table \ref{tab:annotatora}.}
 \label{tab:annotatorb}
\end{table}

\begin{table}[ht!]
\centering
 \begin{tabular}{|c c c|} 
 \hline
 Class & Annotator A & Annotator B \\
   \hline\hline
Empty outer space & 0.996 & 0.997 \\
Structure outer space & N/A & 0.802 \\
Bright region & 0.687 & 0.650 \\
Filament & 0.370 & 0.507 \\
Prominence & 0.568 & 0.674 \\
Coronal hole & 0.687 & 0.783 \\ 
Quiet Sun & 0.945 & 0.958 \\
Limb & 0.929 & 0.923 \\
Flare & 0.327 & 0.622 \\ \hline
Overall & 0.941 &  0.956\\
 \hline
 \end{tabular}
 \caption{Annotator consistency: For each annotator and image, Cohen's kappa for each class is shown. Annotator B is more consistent for each class. The overall agreement is skewed by the large number of outer space pixels. Annotator A has no agreement score for structured outer space because the label was disregarded.}
 \label{tab:annotatorconsistency}
\end{table}

From Table \ref{tab:annotatorconsistency}, it is clear that labeling filaments is very difficult. Oddly, flares, which were anticipated to be simple were inconsistent as well. However, by looking at Tables \ref{tab:annotatora} and \ref{tab:annotatorb} it is clear that the inconsistency is not in  identifying them but drawing the appropriate boundary. Since there are so few pixels, bleeding over into the surrounding bright region or being too limited in the boundary has a huge skew. Classes that are fairly automatic like empty outer space and limb, where suggestion is provided in the annotating software, have high agreement. 

\subsubsection{Agreement among annotators} \label{sec:agreement}
Further, there is no universally agreed upon definition for each labeling class. Some cases are obvious, for example outer space. However, there are times where it is unclear which label to apply, for example a filament versus a coronal hole, or where exactly the boundary of the label should be drawn, for example which portions consist of active regions. The labeled database gives data-driven insight on where solar physics experts disagree. The disagreement may be amplified if experts from different research institutions or very different research focus are selected. 

In this labeling, Annotator A did not use the structured outer space class. However, it is a rare class, and they chose to skip the image group that Annotator B labeled substantial structured outer space. 
\begin{figure}[ht]
  \begin{center}
    \includegraphics[scale=0.45]{g20170619060208}
    \includegraphics[scale=0.125]{g20170619060208-0b}
    \caption{{\bf Structured outer space discrepancy} At left is the 30.4 nm image for Group 6. Very faintly in the upper left, some structure is visible in outer space as a CME moves outward. This is only seen in the 30.4 passband. Thus, if Annotator A had chosen to label this image they may or may not have missed this class.} 
    \label{fig:sos}
 \end{center}
\end{figure}

A much more typical example of disagreement is shown in Figure \ref{fig:annotatordifference}. 
\begin{figure}[ht]
  \begin{center}
  	\includegraphics[scale=0.1]{g20170415000148-a}
    \includegraphics[scale=0.35]{g20170415000148}
    \includegraphics[scale=0.1]{g20170415000148-b}
    \caption{{\bf Inter-Annotator disagreement} Group 1 provides a key example for disagreement among annotators.Annotator A is shown at left while Annotator B is shown at right. In the center, is the three-color image of the Sun at that time. Annotator A identified smaller coronal holes than Annotator B and included a filament Annotator B did not. This filament is not seen in \halpha but does look rather strong in the EUV three-color. In general, they identifies the same features but with different boundaries.} 
    \label{fig:annotatordifference}
 \end{center}
\end{figure}

This disagreement can be quantified with Cohen's kappa in Table \ref{tab:annotatoragreement}. 
\begin{table}[ht!]
\centering
 \begin{tabular}{|c c|} 
 \hline
 Class & Inter-annotator agreement\\
   \hline\hline
Empty outer space & 0.992 \\
Structured outer space & 0.0 \\
Bright region & 0.471\\
Filament & 0.380 \\
Prominence & 0.432 \\
Coronal Hole & 0.703 \\
Quiet Sun & 0.935 \\
Limb & 0.902 \\
Flare & 0.366 \\
 \hline
 \end{tabular}
 \caption{Inter-annotator agreeement: For each class the agreement between annotators on what pixels are declared that class is shown. Annotator A did not use the structured outer space class. Coronal hole is really the only non-automatic class that is reliable.}
 \label{tab:annotatoragreement}
\end{table}

A third annotator submitted only a handful of images, however they provide an interesting case study in Section \ref{sec:casestudy}.

\subsection{Case Study} \label{sec:casestudy}
    
\begin{figure}[ht]
  \begin{center}
  	\includegraphics[scale=0.4]{casestudycomparison}
    \caption{{\bf An interesting case study from Group 13} For this group, Annotator A and Annotator B submitted double labeling and Annotator C also submitted a labeling. This group comes from 2017-09-08 during a solar flare. All labelings include the annotated flare in the bottom right of the image. However, there are many other differences discussed in Section \ref{sec:casestudy}.}
    \label{fig:casestudy}
 \end{center}
\end{figure}

Some of the annotator disagreement noted in Section \ref{sec:agreement} and Table \ref{tab:annotatoragreement} for bright regions stems from the size of bright regions labeled. Annotator A tends to draw a larger boundary including the cascades of loops. Annotator B was a bit more conservative and only included the brighter regions of the cascades. Annotator C was the most conservative and only included the bright core. Along those same lines, Annotator C left a gap between the flare and surrounding bright region. The upper left bright regions were only sometimes separated even though a clear gap divides them. It appears Annotator C was also uncertain about the southernly coronal hole and chose to leave it unlabeled. However, they did identify another coronal hole that was not labeled by any others. The southern filament was only identified half the time. Annotators A and C agreed that the upper left dark region was a coronal hole while Annotator B decided it was filament. Upon further examination there is no filament visible in \halpha here so it likely is coronal hole. This may be why Annotator B also left it as quiet Sun. Prominences were very inconsistently labeled. This likely stems from them only being visible in 30.4 nm with special attention. Thus, a quick labeling may overlook some. Annotator A identified a spicule as a prominence in trial one. This potentially argues for another category for spicules or other "extra" features. 

Overall, there are a few types of disagreements:
\begin{itemize}
\item Uncertainty in data for labeling: coronal holes and filaments are often unclear. 
\item Boundary: It is unclear where the edge of a feature should be drawn and this creates disagreement. 
\item Omission: Sometimes, one annotator completely skips a clearly identifiable feature another annotator sees or even worse completely skips labeling a class by mistake. 
\end{itemize}

\subsection{Labeled database}
Ideally, the labeled database would be compiled with input from all annotators. However, only Annotators A and B consistently submitted labeled images. Therefore, there is no third vote to break ties when they have disagreements. Picking a label at random for each pixel when there is a disagreement would result in a very pixelated image, for example when it is unclear whether a feature is a filament or coronal hole the feature would be a mess of pixels switching between coronal hole and filament with quiet Sun on boundary disagreements. This would create a poor gold standard database.

Instead, when there are multiple annotators or trials for an image, one is selected at random as the gold standard. This incorporates definitions from all annotators and should spread out and hopefully eliminate systematic biases by preferencing one annotator's definition. The database is currently available upon request but will subsequently be made available online at the author's website. 

The database was divided in half, a training set and a labeling set, for the machine learning goals. For more details, see Section \ref{sec:evaluationmethods}. 


\chapter{Classifiers} \label{ch:classifiers}

In best conditions, a deep learning approach such as a convolutional neural network would be used because in other computer vision applications they have greatly outperformed other machine learning techniques. However, they require large amounts of training data. Therefore, this chapter introduces the machine learning techniques used. 

Instead of treating the images as a collection of individual pixels as described in the related work (Chapter \ref{ch:relatedwork}) or in a convolutional sense, these classifiers use neighborhoods of multichannel pixels, an compromise between the two approaches. The neighborhood of neighborhood of $(x_0, y_0)$ is $\{(x_i, y_i) | x_0 - w \le x_i \le x_0 + w, y_0 - h \le y_i \le y_0 + h\}$  where $w$ and $h$ are the number of pixels on either side of the reference pixel $(x_0, y_0)$ used in the horizontal and vertical dimension respectively. Each multichannel pixel is a $m \times 1$-dimension column vector where each entry corresponds to a spectral passband or derived channel. The derived channels are ratios of spectral passbands or spatial information but could be extended to any kind of value represented as a matrix of values the same size as the input images, in this case 1280x1280 pixels. The goal is to assign each multichannel pixel a label from Table \ref{tab:mylabels}. 


\section{Naive Bayesian Maximum Likelihood}

This approach mimics the maximum likelihood estimation of \cite{rigler:2012} described in Section \ref{sec:riglermaxlikelihood}. It is modified for pixel neighborhoods. If $w=h=0$, this model is identical to \cite{rigler:2012}. 

As before, every pixel is classfied into exactly one class using multispectral ultraviolet images, \halpha, and other derived channels as features. A pixel is a member of a pixel neighborhood that is used in classifying. Therefore, a pixel at $(i,j)$ is the reference pixel to the neighborhood $\{(i, j) | x_0 - j \le x_i \le x_0 + j, y_0 - k \le y_i \le y_0 + k\}$ where each neighborhood pixel has $c$ channels and can be described as a vector:
\[x_{(i,j)} = \begin{bmatrix} x_1 & x_2 & \hdots & x_c \end{bmatrix}^T \]
Thus, the neighborhood of pixels is generalized as a $(2w + 1) \times (2h + 1) \times c$ dimensional matrix, the amalgamation of all neighborhood pixels. 
For each $x_{(i,j)}$ pixel a label $w_k$ from the set of classes $W$. This approach employs Bayes' Theorem:
\[ P(w_k | x_{(i,j)}) = \frac{P(x_{(i,j)} | w_k) P(w_k)}{P(x_{(i,j)})} \]
Since $P(x_{(i,j)})$ is not a function of the label classification, it can be ignored.
\[ P(w_j | x_{(i,j)}) \propto P(x_{(i,j)}) P(w_{k}) \]
Following \cite{rigler:2012}: 
\[ P(w_k | x_{(i,j)}) \propto P(x_{(i,j)} | w_k) \]
This approach is the maximum likelihood solution to this problem.

Each class is modeled as a multivariate normal distribution, i.e. for each potential label there is an archetypal example pixel and all pixels with that label should be distributed normally about it. This is overly constraining if any given class has multiple distinct modes with respect to the selected data. However, this can be solved adding more subclasses, for example diving the bright regions into more types. Alternatively, this section could be replaced with a Gaussian mixture approach. The multivariate distribution for class $w_k$ is characterized by a mean vector $\mu_k$ and covariance matrix $C_k$ which are calculated as:
\[ \mu_k = \frac{\sum_{x \in W_k} x}{|W_k|} \]
where $W_k$ is the collection of pixel neighborhoods where the reference pixel has label $w_k$. Similarly,
\[ C_k = \frac{\sum_{x \in W_k} \begin{bmatrix} x - \mu_k \end{bmatrix} \times \begin{bmatrix} x - \mu_k \end{bmatrix}^T}{|W_k|} \]
These mean vectors and covariance matrices characterize the class. Given this characterization for class $w_k$ one can calculate the conditional probability of a pixel $x_{(i,j)}$ having label $w_k$:
\[ P(x_{(i,j)} | w_k) = \frac{1}{\sqrt{(2 \pi)^h} \sqrt{|C_k|}} \mathrm{exp} \left( \frac{-1}{2} \times \left(x_{(i,j)} - \mu_k \right)^T \times C_k^{-1} \times \left(x_{(i,j)} - \mu_k \right) \right) \]
Thus, the label that maximizes this probability is assigned for the reference pixel.

No smoothing is done in this version as in Rigler et al. (2012) \cite{rigler:2012}. 

\section{Random Forest}
The random forest is an ensemble of decision tree classifiers and are used in a variety of Earth remote sensing applications \cite{puissant2014object, salas2016multispectral, clark2016mapping, kulkarni2017multispectral, lowe:2015}. Solar image segmentation is very similar to remote Earth sensing, just with a different subject matter and often fewer passbands. In this application, an ensemble of 20 decision trees with maximum depth of 10 to form the random forest. The random forest is given the same types of input as the maximum likelihood classifier: the EUV channels, the \halpha channel, and derived channels (spatial, line ratios). The random forest implementation in Scikit-Learn is used \cite{scikit-learn}. 

\begin{figure}[ht]
  \begin{center}
    \includegraphics[scale=0.8]{rand-forest.jpg}
    \caption{{\bf An example random forest construction}: This helpful graphic shows the general layout of how a random forest works. Multiple decision trees are evaluated to determine a majority class vote \cite{randforestgraphic}}
 \end{center}
\end{figure}

Another benefit of the random forest is its ability to indicate what features are most important for classification. 

\section{Artificial Neural network}
An artificial neural network is a network of nodes and weights. The neural network takes inputs with a node for each input type. These inputs are transformed and passed onto layers of intermediate nodes. Each node has an activation function. The inputs are combined with different weights through a transfer function. The combined input is then compared via an activation function to an activation threshold. If the combined input is strong enough the node activates and passes a transformed value along to subsequent nodes in the next layer. These propogate forward to an output layer that provides the labeled class. 

\begin{figure}[ht]
  \begin{center}
    \includegraphics[scale=0.5]{ffneural}
    \caption{{\bf An example neural network construction}: This helpful graphic shows the general layout of a feedforward neural network. The number of nodes and hidden layers may vary. The size of the input layer is determined by the input features, and the output size is determined by the classification task \cite{neuralexample}.}
 \end{center}
\end{figure}

A simple fully-connected feed forward network is employed here consisting of three densely connected hidden layers with an input layer with similar neighborhood inputs and an output layer for classes. The hidden layers had 16, 16, and 8 nodes each with relu activation. 

A convolutional neural network was attempted with the expectation of poor results because of insufficient training. The system was based on the Fully Convolutional Neural Network appraoch by Long, Shelhamer, \& Darrell (2015) \cite{fcnn}, a method to take arbitrarily sized image input and create similarly sized image output with classification labels. The specific implementation used was a modification of Sarath Shekkizar's Tensorflow implementation \cite{fcnntensorflow}. This approach was intialized with VGG-16 pretrained model, although it is unclear if this training helps for solar image segmentation as well.

\chapter{Software for automatically generating theamatic maps} 

This chapter details how the software is designed and how future features can be included. The codebase is available at the \href{https://github.com/jmbhughes/smachy}{author's Github}. 

\section{Overview}
The software is available via Github in a Python package called "Smachy" for \textit{solar machine learning in Python}. It consists of a few modules to partition the code: \code{gui}, \code{filter}, \code{train}, \code{classify}, \code{evaluate}, and \code{config}. The \code{gui} module contains the annotation software. The \code{filter} module provides support for creating derivative images such as entropy images and ratios of EUV images. The \code{train} module organizes the training objects for the various classes and provides a \code{TrainingLoader} class for ease of use. The \code{classify} module performs solar image classification using the aforementioned classifiers. Evaluation is done through the \code{evaluate} module and provides support for performance metrics and confusion matrices. Finally, the \code{config} module defines global configuration parameters such as label mapping. This will eventually be moved to a json configuration file. 

\begin{figure}[ht]
  \begin{center}
    \includegraphics[scale=0.35]{codebase}
    \caption{{\bf Code layout}: A outline of the major classes in each module of \code{smachy} }
    \label{fig:codebase}
 \end{center}
\end{figure}

\section{Data structuring}
\textit{Flexible Image Transport System} or FITS is the standard image format for astronomical and solar images. The input solar images as well as any derived images are stored in this format. It has the benefit of providing convenient support for varied meta-data. For example, the annotated images have additional meta-data that indicates which images they were labeled using. All the channel names used in annotating are listed in a pipe-delimited entry under the keyword \code{CHANNELS}. Then, each channel name has an associated keyword with a relative path to the precise image used, in example \code{C131} might map to \code{131/NCEI_SUVI_COMPOSITE_131_G16_20170908_000148_v_0_2_0.fits}. The annotated image is an image of integer values where each integer corresponds to a label. The order of these integers is stored in the meta-data under the \code{CLASSES} keyword in case it is ever changed within the software. For example, the \code{CLASSES} entry may be:
\begin{lstlisting}
unlabeled|empty_outer_space|structured\outer_space|bright_region|filament|prominence|coronal_hole|quiet_sun|limb|flare
\end{lstlisting}
This can be split on the pipe and indicates that values of 0 map to \code{unlabeled}, values of 1 map to \code{empty_outer_space}, values of 2 map to \code{structured_outer_space}, and so forth sequentially. 

Thus, an annotated image can be opened to recover which solar images are associated with a multichannel image group. Alternatively, the group database, an HDF5 databse, also includes this data. The group database is generated first and handed to various portions of the software so it knows which images to open for analysis or annotation. This database is often called \code{groups.h5}. It retains when it was calculated as well as the path to the directory of images. Then, the \code{groups} entry contains a dictionary of each group. A group is given an unique designator starting with "g" based upon the observation time. For example, "g20170401000218" indicates the group is for April 1, 2017 at 00:02:18. The entry for the group then contains the relative paths for images associated with it: 
\begin{minted}{python}
'g20170401000218': 
    {'files': 
        {'c131': '131/NCEI_SUVI_COMPOSITE_131_G16_20170401_000143_v_0_2_0.fits',
        'c171': '171/NCEI_SUVI_COMPOSITE_171_G16_20170401_000303_v_0_2_0.fits',
        'c195': '195/NCEI_SUVI_COMPOSITE_195_G16_20170401_000213_v_0_2_0.fits',
        'c284': '284/NCEI_SUVI_COMPOSITE_284_G16_20170401_000333_v_0_2_0.fits',
        'c304': '304/NCEI_SUVI_COMPOSITE_304_G16_20170401_000133_v_0_2_0.fits',
        'c94': '94/NCEI_SUVI_COMPOSITE_94_G16_20170401_000233_v_0_2_0.fits',
        'chalpha': 'halpha/gong_6567_20170401_000130.fits'},
   'timespread': 123.0},
\end{minted}
This groups database can be automaticallly generated by using the provided \code{build_file_databases.py} script with commandline argument of directory path to images. Then, each channel should be separated into subdirectories of the desired channel name. The script will create $n$ groups where $n$ is the smallest number of images in any channel. It will then greedily pick other images such that the group has the smallest spread over observation time. 

\section{Annotation GUI}
The annotation GUI's functionality is described in Section \ref{sec:annotatesoftware}. It is implemented in Tkinter in Python. In hindsight, this was not the best decision because Tkinter appears differently on different operating systems. Even more importantly, since it was part of the \code{smachy} package it required installation of various dependencies to run. Annotators were given installation scripts and instructions to use virtual environments, but it still was ultimately more complicated than a more portable, stand-alone Python program, a very portable Java JAR, or a web application. 

To make matters even easier, two execution scripts for the GUI are provided. The first \code{run_gui.py} requires very specific input regarding which image group to annotate, where to save, and other flags that can be accessed through the commandline help menu. Since annotators would be using the same directory structurinig, an easier script \code{easy_gui.py} was provided. It defaulted all the parameters and could be run without designating any flags. It expects to be run in a directory containing a \code{groups.h5}, a directory of images, and a directory called \code{labeled} of annotated images. It will run until every group in the groups database has the requested number of annotated images. 

\section{Trainers}
The \code{train} module implements a generic \code{Trainer} with some helper methods and abstract methods required to be implemented in any specific \code{Trainer}. A \code{Trainer} takes input of a list of annotated images, a dictionary of the paths to images associated with the annotated images, list of labels to classify on, and a list of image channels to utilize. Determining these paramters is made easy with the \code{TrainingLoader} class. The \code{Trainer} then provides helper functions to access pixels associated with any label. Any specific \code{Trainer} must implement a \code{train()} method and a \code{make_classifier()} method. Train does the specific training routine for that kind of classifier and sets up the appropriate data structures. \code{make_classifier()} calls the associated \code{Classifier} object and returns it with the trained parameters. 

Training is done with the appropriate \code{train_*.py} script, for example \code{train_bayesian.py} for the Bayesian model. The required inputs and outputs are described in the commandline help accessible with the \code{-h} flag. To do training en masse, \code{train_all.sh} runs all all the specific training scripts with specified inputs. 

Thus, to add a new trainer inherit the generic \code{Trainer} and implement a specific training script in the same fashion as one of the existing scripts. 

\section{Classifiers}
The implementations of the classifiers was done in a polymorphic fashion so that they can be exchanged easily when running and evaluating performance. Each classifier is designed to include the following methods:
\begin{itemize}
\item \code{classify_pixel}: Given a single pixel's feature set, classify it 
\item \code{classify_image}: Iterates over an entire image and performs classification
\item \code{save}: saves a classifier to a Python serialized file
\item \code{load}: loads a classifier from a Python serialized file
\end{itemize}
The parent \code{Classifier} class also provides helper methods to load essential data. 

Similar to training, there are specific classifier scripts for running and a \code{classify_all.sh} to run them all. To add a new \code{Classifier} inherit the generic one and provide model specific classification methods as well as a way to save and load. Then, provide a classification script, and it should run. 

\section{Filters}
More complicated features than raw data values are desirable. The \code{filter} module provides support for generating output images. The main \code{Filter} class provides support for opening and saving an image/derived image as well as a general purpose \code{process} method. This \code{process} method utilizes the \code{_filter} method to process a single group in a group database. The \code{filter} method is implemented separately in every subclass. For example, the \code{RatioFilter} implements a \code{_filter} method that takes a group and then passes each channel pair to a \code{_filter_one} method that actually performs the division to scale an image. All the implemented filters can be called in the provided \code{run_filters.py} script. 

\section{Evaluator}
To support consistent and convenient analysis, the \code{Evaluator} code in the \code{evaluate} module will compare a set of computer classified images to the gold standard annotated images. It will in turn generate confusion matrices and other performance metrics for analysis. It is accessible through the \code{run_evaluate.py} script. 


\chapter{Experiments and Results} \label{ch:experiments}

This chapter describes various experiments performed on the data and the conclusions derived while working toward the optimal machine learning solar image classifier, most importantly it explores feature selection. It begins by introducing the methods of evaluation in Section \ref{sec:evaluationmethods} and then proceeds through the experiments. 

The database of annotated images was broken into two categories: a training set and a testing set. Fourteen images were used in training and thirteen were used for testing. These were randomly selected so that no observational time was biased. The classifiers in the experiments are trained solely on the images in the training set and only exposed to the testing set for evaluation.

\section{Evaluation approaches}\label{sec:evaluationmethods}

\subsection{Quantitative}
The testing set and classifier generated results are compared quantitatively. For each class several several statistics are calculated based on true positive count (TP), true negative count (TN), false positive count (FP), and false negative count (FN). For a given clas label, TP is the number of pixels classified with that class and the gold standard database agreed. On the other hand, FP is the number of pixels classified with that class when the gold standard database disagreed. Similarly, TN is number of times the classifier correctly did not assign the class and FN is when the classifier labeled a class other than the designated class. From these values, several metrics of performance are derived. One of the more widely known metrics is accuracy, how often a classifier is correct:
\[\text{accuracy} = \frac{TP + TN}{TP + TN + FP + FN}\] However, accuracy is not as helpful when classes are skewed, for example flares are very uncommon so never using the label of flare trivially optimizes accuracy. Other metrics are more revealing in this case. Precision is defined as:
\[ \text{precision} = \frac{TP}{TP + FP} \]
It measures the ability of the classifier to not label a pixel the given class when it is not. Recall is another metric that characterizes the classifier's ability to identify all pixels of a given class:
\[\text{recall} = \frac{TP}{TP + FN}\]
Both precision and recall are important but are often anti-correlated. The f1 score combines them:
\[\text{f1} = \frac{2 \times \text{precision} \times \text{recall}}{\text{precision} + \text{recall}}\]

The aforementioned metrics do not indicate exactly how the classifier fails. For example, the accuracy for coronal holes mmight be low, but it isn't clear what the coronal pixels are being classified as instead. A confusion matrix like the xample in Figure \ref{fig:exampleconfusionmatrix} helps remedy this.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[scale=0.8]{confusion_matrix_example}
    \caption{{\bf An example confusion matrix}: A confusion matrix is a method of showing label classification. On the vertical axis the correct label is shown while on the horizontal axis the annotator's label is shown. Then, looking at the cell defined shows the percentage of labels in that region. Ideally, the diagonal should be 100 indicating every feature was labeled with the correct label always. The confusion matrices presented in this thesis are normalized.}
    \label{fig:exampleconfusionmatrix}
 \end{center}
\end{figure}

The confusion matrices in this thesis are presented normalized such that each row should sum to 100 ($\pm 1$ for rounding). Confusion matrices might typically be presented as raw counts, but it is not beneficial in this case since some classes greatly outnumber others. So, in this matrix, prominences are correctly labeled for 5\% of prominence pixels in the gold standard database. They are often labeled as limb instead, 74\% of the time and sometimes as empty outer space 16\%. This is not too unexpected since prominences are on the limb. While the columns cannot be summed, they do provide some insight. For example, the predicted label of limb comes from prominence and limb. It is important to note that limb is correctly classified 81\% of the time, so the 74\% between prominence and limb should not be interpreted to mean limb is equally likely to be labeled as prominence and limb. 

\subsection{Qualitative}
However, quantitative metrics do not fully capture the subtleties of analysis. It does not provide clear insight into why problems occur. Thus for each experiment, the generated thematic maps from the trained classifiers are qualitatively compared to the testing images. Several qualities are inspected:
\begin{itemize}
\item Are all classes expected in this image present? If not, what classes are missing or overly present?
\item How do the boundaries of each region compare to the testing image? 
\item Is the classification noisy with neighboring pixels varying? 
\item Is the limb correctly aligned? 
\item Are there any other pecuiliarities? 
\end{itemize}

\section{Baseline results}
To provide a baseline, the classifiers were first run with a neighborhood size of 0 meaning that classification only looked at a pixel at a time using only the EUV imagery from SUVI. For each classifier, a typical example of the classification is shown:

\begin{figure}[H]
  \begin{center}
    \includegraphics[scale=0.8]{baseline}
    \caption{{\bf Baseline performance for single pixel classfiers}: Each classifier is labeled and compared to the human expert gold standard.}
    \label{fig:baseline}
 \end{center}
\end{figure}

As shown in Figure \ref{fig:baseline}, the Bayesian classifier created a very noisy result with pixels even in outer space being misclassified. The distribution for structured outer space and unstructured outer space are very similar: nearly completely empty in every channel. Brightness will decline radially from the center of the solar disk. In the training data set, the only example of structured outer space was at a far distance from the solar disk and thus darker. Since this classifier does not see beyond a single pixel, the structure is relatively unimportant compared to the darkness. The radial brightness dependence is repeated for prominences. Coronal holes and filaments are severely overestimated. However, this classifier does not appear to have confused coronal hole and filaments and applied the correct one in each setting generally. However, it struggled to differentiate between coronal hole/filament and quiet Sun.  

\begin{figure}[ht]
  \begin{center}
    \includegraphics[scale=0.8]{base_bayesian_conf}
    \caption{{\bf Baseline Bayesian confusion matrix}}
    \label{fig:base-bay-conf}
 \end{center}
\end{figure}

\begin{figure}[ht]
  \begin{center}
    \includegraphics[scale=0.8]{base_rf_conf}
    \caption{{\bf Baseline random forest confusion matrix}}
    \label{fig:base-rf-conf}
 \end{center}
\end{figure}

\begin{figure}[ht]
  \begin{center}
    \includegraphics[scale=0.8]{base_nn_conf}
    \caption{{\bf Baseline neural network confusion matrix}}
    \label{fig:base-nn-conf}
 \end{center}
\end{figure}

The baseline random forest classifier failed to identify large filaments on the disk of the Sun that were clearly identified in all training sets. However, it did a reasonable job with coronal holes and identified more structure in bright regions than human annotators recorded. Upon follow-up it outperforms humans in this regard. The human annotator drew a quick boundary that was smooth and does not capture the complexity of bright regions. The neural network was very similar to random forest results. However, the random forest failed to identify flares while the neural network was successful. 

\begin{table}[ht!]
\centering
 \begin{tabular}{|c | b r n | b r n | b r n | b r n|} 
 \hline
 & \multicolumn{3}{c|}{\textbf{Accuracy}} &  \multicolumn{3}{|c|}{\textbf{Precision}} & \multicolumn{3}{|c|}{\textbf{Recall}} & \multicolumn{3}{|c|}{\textbf{f1}} \\
 & B & R & N & B & R & N & B &  R & N & B & R & N  \\
   \hline 
Empty outer space & 39 & 99 & 99 & 100 & 99 & 99 & 14 & 99 & 99 & 24 & 99 & 99 \\
Structured outer space & 42 & 99 & 99 & 0 & N/A & N/A & N/A & 0 & 0 & N/A & N/A & N/A\\
Bright region & 98 & 99 & 99 & 55 & 66 & 55 & 47 & 47 & 49 & 51 & 55 & 56\\
Filament & 91 & 99 & 99 & 3 & 0 & 0 & 64 & 0 & N/A & 6 & N/A & N/A\\
Prominence & 96 & 99 & 99 &  2 &  57 & 19 & 61 & 4 & 0 & 4 & 8 & 1\\
Coronal Hole & 93 & 98 & 98 & 18 & 85  & 81 & 83 & 53  & 60 & 30 & 65 & 69 \\
Quiet Sun & 84 & 97 & 97  &  93 & 90 & 92 & 39 & 97 & 95 & 55 & 93 & 93 \\
Limb & 97 & 98 & 98 & 64 & 70 & 64 & 52 & 72 & 80 & 57 & 71 & 71  \\
Flare & 99 & 0 & 100 & 0 & 0 & N/A  & N/A & N/A & N/A & N/A & N/A & N/A \\ 
 \hline
 \end{tabular}
 \caption{Baseline performance: When given only a single pixel view and EUV channels, here is the performance for each classifier.}
 \label{tab:baselinemetric}
\end{table}

\newpage

\section{Normalization of data}
Since each EUV channel is not equally bright, the Bayesian classifier may be skewed. (This exploration was prompted by discussion with solar physics experts at NOAA.) The high values in one channel can effectively wash out subtle value differences in another potentially. Thus, the EUV channels were replaced with their normalized counterparts. Normalization was done to the disk of the Sun by first calculating where the disk of the Sun should be in each image. The solar disk location was determined by using orbital information instead of edge detection. For each class, the median pixel value on the disk was calculated over all images and designated the normalization coefficient. Each channel must be normalize to the same value over all images or else when comparing an image without a flare/bright region to one with the actual pixel value loses context. Without consistent coefficients, a pixel of medium value in the flare image would be many times brighter than a pixel of the same value in the unflared image. The calculated coefficients are in Table \ref{tab:normcoefficients}. 

\begin{table}[ht!]
\centering
 \begin{tabular}{|c c |} 
 \hline
 Channel & Normalization coefficient\\
   \hline
   94 &  0.036344 \\
   131 & 0.072936 \\
   171 & 0.675868 \\
   195 & 0.782299 \\
   284 & 0.964538 \\
   304 & 7.265509 \\
   \halpha & 2888.6 \\
 \hline
 \end{tabular}
 \caption{Normalization coefficients for each channel}
 \label{tab:normcoefficients}
\end{table}

However,this does not improve the performance of the Bayesian classifier as seen in Figure \ref{fig:normbayes}. 

\begin{figure}
  \begin{center}[H]
    \includegraphics[scale=0.6]{normcomp}
    \caption{{\bf Normalized performance for classifiers}: Each classifier is labeled and compared to the human expert gold standard.}
    \label{fig:normbayes}
 \end{center}
\end{figure}

The performance of the other classifiers was similarly unchanged. Thus, normalization is omitted. 

\newpage

\section{\halpha inclusion}\label{sec:halphainclusion}
As seen in Figure \ref{fig:halphamotivation}, filaments and coronal holes cannot be differentiated using spectral information in the EUV channels of SUVI. Spatial approaches may discern between them because filaments are long, skinny strands while coronal holes are typically more round and extended. 

\begin{figure}[ht]
  \begin{center}
    \includegraphics[scale=0.8]{degeneracy}
    \caption{{\bf Degeneracy of filament and coronal hole spectrally with EUV images} This histogram compares 19.5 nm pixel values for filaments and coronal holes. They nearly overlap completely making it difficult for classification methods to key on sufficient themes for classification. All other EUV channels suffered from complete overlap. \halpha images differentiate filaments and coronal holes as shown in Figure \ref{fig:halphause}.} 
    \label{fig:halphamotivation}
 \end{center}
\end{figure}


Alternatively, \halpha spectral information easily differentiates between coronal holes and filaments as shown in Figure \ref{fig:halphause}. 

\begin{figure}[ht]
  \begin{center}
    \includegraphics[scale=0.3]{halphacomp1}
    \includegraphics[scale=0.315]{halphacomp2}
    \caption{{\bf Coronal hole and filament in \halpha} In \halpha images like the one at right, filaments are dark strand structures while coronal holes (dark regions in the EUV image at left) have no apparent signature. This defining difference complements the degneracy in EUV images and boosts classifier performance.}
    \label{fig:halphause}
 \end{center}
\end{figure}

In this experiment, the suite of classifiers is trained with both EUV and \halpha channels. The random forest classifier performs twice as well on prominences compared to the baseline. The neural network has an f1 measure of 13\% for prominences compared to 1\% in the baseline. The precision is now 75\% compared to 19\% in the baseline. 

The Bayesian classifier does not improve for reasons mentioned already. However, the random forest and neural network performance changes much more. Figure \ref{fig:rfhalphaimprove} shows that the random forest classifier now identifies prominences. Prominences are caused by the same phenomenon as filaments but are off the solar disk. Thus, they have higher contrast in EUV images. Unfortunately, the filaments did not resolve just by adding \halpha. This is likely due to confusion in the training database. Upon closer inspection, there are instances when annotators identify filaments without \halpha being present. This adds even more complication to the definition of a filament. 


\begin{figure}[ht]
  \begin{center}
    \includegraphics[scale=0.6]{halphaimprove}
    \caption{{\bf \halpha inclusion performance} At bottom left is the performance of the random forest classifier with a single pixel neighborhood with only the EUV channels. This looks very similar to the performance on the bottom right from the random forest including \halpha with the EUV. However, prominences are correctly labeled. They both lack identified filaments in the gold standard image at right. The neural network classifier loses the ability to define classes clearly.}
    \label{fig:rfhalphaimprove}
 \end{center}
\end{figure}

Interestingly, the neural network performance degraded considerably by adding the \halpha channel as seen in Figure \ref{fig:rfhalphaimprove}. The entire disk is now consistently classified as quiet Sun. Filaments are still rather faint in \halpha but the rest of the solar disk is nearly constant. This constant appearance is more strongly weighted in the feed forward neural network than desired and overwhelms any other classification. With extensive tweaking, the peformance could likely be corrected and allow for comparable performance to the random forest. However, this was not pursued. 

\newpage 

\section{Line ratios}
Line ratios can be used as a proxy for temperature in the solar images. It is often more helpful to look at how bright different features are relative to another line because it provides context for the observation. Thus, the ratio between every passband was included as a feature. Some of these ratios may not be all that helpful and pollute the feature set and warrant further investigation of all possible subsets for optimal performance. 

With the line ratios, the number of features grows very large. To determine if the 

\newpage

\section{Spatial features}
In an attempt to incorporate, spatial information into a pixel-by-pixel classifier several spatial filters were used as input features. First, the distance from solar center was used. Since the Sun should be centered in each image after preparation in the earlier data processing stage, a simple estimate of the distance for pixel $(i,j)$ is $\sqrt{i^2 + j^2}$. This is pertinent because some features, such as prominences, will only appear at specific solar radii while others, such as flares, are distributed across the entire solar disk. I also investigated adding a spatial feature based on the image entropy. However, both proved unsuccessful in the small dataset since it made every classification unique. 

\section{Summary} 
Each classifier has different strengths and weaknesses. The Bayesian approach is easy to understand and provides insight into its classification approach with archetypal pixels. However, it is much slower to run because for each pixel there are more than $1280 \times 1280$ matrix multiplications, becoming even more costly as the number of features in the matrix increases. On my laptop, it takes minutes to classify a single image. Runtime is an issue in real-time execution but can be easily overcome with multi-core execution on larger, faster computers. The random forest is very fast in execution but can easily become over-trained. It also provides insight into classification methods. The neural network is a step toward deep learning and can be computed quickly but veils the classification approach because it is difficult to interpret the significance of specific weights. 

\chapter{Applications} \label{ch:application}
Solar image segmentation is helpful for space weather forecasting but is not the end goal for research. Solar physicists can utilize it as a tool in research applications. To aid in finding relevant data, a database of observations with key statistical features can be compiled from thematic maps as described in \ref{sec:database}. Alternatively, the segmentation can be used directly as a boundary as illustrated in an exploration of fractal dimension and class properties in Section \ref{sec:fractal}. 

\section{Database building} \label{sec:database}
Ultimately when running continuously in real time, a thematic map will be generated approximately every five minutes resulting in 288 images each day or 2,102,400 over twenty years. While more manageable than the full dataset, it still is tedious to search the images for specific phenomena because thematic maps only encode labels and not statistical distributions for observed values over an image. Thus, I developed a running database. When a new thematic map is generated, an entry is added for each label class as well as each contiguous solar feature. A contiguous solar feature is defined as a region on an image given the same label, for example each coronal hole in an image is a separate contiguous feature. A given pixel at index $(i,j)$ is a vector of values in each EUV channel, for example $x_{ij} = \begin{pmatrix}2 &  3 & 4 & 1 & 6 & 8 \end{pmatrix}$. Thus, the pixel set $X$ with $N$ entries is the set of pixels associated with that entry. Let $x[k]$ denote the $k$-th entry in $x$, for example $x[1] = 2$ and $x[2] = 3$. Each entry includes the following features:
\begin{itemize}
\item \textbf{mean location horizontally}: the average of the pixel horizontal indices in the entry's pixel set, $\frac{1}{N} \sum_{x_{ij} \in X} i$
\item \textbf{mean location vertically}: the average of the pixel vertical indices in the entry's pixel set, $\frac{1}{N} \sum_{x_{ij} \in X} j$
\item \textbf{number of pixels in entry}: the size of the entry's pixel set, $N$
\item \textbf{width}: the difference between the largest horizontal pixel index and the smallest horizontal pixel index, $\max \{i | x_{ij} \in X\} - \min\{i | x_{ij} \in X\}$
\item \textbf{height}: the difference between the largest vertical pixel index and the smallest vertical pixel index,  $\max \{j | x_{ij} \in X\} - \min\{j | x_{ij} \in X\}$
\item \textbf{average brightness in each channel}: for each EUV passband, the average brightness over the pixel set, for the $k$-th channel $\frac{1}{N} \sum_{x_{ij} \in X} x_{ij}[k]$
\item \textbf{maximum brightness in each channel}: for each EUV passband, the maximum brightness over the pixel set, for the $k$-th channel $\max \{x_{ij}[k] | x_{ij} \in X\}$
\item \textbf{minimum brightness in each channel}: for each EUV passband, the minimum brightness over the pixel set, for the $k$-th channel $\min \{x_{ij}[k] | x_{ij} \in X\}$
\item \textbf{standard deviation of brightness in each channel}: for the $k$-th channel defined as $\sqrt{\frac{1}{N-1} \sum_{x_{ij} \in X} \left(x_{ij}[k] - \overline{x_{ij}[k]} \right)^2 }$ where $\overline{x_{ij}[k]}$ is the average brightness of the pixels in that passband
\item \textbf{25-percentile of brightness in each channel}: the brightness that 75\% of the entry's pixels are greater than in that passband
\item \textbf{75-percentile of brightness in each channel}: the brightness that 25\% of the entry's pixels are greater than in that passband
\item \textbf{assigned label}: the label given that entry, for example coronal hole
\item \textbf{time}: the time of observation for that image group
\end{itemize}

More metrics could be added to this list as desired to make identifying interesting images easy. The database can also be used to study trends over time, between entry values, or the distribution of an entry value.

This database can serve as a sanity check that the classification approach is working. In addition, a myriad of simple results like those in Figures \ref{fig:coronal_hole_area} and \ref{fig:coronal_hole_brightness} can be extracted.

\begin{figure}
  \begin{center}
    \includegraphics[scale=0.6]{coronal_hole_area}
    \caption{{\bf Coronal hole area} This histogram shows the number of pixels in coronal holes in the databaase. There are coronal holes with more than 3000 pixels, but they are much rarer and not shown so detail can be extracted.}
    \label{fig:coronal_hole_area}
 \end{center}
\end{figure}

\begin{figure}
  \begin{center}
    \includegraphics[scale=0.6]{coronal_hole_brightness}
    \caption{{\bf Coronal hole brightness} It may be interesting to look at the average brightness of a coronal hole in various passbands, potentially useful for clustering types of coronal holes. This is immediately accessible from the database. }
    \label{fig:coronal_hole_brightness}
 \end{center}
\end{figure}

\section{Fractal dimension and class properties} \label{sec:fractal}
Natural objects with complexity often possess self-similarity that Mandelbrot (1983) realized corresponded to a fractional dimension measure \cite{mandelbrot}. Fractals can arise from turbulent and chaotic systems and their existence can be used to understand the dynamics \cite{fractalchaos}. Solar magnetic activity is modelled by a dynamic, chaotic system \cite{lawrence+1995}. McAteer et al. (2015) utilized magnetograms from the SOHO mission to compare the fractal dimension of active regions to their classification and flare rate and found that the dimension was invariant under classification \cite{mcateer+2005}. They found that an active region appears to need a fractal dimension of at least 1.2 to produce M-class flares or stronger \cite{mcateer+2005}.  Aschwanden and Aschwanden (2008) found that flares in their sample, including a strong M-class flare, had fractal dimensions ranging from 1.0 to 1.89 \cite{aschwandensquared}. The difference between the lower threshold could stem from different methodology. On the other hand Georguilis (2012) argued that fractal dimension is insufficiently differentiating to determine if an active region will flare or not \cite{Georgoulis2012}. Finally, Rajkumar et al. (2017) examined sunspots, the white light counterpart to active regions, and found the umbra to have a fractal dimension of $2.09 \pm 0.42$ while the penumbra $1.72 \pm 0.4$ \cite{Rajkumar2017}. More study is needed to rectify these disagreements. 

The solar image segmentation presented here provides a tool to assist with that study. As performance improve, the classification of regions can automatically be trusted. Thus, active regions and coronal holes, both possessing complex boundaries, can automatically be studied for large datasets. For example, Figure \ref{fig:boundaries} presents automatically determined boundaries for an active region and coronal hole. The same approach was used in both cases, so I will present the active region method. The classified thematic maps were converting to binary masks, 1 for where an active region was classified and 0 for anything else. Using morphological erosion and dilation, the boundary was extracted as the the difference between the dilation and erosion. These regions have complex boundaries. As the performance of the classifier increases, these boundaries will become more reliable. 

\begin{figure}[ht]
  \begin{center}
    \includegraphics[scale=0.4]{active_region_boundary}
    \includegraphics[scale=0.4]{coronal_hole_boundary}
    \caption{{Boundary of active region and coronal holes}: At left is the boundary of an active region and at right a coronal hole. Both were identified with the classifiers presented in this thesis.} 
    \label{fig:boundaries}
 \end{center}
\end{figure}

There are several definitions of dimension when discussing fractals. The topological dimension is the standard dimension taught in elementary schools and must always be a natural number. Formally, a set $A$ has topological dimension 0 if every point has a neighborhood, an open set around it, of arbitrarily small length where the boundary of $A$ misses $A$, $\text{bd} A \cap A = \emptyset$. Higher topological dimension can be defined recursively. For example, a set $A$ has topological dimension 1 if it is not topological dimension 0 and every point in $A$ has an arbitrarily small neighborhood whose boundary meets $A$ in a set of topological dimension 0. This dimension does not characterize the rough boundaries of many natural features. The Hausdorff dimension is a common dimension utilized, but is more techincal and not used here. In practice, the Minkowski or box-counting dimension is used. For a compact set $A$, in the real plane as these images are a set is compact if it bounded and closed, let $\epsilon > 0$. Then, $N(\epsilon)$ is the least number of boxes of size $\epsilon$ required to cover $A$. For images, this can be thought of as dividing the image into pixel boxes. So a box of size 4 would be 2 pixels on each side. In theory, $\epsilon$ can take any value but in practice only powers of two are used here. Then, $N(\epsilon)$ is the number of those pixel blocks needed to insure the entire edge shown in Figure \ref{fig:boundaries} is within some box. Thus, the Minkowski dimension is defined as $\lim_{\epsilon \to 0} - \frac{\log N(\epsilon)}{\log \epsilon}$. Since a limit is not feasible in this context, it is approximated by the slope of a linear regression between $\epsilon$ and $N(\epsilon)$. 

Using the box-counting algorithm, the Minkowski dimension can be approximated. For active regions, the regions in this sample had average boundary Minkowski dimension of 1.05 with a standard deviation of 0.12. The coronal holes had an average Minkowski dimension of 1.26 with a standard deviation of 0.06. These values are very preliminary as they come from an imperfect boundary detection method and do not correct for perspective distortion. As coronal holes or active regions approach the limb, they smear out. For a very accurate fractal dimesion, features either need to be from the center of the solar disk or corrected for this distortion.

\chapter{Conclusions} \label{ch:conclusion}
This chapter provides an overview of potential future work and research questions as well as a summary of the results.  

\section{Summary}
Ultimately, this project has established a baseline for a publicly available database of expert annotated solar images for machine learning. There are concerns regarding the consistency of labeling of annotators as well as agreement between annotators. This plagued the results and led to a consistency checked database. It was difficult to determine how effective various features were in aiding classification because of the inconsistency and the relatively small database. However, \halpha does seem to improve prominence classification despite filaments being limited by dataset inconsistency. The other features explored had limited improvement.

\section{Future work}
There are many avenues for future inquiry stemming from this project. A few are presented here. 

\subsection{Stability over solar cycles}
The Sun goes through 11-year cycles where behavior changes drastically. During the "solar minimum" phase, where the Sun is now, coronal holes are much more prominent and extend further down to the solar equator, there are fewer flares and bright regions. As solar maximum approaches the distribution and characteristics of the classes will change. This may require new training data. It also may require the addition of a solar cycle feature that links data to when in a solar cycle it was taken. One way to study this is by looking at satellite with a longer history, such as SDO, or waiting until SUVI has a long history of observations. 

\subsection{Converting digitized synoptic charts}
Since obtaining annotated images was among the most difficult portions of this project, it would be nice to utilize as much existing annotation as possible. Space weather forescasters at NOAA create thematic maps daily by hand and have done so since 1972. These are digitized into a PDF format. 

\begin{figure}[ht]
  \begin{center}
    \includegraphics[scale=0.8]{synoptic}
    \caption{{\bf Daily synoptic drawing} Here are four synoptic charts drawn at NOAA ranging from 1974 in the upper left to 2018 in the bottom right. Even more recent synoptic charts vary in style because different forecasters have slightly different styles.}
    \label{fig:synoptic}
 \end{center}
\end{figure}

Despite their accessibility, they are difficult to use as a training database and propose a project all to themself. Figure \ref{fig:synoptic} illustrates the cariety of drawing styles. While they are drawn on the same template (earlier templates are slightly different but anything within satellite observation interest would be consistent), the annotations vary and present a challenge for computer vision recognition. The color of red for prominences and flares can change. In the bottom right, the neutral magnetic lines do not extend to the solar limb, likely because of an uncertainty in their location, while in other figures they do. Coronal holes, indicated by bounaries with lines perpendicular to the boundary, are complicated since hatchings are drawn at variable widths and determining the interior of the the coronal hole relies upon that. Accompanying annotations are sometimes drawn next to features and other times drawn further away and linked by an arrow. Finally, these thematic maps do not detail features off the disk (except sometimes prominences) and do not include all the features (filaments) discussed in this thesis. If these challenges were overcome, the level of detail and care in the drawings makes them ideal for the labeled database. 

In the meantime, a more realistic goal is for forecasters to make a daily drawing using the annotation software. This is being pursued through a more accessible online tool with a funding proposal in the process of submission. 

\subsection{Prediction}
Potentially the most interesting and beneficial avenue for development is transitioning the classification paradigm to a prediction paradigm. There are many systems that predict solar flares within one to two days with fair success with machine learning techniques such as Bayesian methods \cite{predict1} and support vector machines \cite{predict2}. However, these systems are still far from satisfactory: "To date, various systems and models designed to predict the occurrence of solar flares have made significant progress, but the achieved prediction performances are far from what is required by operations teams" \cite{predict3}. Attempts at using CNNs have not been as successful, potentially because of insufficient data \cite{predict4}. However, that model is based off of sunspot counts instead of imagery. Using the approaches presented here with an added time domain and solar flare prediction label could be very productive. The annnotated data set in this thesis was too sparse for an investigation into prediction. 

\subsection{Machine learning with inconsistent database}
If further annotated images confirm the inconsistency observed here, a new approach will be necessary to rectify the inconsistent database issue. Sleeman et al. (2012) developed a protocol to increase consensus among experts in expert knowledgebases \cite{sleeman}. Furthering this work and developing a more robust classifier would have ramifications outside of solar image segmentation while improving performance. 

%%%%%%%%References %%%%%%%%%%
\bibliographystyle{acm}
\bibliography{references}
%%%%%%%% End References %%%%%%
\end{document}
