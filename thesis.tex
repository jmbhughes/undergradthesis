%
% Thesis template conforming to Williams College rules.
% Thanks to Ben Wood '08 and other contributors.
%
\documentclass[twoside]{report}
\usepackage[top=1.0in, bottom=1in, left=1.5in, right=1in, includehead]{geometry}
\pagestyle{headings}
\usepackage{setspace}
%% Special math fonts and symbols
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
%% Rotate tables and figures
\usepackage{rotating}
%% Used for TODO items
\usepackage{xcolor}
%% used for code listings.
\usepackage{float} 
%% Used to replace LaTeX's ugly emptyset with diameter, which looks nicer.
\usepackage{wasysym}
%% Nicely formatted algorithms.
\usepackage{algorithmicx}
\usepackage[chapter]{algorithm}
\usepackage{algpseudocode}
%% Nicely formatted listings.
\usepackage{listings}
%% More kinds of arrow with stuff

\usepackage{empheq}
\usepackage{multicol}
\usepackage{subfigure}
\usepackage{indentfirst}
%% 
%% Hughes additions
%%
%% set table width
\usepackage{array}
%% graphics path
\graphicspath{{figs/}}
%% hyperlinks
\definecolor{color1}{RGB}{0,0,90} % Color of the article title and sections
\definecolor{color2}{RGB}{0,20,20} % Color of the boxes behind the abstract and headings
\usepackage{hyperref} % Required for hyperlinks
\hypersetup{hidelinks,colorlinks,breaklinks=true,urlcolor=color2,citecolor=color1,linkcolor=color1,bookmarksopen=false,pdftitle={Solar Thematic Map Generation via Machine Learning},pdfauthor={J. Marcus Hughes}}
%% Used to set up better TODO items
% \usepackage[paperwidth=210mm,
%             paperheight=297mm,
%             left=50pt,
%             top=50pt,
%             textwidth=345pt,
%             marginparsep=25pt,
%             marginparwidth=124pt,
%             textheight=692pt,
%             footskip=50pt]
%            {geometry}
\usepackage{xargs}

\newcommand{\halpha}{H$\alpha$\,}
\newcommand{\todo}[1]{{\color{red}{\textbf{#1}}}}

%\usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}
%\newcommandx{\unsure}[2][1=]{\todo[linecolor=red,backgroundcolor=red!25,bordercolor=red,#1]{#2}}
%\newcommandx{\change}[2][1=]{\todo[linecolor=blue,backgroundcolor=blue!25,bordercolor=blue,#1]{#2}}
%\newcommandx{\info}[2][1=]{\todo[linecolor=OliveGreen,backgroundcolor=OliveGreen!25,bordercolor=OliveGreen,#1]{#2}}
%\newcommandx{\improvement}[2][1=]{\todo[linecolor=Plum,backgroundcolor=Plum!25,bordercolor=Plum,#1]{#2}}
%\newcommandx{\noshow}[2][1=]{\todo[disable,#1]{#2}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Thesis body %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Title page %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{titlepage}
$\;$
\vskip1.5in
\onehalfspacing
\begin{center}
{\LARGE
Solar Thematic Map Generation via Machine Learning
}
\large
\vskip.25in
by\\
J. Marcus Hughes\\
\vskip.125in
Professor Jon Park, Advisor\\
\singlespacing
\vskip.5in
\small
A thesis submitted in partial fulfillment\\
 of the requirements for the\\
Degree of Bachelor of Arts with Honors\\
in Computer Science\\
\vskip.5in
Williams College\\
Williamstown, Massachusetts\\
\vskip.5in
\today
%%\vskip.5in
%%{\Huge \textbf{DRAFT}}
\end{center}
\end{titlepage}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\tableofcontents
\listoffigures
\listoftables
\onehalfspacing
\chapter*{Abstract}
The new Solar Ultraviolet Imager (SUVI) instruments aboard NOAA’s GOES-R series satellites collect continuous, high-quality imagery of the Sun in six wavelengths. SUVI imagers produce at least one image every 10 seconds, or 8,640 images per day, considerably more data than observers can digest in real time. Over the projected 20-year lifetime of the four GOES-R series spacecraft, SUVI will provide critical imagery for space weather forecasters and produce an extensive but unwieldy archive. In order to condense the database into a dynamic and searchable form we have developed solar thematic maps, maps of the Sun with key features, such as coronal holes, flares, bright regions, quiet corona, and filaments, identified. Thematic maps will be used in NOAA’s Space Weather Prediction Center to improve forecaster response time to solar events and generate several derivative products. Likewise, scientists use thematic maps to find observations of interest more easily and guide experiments. 

Using a naive Bayesian classifier, a random forest, and a neural network, we create thematic maps in real-time. We created software to collect expert classifications of solar features based on SUVI images. Using this software, we compiled a database of expert classifications. These are used to establish experimental limits on human performance. Given new images, the classifiers assign each pixel the most appropriate label according to the trained distribution. Here we describe the software to collect expert training and the successes and limitations of the classifier. These results are promising and encourage future research into an ensemble classification approach and solar weather prediction with similar methods.

This abstract will be updated throughout the thesis process.

\chapter*{Acknowledgments}
I'd like to thank Dan Seaton and Jon Darnel for guiding me during this project and supplying data and annotations.  I'd like to thank Jay Pasachoff for his support in solar physics and discovery of this project. And, I'd like to thank Jon Park for helping me organize and embark on an independent thesis.

\chapter{Introduction}
\section{Motivation}
From August 28th to September 5th, 1859, the night sky nearly all over the world blazed with auroral displays for hours. Sources reported, ``there was another display of the Aurora last night so brilliant that at about one o’clock ordinary print could be read by light'' (The New York Times, New York Herald, Washington Daily National Intelligencer, September 2, 3, 5, 1859) \cite{green:2006}. Normally, aurora, a visible manifestation of material streaming in from the Sun and interacting Earth's magnetic field, are confined to polar regions. However, there were observations in New Orleans and even as far south as Honolulu, shown in Figure \ref{fig:carrington-spatial}  \cite{cliver:2004}. These displays were symptomatic of a massive solar event impacting Earth's magnetic fields. Consequently, telegraphs ceased to work, and human operators reported burns and other injuries as small fires started \cite{green:2006}. Events like this, while rare, are fairly periodic, with a 12\% probability of another within the decade \cite{riley:2012}. 

\begin{figure}[ht]
  \begin{center}
    \includegraphics[scale=1]{carrington-spatial.png}
    \caption{{\bf Spatial extent of Carrington Event}: As noted by Cliver and Svalgaard (2004) the Carrington event was observed to very low latitudes in the Americas. Closed circles represent overhead aurora; open circles represent visible aurora. The heavy curved line denotes the geomagnetic equator and the $\bigoplus$ symbol indicates the anti-Sun point. The lowest geomagnetic latitude at which the storm was observed was Honolulu (not shown).}
    \label{fig:carrington-spatial}
 \end{center}
\end{figure}

On March 13th, 1989, another geomagnetic storm, less than a third of the strength of the 1859 Carrington event, disrupted power in Canada and the United States, resulting in blackouts for the majority of Quebec for nine hours; the cost for repair was \$6 billion \cite{cervantes:2013a}. A much smaller event occurred in 2003, known now as the Halloween event \cite{muller:2014, viljanen:2014}. In preparation, satellites were placed into a stable stand-by mode. Ultimately, only one satellite was damaged with most satellites unscathed and exceeding their nominal lifespan. An extreme Carrington event was observed pointed away from Earth in 2012 \cite{eastwood:2017}. It is difficult to quantify the total cost; estimates of the damage a modern Carrington event would cause range from \$140 billion to \$3.4 trillion \cite{eastwood:2017}.

There are many much more frequent but less devastating solar events, discussed in Section \ref{sec:space-weather-events} that can cause damage to power grids, satellites, Earth communications, astronauts, and many other sensitive systems. While they cannot be prevented, advanced warning allows for preparation that can mitigate the damage. Coronal mass ejections, one danger resulting when the Sun spews large amounts of charged material sometimes towards Earth, have been recorded to reach speeds of up to 2000 miles per hour, reaching Earth within the day \cite{fastcme}. Within minutes to hours of a solar flare, the ionospheric disturbance can interrupt radio communications \cite{swap}. For proper safety protocols to be enacted, fast warning of an event is necessary.  This thesis explores solar feature classification through modern machine learning approaches, specifically the creation of a computer vision systems utilizing satellite imagery to identify solar activity in real time for quick response. These classification systems allow for real-time warning of space weather events.

\section{Goals}

This thesis has several key goals as outlined here:
\begin{itemize}
\item There does not exist a curated database of human annotated solar images. This thesis will present the first of its kind, comparing it to existing automated databases. In addition, this thesis will analyze the human labeling to understand agreement/disagreement between different annotators and consistency for each annotator. In order to create this database, the necessary labeling software will be created in this thesis.
\item This thesis will provide a suite of modern machine learning approaches to solar image classification with a random forest, Bayesian, and neural network implementation. These will be compared to existing solar classification approaches as well as each other to characterize their strengths, weaknesses, and overall performance.
\item Ultimately, high-quality solar image classification opens up avenues of research for solar physics. Thus, a prototype solar feature database will compiled from the images, indexing the images and allowing solar physicists to easily find interesting events. Further, a research application estimating the fractal dimension of active regions and properties of coronal hole will be presented.
\end{itemize}

\section{Organization}
Chapter \ref{ch:background} introduces the background information for solar physics. This includes a definition of the relevant solar structures to classify.  Then, Chapter \ref{ch:relatedwork} documents prior work in solar imge classification including both the unsupervised systems in Section \ref{sec:unsupervised} and superverised systems in Section \ref{sec:supervised}. The author's original contributions begin in Chapter \ref{ch:data}, after a description of the solar imagery, with an overview and analysis of the human annotated images. Chapter \ref{ch:classifiers} details the classifiers tested in this approach through experiments in Chapter \ref{ch:experiments}. Two applications of the solar classification are explored in Chapter \ref{ch:application}: the labeled solar database in Section \ref{sec:database} and fractal dimension estimastion for solar features in Section \ref{sec:fractal}. Finally, Chapter \ref{ch:conclusion} outlines the results of the entire project and potential future work. 

\chapter{Solar Physics Background} \label{ch:background}

As mentioned, space weather has dangerous and expensive consequences including harm to astronauts and satellites, destruction of power grids, and routine rerouting of intercontinental flights over the north pole. These different effects stem from a variety of solar events. These are outlined in this section with a description of the structure of the varying phenomena.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[scale=0.4]{sunparts}
    \caption{{\bf Parts of the Sun}: This image indicates many of the parts of the Sun. The top cutaway portion shows the internal structure of the Sun, not discussed in this thesis. The bottom portion shows many of the features discussed here which comprise the solar atmosphere. The solar corona is the uppermost portion of the Sun which is millions of degrees Celsius. From this and the cooler and lower photosphere and chromosphere, the space weather emanates. Several different features discussed in Section \ref{sec:solarstruct} are shown, for example prominences, flares, and coronal holes \cite{sunparts}.}
    \label{fig:sunparts}
 \end{center}
\end{figure}


\section{Space Weather Phenomenon} \label{sec:space-weather-events}
The Sun is responsible for many phenomena in daily life: heat, light, even life itself is traced back to the Sun. In 1733, Jean Jacque d'Ortous de Mairan proposed an explanation for aurora: it's from solar gas entering Earth's atmosphere \cite[p. 51]{langbook}. Later in the 20th century, Birkeland argued that it's fom Earth's magnetic field focusing electrons around Earth's poles \cite[p. 51]{langbook}. Today, the Sun's magnetic field is a recognized as the driver of many of its internal and visible properties: the cycle of sunspots and its temperature to name a couple. In 1958, Eugene Parker advanced a model for the Sun's impact on Earth via the solar wind, the pressure caused from outflowing solar material, with a theortical model where the wind propogates as a spiral into interplanetary space from the Sun's rotation and radial flow \cite[p. 62]{langbook}. This is now referred to as the Parker Spiral in Figure \ref{fig:parkerspiral}. 

\begin{figure}[ht]
  \begin{center}
    \includegraphics[scale=0.6]{parkerspiral}
    \caption{{\bf Parker Spiral}: The Parker Spiral explains how events on the Sun propogate outward. Events on one region of the Sun have a direct line along the Parker Spiral toward Earth while events on another portion will miss Earth \cite{spiralimage}}
    \label{fig:parkerspiral}
 \end{center}
\end{figure}

The solar wind continuously is blowing millions of tons of material away from the Sun as a stream of rarefied plasma. When it reaches Earth's orbit, it's traveling at roughly 600 km/s  with 10 million particles per cubic meter \cite[p. 67]{langbook}. As the Sun goes through an 11-year cycle, this stream of material shifts from stronger to weaker and back again. During the most energetic portion of the cycle, there are frequent sudden and rapid outpbursts called solar flares. These release energy equivalent to million of 100-megaton bombs exploding at the same time over less than ten minutes \cite[p. 89]{langbook}. These solar flares hurl high energy particles along the Parker Spiral into interplanetary space at nearly the speed of light. The strongest of these flares can cause "planet-wide radio blackouts and longlasting radiation storms" \cite[p. 255]{langbook}. 

Flares are often accompanied by coronal mass ejections (CMEs), although CMEs and flares can occur independently of each other. These eruptions send roughly 10 billion tons of solar material into interplanetary space. These have been reported to reach speeds of up to 2000 miles per hour, reaching Earth within the day \cite{fastcme}. Smaller outbursts called "filament lift offs" can also release material towards Earth. Beyond these eruptions, there is variable strength solar wind constantly baragging Earth. Wind from coronal holes, discussed shortly, is faster and can be more of a constant threat. 

\section{Solar Structures}\label{sec:solarstruct}
Figure \ref{fig:sunparts} showed some of the different parts of the Sun. In Table \ref{tab:solarfeatures} there is a listing of some of the more prominent solar features with a brief description of them. 

\begin{table}[ht!]
\centering
 \begin{tabular}{||c m{0.5\textwidth}||} 
 \hline
 Name & Description \\
   \hline\hline
   Active regions & Complexes of brighter and darker regions in UV observations caused by the solar magnetic field piercing through the solar atmosphere \\ \hline \\
   Filaments & A suspension of material high in the solar atmosphere by magnetic arches\\ \hline \\
   Prominences & A filament observed off the disk of the Sun \\ \hline \\
   Coronal holes & Regions where the magnetic field is open with no clear reconnection back into the Sun which allow fast outflow of material \\ \hline \\
   Flares & A sudden brightening on the Sun \\ \hline \\
   Coronal mass ejection & An event when the Sun dispels mass and charged particles, sometimes towards Earth \\ \hline \\
   Sigmoids & S-shaped structures on the surface of the Sun thought to be precursors to flares \\ \hline \\
   Quiet corona & Parts of the Sun devoid of activity \\ 
 \hline
 \end{tabular}
 \caption{Solar phenomena: This is a short description of some of the solar events related to space weather. }
 \label{tab:solarfeatures}
\end{table}

Active regions will often be called bright regions in this work. Bright regions is a broader term used in space weather forecasting while active regions is the more technical description. 

\chapter{Related work} \label{ch:relatedwork}
This chapter surveys some of the underlying work on solar image classification. 

\section{Synoptic Charts and Thematic Maps}
When classifying space weather phenomena, it is important to understand its context: where it is on the solar disk, when it developed, and its strength. The Sun has a consistent radial outflow of material at approximately 400 km/s as it rotates on its axis every 25 days. This results in magentic field lines in a spiral pattern called the ``Parker Spiral.'' As such, an ejection of material from the Sun on the eastern side of the Sun will reach Earth quicker than an ejection from the western side of the Sun.  Thus, knowing the both the location of the event and the time of the event is very important when considering when the impacts will be felt on Earth. A detailed understanding of the strength of the event and exigent conditions is required to estimate the impact on Earth when it does arrive.

The necessary information can be summarized in a solar synoptic chart. Solar synoptic charts (also referred to as solar thematic maps) detail the solar activity at any given moment using a labeled image of the Sun. While not reponsible for synoptic maps' invention Song et al. (2015)  describe the necessary components of synoptic charts \cite{song:2015}. The synoptic chart must be produced in real-time so that space weather forecasters can read and respond as needed. It must be quantitative when describing observations and object boundaries so that the information can be used in other follow-up systems such as expert validation and database generation. Finally, it must be comprehensive, providing more than sufficient information and easy to examine images of the Sun at various important solar atmospheric heights and temperatures. Based upon a literature study, Song et al. (2015) argued that magnetogram and extreme ultraviolet (EUV) imagery are most valuable for general solar event classification \cite{song:2015}. They created a database of 1586 space weather papers and investigated which types of solar phenomena are correlated to different wavelengths in modern research. After analyzing these trends for active regions, coronal holes, filaments/prominences, flares, and coronal mass ejections, they found that for all the categories 87.4 \% used magnetograms and 59 \% used extreme ultraviolet images. Thus, these two types of data should be featured most prominently on any synoptic charts.

At the moment, reliable synoptic charts used in forecasting are predominantly human drawn. There are existing automatic classifiers, but they often only detail one type of feature. Space weather forecasters at NOOA's Space Weather Prediction Center (SWPC) still hand draw synoptic maps daily, outlining magnetic field lines, coronal holes, flares, filaments/prominences, and plages. Historical maps are \href{http://www.swpc.noaa.gov/products/solar-synoptic-map}{available in PDF format} back until 1972. In a future project, this could serve as an interesting source of labeled data, especially for the difficult task of finding magnetic neutral lines. Zheng et al. (2016) utilized similar synoptic drawings from Yunnan Observatory to extract text annotations about sunspots with a convolutional neural network \cite{zheng:2016}. Some observatories are moving toward automated feature classification. For example, when the person in charge of synoptic maps at the Meudon Observatory was set to retire, they implemented a filament classifier and tracker \cite{meudon:2007}. 

\begin{figure}[ht]
  \begin{center}
    \includegraphics[scale=0.25]{ssc_example-song-2015.png}
    \caption{{\bf Synoptic map example}: Song et al. (2015) propose the solar synoptic map to include a labeled composite image with different wavelength images accompanying to provide a full image of solar activity \cite{song:2015}. }
    \label{fig:sccexample}
 \end{center}
\end{figure}

\section{Data sources}
\subsection{Raw imagery to classify} 
For solar image classification many types and sources of images can be utilized. While brand new imagers like the Solar Ultraviolet Imager (SUVI) do not yet have public data repositories, Sunpy, a python package for solar physics, provides an interface for the Virtual Solar Observatory's large database of solar images \cite{sunpy}. Unsupervised approaches to the problem use this tool and others like it to fetch \halpha, ultraviolet, x-ray, white light, magnetic, and other forms of solar images. They utilize from the Solar Dynamics Observatory, the Solar and Heliospheric Observatory, Solar Terrestrial Relations Observatory, Transition Region And Coronal Explorer, the Global Oscillation Network Group, and many other sources. There is continuous imagery of the Sun at many wavelengths and heights; raw data is generally not a limitation. Instead, the limitation is incorporating raw imagery into a classifier or obtaining labeled data for a supervised trainer. 

\subsection{Labeled data for training} 
The supervised techniques discussed in Section \ref{sec:supervised} require labeled data to train their classifiers. A research group formerly at Montana State University and now at Georgia State University has collated large amounts of images with labeling from unsupervised classifiers. The original dataset in 2013 comprised over 15,000 images with 24,000 events observed in the first half of 2012 by the Solar Dynamics Observatory (SDO). Using the SDO unsupervised classifiers, small grid regions of the image were labeled active region, coronal hole, filament, flare, sigmoid, and sunspot. Each grid region is also statistically analyzed and assigned an entropy, mean, standard deviation, fractal dimension, skewness, kurtosis, uniformity, relative smoothness, contrast, and directionality measures \cite{schuh:2013}. This data was later incorporated into a database tool that allows the user to identify an example image and query the full dataset for similar events \cite{banda:2014}. This dataset and tool was later expanded to the full SDO observing database \cite{schuh:2016}. 

\section{Solar machine learning}
Solar machine learning can be divided into two categories: unsupervised and supervised. Unsupervised techniques do not required human input of labeled images but instead often run on rules; this approach is much more common in astronomy. Supervised techniques are often more flexible and outperform unsupervised techniques in other related fields \cite{anzanello:2014, yu:2013,guerra:2011, huang:2017}.

\subsection{Unsupervised solar segmentation} \label{sec:unsupervised}
Unsupervised solar segmentation can be broken into many approaches: edge-based algorithms, region-based algorithms, hybrid algorithms, and artificial intelligence approaches. The first three categories are more image-processing techniques solely while artificial intelligence approaches are more generic examples of clustering, support-vector machines, and other tools. These can be used in tandem with the pure image processing techniques. 

\subsubsection{Edge-based algorithms}
Edge-based techniques utilize discontinuities and identify different features utilizing boundaries. Curto, Blanca, \& Martinez (2008) employed edge-based unsupervised detection when identifying sunpots in \halpha images \cite{curto2008automatic}. Since sunpots have crisp boundaries their edges can be used to quickly identify them. Curto, Blanca, \& Martinez (2008) used morphological operations to emphasize these boundaries: erosion, dilation, opening, closing, and the top hat transformation. Erosion shrinks bright regions by removing boundary pixels while dilation grows them. Dilation will also fill holes in features. Opening is an erosion followed by a dilation while closing is dilation followed by erosion. Both closing and opening smooth the image: opening fills shape holes, whereas closing breaks wide lines and erases thin lines \cite{curto2008automatic}. The top hat transformation subtracts the original image by the closing of the image. It results in an image showing only the erased parts. By stringing together an empirically determined set of operations, they were able to reliably identify sunpots. Qu et al.(2005) ave a similar system that identifies filaments \cite{qu2005automatic}. 

\subsubsection{Region-based methods}
Region based approaches include histogram segmentations, clustering/thresholding, and region-growing approaches. Fuller, Aboudarham, and Bentley (2005) implemented a filament classifier using region growing \cite{fuller:2005}. This work is based off more generic computer vision region growing by Gonzalez \& Woods (2002) \cite{gonzalez:2002}. After calibrating, removing dust, and sharpening the solar \halpha images \cite{standardization}, seed pixels are chosen for region growing using a thresholding technique. Only the dimmest pixels are chosen since they should be at the center of filaments. For multiple iterations, the region grows adding new pixels that are connected to seeds and follow the mean and standard deviation of the neighborhood and consequently are similar to the seed. Finally, a morphological closing operation is applied to remove any holes and make the filaments smoother. After this, the center line of the filament can be determined and characterized using a combination of convolutions, dilations, and erosions. This characterization makes it easier to track filament evolution and measure their length. Ultimately, this technique produced 1149 filaments compared to a human labeled 1232 filaments \cite{fuller:2005}. This resulted in missing 10\% of the filaments in an image. Roughly 5\% of the detections were false positives, keying on sunspots instead of filaments because there was no spatial requirement for filaments to be long and skinny. Thus, they could be confused for sunspots which are also dark in \halpha images. Other region growing methods include: Benkhalil et al.(2006) which used ionized calcium, \halpha, and extreme ultraviolet imagery to grow active region boundaries \cite{benkhalil}, Higgins et al. (2011) which combined magnetograms, image differencing, and region growing to identify and track emerging active regions \cite{higgins}, and McAteer et al. (2005) which used full-disk magnetograms to identify magnetically significant regions and characterize their flare potential \cite{mcateer}. 

Instead of growing regions, one can identify significant features by looking at the histogram of their intensities in various wavelengths. Olmedo et al. (2008) designed such a system to identify coronal mass ejections. The intensity in solar images can be plotted as a histogram as a function of position angle as shown in Figure \ref{fig:olmedo}. A threshold is used to determine what is a significant event in the histogram. If portions of the histogram exceed this, they are declared a region in the image and grouped together. Some region growing is also used in this approach. Ultimately, they were able to recover about 75\% of the human identified coronal mass ejections in a 12-month period. Interestingly, they found an equal number of small coronal events that had been overlooked by humans, often weaker but creating an interesting new population for scientific research and space weather awareness \cite{olmedo2008automatic}. 

\begin{figure}[ht]
  \begin{center}
    \includegraphics[scale=1]{olmedo.png}
    \caption{{\bf Histogram segmentation}: Olmedo et al.(2008) utilized histogram segmentation to identify coronal mass ejections. ``The intensity profile along the angular axis showing the 1D projection of the CME image. Only positive pixels along the radial axis are used. This profile effectively indicates the angular positions of a CME when it is present.'' \cite{olmedo2008automatic}}
    \label{fig:olmedo}
 \end{center}
\end{figure}

Other histogram based methods include Preminger et al.(2001) which used contrast ratios between ionized calcium and magnetograms to identify sunspots and faculae \cite{preminger}.

\subsubsection{Hybrid}
Some approaches bridge between region-based and edge-based techinques. One key example is SPoCA, a fuzzy classifier that uses multiwavelength extreme ultraviolet observations to assign multiple classes: quiet Sun, active region, and coronal hole. SPoCA is more accurately a suite that implements three types of fuzzy clustering algorithms tailored to the segmentation of solar coronal EUV images: Fuzzy C-means (FCM),  Possibilistic C-means (PCM) algorithm, and Spatial Possibilistic Clustering Algorithm (SPoCA) \cite{verbeeck2014spoca}. It differentiates itself from other techniques because of its reliance on fuzzy logic. Barra et al. (2008) note that the use of fuzzy logic allows the algorithm to overcome noise in the images and the scientific definitions of solar features while meeting many needs. Often, unsupervised solar segmentation techniques are developed with a specific research question and thus are very restricted. For example, one classifier may focus only on the brightest cores of active regions to study energy transfer while another may be designed to catch active regions as they form and thus catch many weak active regions. SPoCA is generic enough to overcome this conflict.

SPoCA works by utilizing the assumptions of fuzzy logic. Each pixel has membership in all of the different classes used in the thematic map. The sum of all the memberships for a pixel must be one, i.e. the pixel membership indicates how much the pixel matches that theme. In fuzzy c-means \cite{cmeans}, a generalization of K-means clustering to fuzzy logic, the variance within a cluster, all the pixels labeled a theme, is minimized. This approaches is very susceptible to the noise in astronomical images though \cite{krishnapuram1993possibilistic}. Thus, the fuzzy c-means requirement of membership summing to one is relaxed to form of possibilistic c-means \cite{krishnapuram1996possibilistic}. SPoCA is a further modification that incorporates weighting by spatial extent such that neighboring pixels should be assigned similar labels \cite{barra:2008, barra:2009}. 

Barra et al. (2009) used SPoCA to both segment and track features on the Sun \cite{barra:2009}. They exercised the power of SPoCA to perform two very different experiments. First, they tracked the biggest active region for a month, quantifying its size, average intensity, fractal dimension, and other parameters for scientific inquiry. Then, they identified and tracked coronal bright points, a feature not initially intended in SPoCA. These short lived ($< 2$ days) bright regions have some spectral similarity to active regions but are much smaller and can appear within coronal holes. They impact the structure and dynamics of the solar corona. During the study, Barra et al. (2009) tracked their counts, north/south asymmetry, intensity fluctuations, and other parameters \cite{barra:2009}.  

There are other types of unsupervised classifiers that do not fit nicely into the two main categories. For example, Bratsolis \& Sigelle (1998) utilized mean field fast annealing to segment sunspots \cite{bratsolis1998solar}. The approach uses simulated annealing to minimize the classification into $q$ classes. Each pixel is assigned a label. This labeling has an energy described by mean field theory and the Potts interaction between pixels. Essentially, this approach attempts to find the most meaningful classification. It excels over histogram methods which often are not granular enough to separate regions of different activity in the sunspot.

\subsubsection{Example system: Solar Dynamics Observatory}
Many unsupervised approaches that only deal with one class at a time can be chained together to create all the necessary data for a thematic map. The Solar Dynamics Observatory (SDO) satellite mission produces 1.5 TB of imagery per day in multiple ultraviolet wavelengths, a magnetogram, and other data channels. To deal with this influx of data, teams of researchers developed classifiers that identified specific classes of features \cite{sdo}. All of the component parts can be seen in Figures \ref{fig:sdo1} and \ref{fig:sdo2}.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[scale=0.5]{sdo1.png}
    \caption{{\bf First half of SDO classifiers} \cite{sdo}}
    \label{fig:sdo1}
 \end{center}
\end{figure}

\begin{figure}[ht]
  \begin{center}
    \includegraphics[scale=0.5]{sdo2.png}
    \caption{{\bf Second half of SDO classifiers} \cite{sdo}}
    \label{fig:sdo2}
 \end{center}
\end{figure}

The SDO suite suite is a comprehensive approach to classification that utilizes all of the techniques mentioned thus far and more. For features that are beyond the individual classifiers' scope, a trainable module is employed. While this is a supervised system, it is mentioned here to emphasize the difference between supervised and unsupervised approaches. A user can identify a specific type of feature they are interested in by identifing them within an image. These are placed in feature vectors of 12 texture paramters (for example mean, entropy, uniformity). These train either a support vector machine or a C4.5 decision tree and will then identify similar features from the rest of the database \cite{lamb2008example}. The SDO suite lacks the ability to combine all the classifications into a single thematic map. 

\subsubsection{Comparisons}
\begin{figure}[ht]
  \begin{center}
    \includegraphics[scale=1]{revathy.png}
    \caption{{\bf Performance of different methods}: Revathy, Lekshmi, \& Nayar (2005) compared the performance of different segmentation techniques in identifying active regions \cite{revathy}. At left is the result of a histogram thresholding approach while at the right is fuzzy-based segmentation. }
    \label{fig:revathy}
 \end{center}
\end{figure}

As shown in Figure \ref{fig:revathy}, different methods produce often similar but still different results. By computing the fractal dimension over the segmented image, they were able to characterize the difference between fuzzy clustering, region growing, iterative thresholding, and histogram thresholding. They found that depending on the height of the active region in the solar atmosphere it was segmented differently. In general, the fuzzy-based and histogram approaches outperformed the others. They propose that using longer wavelength ultraviolet images tends to larger area active regions.

Caballero \& Aranda (2013) conducted an independent comparison of unsupervised techniques for active regions \cite{caballero}. Using 6000 images from SOHO in 195 angstroms, they initially segmented the images using region growing techinques. Then, the different independent regions were clustered together into units using either partition approaches or a hierarchial classification. They found that the hiearchial classification, the idea that nearby regions should be more strongly connected, produced more human-like clusters. However, this approach requires exponential time complexity and results in a hierarchy instead of a simple division into clusters.

For a much longer comparison of many different techniques see Aschwanden (2010) \cite{aschwanden:2010}. 

\subsection{Supervised solar segmentation} \label{sec:supervised}
There are relatively few supervised solar segmentation approaches but the existing approaches are very promising and tend to perform on a broader set of classes and image types. Thus, a bit more detail is provided in describing each approach. 

\subsubsection{Maximum likelihood estimation}\label{sec:riglermaxlikelihood}

Rigler et al. (2012) proposed a preliminary naive Bayesian classifier for SUVI images using the Solar Dyanmics Observatory (SDO) Advanced Imaging Array (AIA) observations for testing \cite{rigler:2012}. Their work focused on eight classes: flare, prominence, active region, quiet corona (off-disk), quiet corona (on-disk), coronal hole (off-disk), coronal hole (on-disk), and outer space. They were able to achieve an average accuracyof 86\%. This was calculated by training the classifier on a set of data then testing it on a classified image that was not used for training. The tabulation was done strictly on a pixel by pixel basis, not checking if the error was coherent or random noise. Prominence was the most problematic class with only 41\% of prominence pixels being classified correctly. They were often misclassified as off-disk quiet corona or on-disk coronal hole.

The naive Bayesian approach works by classifying every pixel into one of $n$ classes using multispectral ultraviolet images. Therefore, a pixel, a spatial element at $(i,j)$ corresponds to $h$ channels and can be described as a vector:
\[ x_{(i,j)} = \begin{bmatrix} x_1 & x_2 & \hdots & x_h \end{bmatrix}^T \]
The approach is assign each $x_{(i,j)}$ pixel a label $w_k$ from the set of classes $W$. This approach employs Bayes' Theorem:
\[ P(w_k | x_{(i,j)}) = \frac{P(x_{(i,j)} | w_k) P(w_k)}{P(x_{(i,j)})} \]
Since $P(x_{(i,j)})$ is not a function of the label classification, it can be ignored.
\[ P(w_j | x_{(i,j)}) \propto P(x_{(i,j)}) P(w_{k}) \].
Rigler et al. (2012) simplify this even further by stating that ``if there is no a priori reason to believe a pixel should be assigned label $w_k$, $P(w_k)$ can be assumed to be drawn from a uniform distribution''. Thus,
\[ P(w_k | x_{(i,j)}) \propto P(x_{(i,j)} | w_k) \]
This approach is the maximum likelihood solution to this problem.

For training they simplify each class into a multivariate normal, i.e. for each potential label there is a archetypal example pixel and all pixels with that label should be distributed normally about it. This is overly constraining if any given class has multiple distinct modes with respect to the selected data. The multivariate distribution for class $w_k$ is characterized by a mean vector $\mu_k$ and covariance matrix $C_k$ which are calculated as:
\[ \mu_k = \frac{\sum_{x \in W_k} x}{|W_k|} \]
where $W_k$ is the collection of pixels with label $w_k$. Similarly,
\[ C_k = \frac{\sum_{x \in W_k} \begin{bmatrix} x - \mu_k \end{bmatrix} \times \begin{bmatrix} x - \mu_k \end{bmatrix}^T}{|W_k|} \].
These mean vectors and covariance matrices characterize the class. Given this characterization for class $w_k$ one can calculate the conditional probability of a pixel $x_{(i,j)}$ having label $w_k$:
\[ P(x_{(i,j)} | w_k) = \frac{1}{\sqrt{(2 \pi)^h} \sqrt{|C_k|}} \mathrm{exp} \left( \frac{-1}{2} \times \left(x_{(i,j)} - \mu_k \right)^T \times C_k^{-1} \times \left(x_{(i,j)} - \mu_k \right) \right) \]
Thus, the pixel is assigned the class that maximizes this probability.

Since each pixel is treated separately, any noise in single pixels or in the image as a whole can result in a noisy classification where a pixel class does not agree with its neighboring pixels as expected. Therefore, Rigler et al.(2012) propose a smoothness prior be enforced so that a pixel's labeling relies on its neighbors This can be enforced by iteratively calculating the thematic map, calculating a smoothed map relying on neighbor probabilities, and repeating until convergence using simulated annealing, maximizing posterior marginals, or iterated conditional modes as proposed by Tso and Mather (2009) \cite{tso:2009}. 

Their results were promising with high accuracies and maps that generally coherent. However, their results are concerning because some statistics and accuracy measurements come from running the classifier on training data, providing no indication on how the classifier would perform on unfamiliar, real-world examples. 

Rigler et al.(2012) built upon earlier work by de Wit (2006) who suggested the Bayesian approach \cite{dewit:2006}. After decreasing the noise and normalizing the intensity in each image, de Wit(2006) instead used only four ultraviolet wavelengths and projected them into a three-dimensional parameter space using singular-value decomposition \cite{dewit:2006}. Thus, de Wit(2006) ran a naive Bayesian classifier on these transformed parameters instead of the higher dimensional wavelengths. Figure \ref{fig:dewit} shows an example result by de Wit(2006), emphasizing the coherence of this segmentation without any forced smoothness \cite{dewit:2006}. This approach ran in near real-time, taking only a few minutes to classify every pixel. 

\begin{figure}[ht]
  \begin{center}
    \includegraphics[scale=1]{dewit.png}
    \caption{{\bf de Wit Segmentation}: This figure indicates the power of quick, multi-spectral, supervised segmentation done by de Wit (2006), very similar to other work by Rigler et al.(2012) and Visscher et al.(2015) \cite{dewit:2006}. The classes for this study are: ``(1) Tenuous corona outside of the disk, in regions with open magnetic-field lines.(2) Dense corona outside of the disk. (3) Coronal holes. (4) Quiet sun, including the chromospheric network and regions inside the network boundaries. (5) Active regions on the disk'' \cite{dewit:2006}. }
    \label{fig:dewit}
 \end{center}
\end{figure}

\subsubsection{Maximum A Posteriori}
Visscher et al. (2015) improve upon Rigler et al. (2012)'s result by recognizing that the classes are not equally likely (for example the majority of the Sun is covered by quiet corona at any given time) and that crisp segmentation procedures are too limiting compared to fuzzy segmentation \cite{visscher:2015}. Further, they incorporate spatial information by letting the probability of a class rely on both the intensity and latitude, assuming these are statistically independent variables so $p((x, L(x)) | w_k) \approx P(x|w_k) P(L(x)|w_k)$ where $L(x)$ is the latitude of the pixel and $w_k$ is a class. This addresses the observable fact that coronal holes tend to form at solar poles, high latitudes, while active regions form near the equator. Unlike Rigler et al. (2009), Visscher et al. (2015) use only one wavelength of light in their dataset: the 19.3 nm SDO-AIA channel. Additionally, they only assign three classes: active region, coronal hole, and quiet sun. This makes it difficult to directly compare their results because the troubling classes Rigler et al.(2009) observed are not included.  Therefore, the probability they are maximizing is instead: 
\[P(x|w_k)p(L(x) | w_k) p(w_k) \]
While doing this, they have assumed a fuzzy segmentation that allows for degrees of membership in each class. This approach allows estimation of how certain different classes are under different conditions. For example, they confirmed that wrongly classified pixels were classified as half one class and half another class and often on the object boundaries. In adition, they were able to pinpoint that there is disagreement with the gold standard reference. Upon second examination, they find that the human assigned labels may actually be wrong in this region, highlighting a key problem in solar segmentation: there is no clear universally agreed upon definition of some of the classes so no human classification can be accepted as completely correct. Even if a human standard does exist, it is sometimes still impossible to discern between classes due to degeneracy in the observational parameters. Ultimately, they report average 94\% accuracy over all classes.

In addition to accuracy with a single frame, Visscher et al.(2015) establish a criteria for accurate segmentation into large-scale features \cite{visscher:2015}:
\begin{itemize}
\item ``Stable segmentations on short timescales in the absence of major solar activity''
\item Consistent and smooth trends and classifications over longer periods of time
\item Consistency with human drawn maps
\end{itemize}
While there is no quantified method for the first two criteria, they are highly relevant and often not addressed in other approaches explicitly. 

\subsubsection{Neural Networks}
Deep convolutional neural networks have proven to be very skilled in classifying and segmenting in various contexts \cite{szegedy:2015, tso:2009, krizhevsky:2012}. Convolutional neural networks are specifically designed for image data such as solar images. Each layer extracts local features from an image using a kernel which are combined in intermediate pooling layers. This allows for a robust classification with respect to distortions or noise in the images. Activation functions allow only significant features to influence the final classification. Deep learning approaches are advantageous because they automate feature selection by weighting input data according to their training algorithms instead of having a scientist develop detailed rules about what data components indicate which classes.

Kucuk et al.(2017) applied the first convolutional neural network to solar imagery for classification \cite{kucuk:2017}. While classifying over a finer granularity of classes they were able to achieve an average 70\% accuracy across each class. This convolutional neural network approach outperformed the only other published neural network solar segmentation found during this review. Zharkova \& Schetinin (2005) employed a feed-forward neural network with two hidden neurons and one output neuron to identify solar filaments at 82\% accuracy \cite{zharkova:2005}. This result is not directly comparable since it only classified one type of feature. However, it illustrated the power of neural networks in solar images. Filament classification by classical image processing techniques is often confused by the highly variable background between different parts of a filament and from filament to filament. The artificial neural network was able to flexibly learn many patterns and more accurately identify filaments. At the time, it was only outperformed by a region-growing approach \cite{fuller:2005}. 

Up until now, classification has only been discussed in a spatial domain. However, flares and coronal mass ejections have a temporal components. They are by definition changing features. Borda et al. (2002) implemented a simple neural network consisting of two layers (not including input): a hidden layer of nonlinear neurons and
an output layer of one linear neuron \cite{borda2002automatic}. Given optical \halpha images, it identifies solar flares in real-time. It operates on 7 input features: mean image brightness, standard deviation of the brightness, the pixel of maximum brightness change between images, absolute brightness of pixel with maximum change, radial position of that pixel, variation of mean brightness between two images (to characterize possible weather influences), and the contrast between the pixel with largest change in brightness and its neighbors. Given 124 test events, fewer than 5\% were misidentified. (The paper does not make clear about false positives and false negatives.) Accounting for normal operations, this would be a misclassification every few days. There has been limited solar time series neural networks beyond this, but it establishes a baseline system for future architectures and generalizations to other feature types.

\section{Earth Remote Sensing}
Earth remote sensing of multiwavelength features has many more applications and a longer availability of data and thus has advanced further than solar machine learning techniques. 

This author could not find an example of random forests for solar image classification. However, they are routinely used in Earth remote sensing classifications. Random forests are an ensemble of tree classifiers. To classify a new feature vector, the input vector is classified with each tree in the forest, and the forest chooses the classification having the most votes over all the trees in the forest. Random forests have many advantages: high accuracy compared to current algorithms, efficient implementation on large data
sets, and an easily storable data structure for future use \cite{ghose2010decision}. Lowe \& Kulkarni (2015) used a random forest to identify terrain type in hyperspectral images \cite{lowe:2015}. For this application, the random forest had 96.25 \% accuracy compared to neural network's 76.87\%, support vector machines 86.88\%, and maximum likelihood's 83.11\% \cite{lowe:2015}. This high performance for random forests with this type of problem is not uncommon \cite{puissant2014object, salas2016multispectral, clark2016mapping, kulkarni2017multispectral}.

Similarly, neural networks have a rich tradition in Earth remote sensing. Lee \& Kwon (2017) developed a 9 layer convolutional neural network, both wider and deeper than state-of-the-art methods for this problem, to classify land types in Earth remote sensing \cite{lee2017going}. This network achieves over 95\% accuracy in nearly every class. It is specially designed for spectral-spatial data and explores neighborhood relationships in a more optimized fashion that previous networks by allowing for multi-scale examination.

Li et al. (2014) present a comprehensive review of Earth remote sensing classification techinques based on spatial techniques \cite{li2014review}. They detail the usage of K-means, ISODATA, SOM,
hierarchical clustering, Maximum likelihood, Minimum distance-to-means, Mahalanobis distance, Parallelepiped, k-nearest Neighbors,  artificial neural network, classification tree, random forests, support vector
machine, genetic algorithms, Fuzzy classification, neural networks, regression modeling, regression tree analysis, spectral mixture analysis, fuzzy-spectral mixture analysis,  and image segmentation and object-based image
analysis techniques in Earth remote sensing. 

\section{General Computer Vision}
Cutting edge computer vision research can be applied to the solar segmentation problem. The solar segmentation problem and producing thematic maps is an example of semantic segmentation, a well studied problem in computer vision. However, it is difficult to apply these techniques to solar image segmentation because they require large labeled datasets. 

\subsection{Fully convolutional neural networks}
The notion of extending convolutional neural networks to do dense prediction, effectively creating a thematic map, was first proposed by Matan et al. (1991) \cite{matan} to extend the LeNet convolutional neural network \cite{lenet} for handwritten digit recognition. Shelhamer, Long, \& Darrell (2016) presented a new implementation, the fully convolutional neural network (FCN), that takes arbitrarily sized input and creates a similarly sized semantic segmentation \cite{fcnn}. During their development, they thoroughly describe convolutional neural network for semantic segmentation up until 2016 \cite{fcnn}.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[scale=1]{fcnn.png}
    \caption{{\bf Fully convolutional neural network}: This is an example architecture for a fully convolutional neural network (figure 1 from Shelhamer, Long, \& Darrell (2016) \cite{fcnn}).  }
    \label{fig:fcnn}
 \end{center}
\end{figure}

A FCN works by using existing full neural networks in a convolutional fashion. Then, the last steps of the existing network are removed so that it cannot make a classification for the entire input image at that time. Instead, a pixelwise prediction layer is added using deconvolution and striding over the input image. Without specialized refinement, the FCN can then create a dense output map. At the time, the FCN gave 20\% improvement over state-of-the-art semantic segmentation in a shorter inference time.

\subsection{Mask R-CNN}
He, Gkioxari, Doll{\'{a}}r, \& Girschick developed Mask Regional Convolutional Neural Network (Mask R-CNN), an extension of Faster R-CNN \cite{ren:2017} that creates another form of a semantic segmentation \cite{he:2017}. Technically, Mask R-CNN is a type of FCN \cite{fcnn}. Faster R-CNN worked by quckly determining bounding boxes for various objects in a scene. Mask R-CNN extends this by determining a pixel mask for each bounding box in parallel. Thus, it can distinguish both the type of an object and between neighboring objects in a scene. 

\begin{figure}[ht]
  \begin{center}
    \includegraphics[scale=0.5]{mrcnn.png}
    \caption{{\bf Mask regional convolutional neural networks}: The top row was the existing state-of-the-art instance segmentation \cite{li2016fully}, an example of a FCN, compared to the bottom row of Mask R-CNN performance on the same scene. The overlaid coloration indicates the segmentation while bounding boxes indicate where Mask R-CNN evaluated these masks. Clearlly, the Mask R-CNN produces more coherence classifications. In addition, it runs in less time with higher accuracy than the existing state-of-the-art systems and was consequently awarded the 2017 International Conference on Computer Vision best paper award.}
    \label{fig:mrcnn}
 \end{center}
\end{figure}

The Mask R-CNN design was tested in several domains: cityscapes, human pose estimation, common objects. In all cases, it outperformed existing systems. With modification, this type of architecture solves the problem of solar segmentation. 

\section{Summary}
Solar image segmentation is well motivated by space weather and archival concerns. Up until now, there have been numerous unsupervised approaches, too many to completely document in this paper. These approaches employ a variety of computer vision techniques but are often limited to only determining membership in one class, for example determining which pixels are filament. They can be used in tandem to create solar thematic maps that label all classes on the Sun in an ensemble classifier. Settling disputes between the independent classifiers can be difficult and running them all in parallel can be costly. Instead, machine learning techniques can be applied to label the entire image at one time. A few supervised approaches have been developed and perform well. However, there is limited quantification and even more limited consistent quantification of that performance making it difficult to compare systems. By looking to Earth remote sensing and state-of-the-art computer vision machine learning approaches, solar image segmentation can be advanced in a systematic and measurable fashion. 

\chapter{Data} \label{ch:data}

This chapter first describes the raw solar data utilized in this work. It then details the processing done to use the images in the project. Finally, it describes the annotation process for the images and analyzes the results. 

\section{Solar Imagery}
Ultraviolet observations of the Sun are optimal for seeing both dynamic, high temperature solar eruptions and cooler, more stable coronal holes. There are a few ultraviolet solar imagers: the Atmospheric Imaging Assembly (AIA) aboard the \textit{Solar Dynamics Observatory} (SDO) spacecraft \cite{lemen:2012}, the Extreme-ultraviolet Imaging telescope aboard the \textit{Solar and Heliospheric Observatory} (SOHO) spacecraft \cite{eit}, and the Sun Watcher with Active Pixels and Image Processing (SWAP) aboard the \textit{Project for On-Board Autonomy 2} spacecraft \cite{halain:2013, seaton:2013}. This project instead uses data from the new Solar Ultraviolet Imager (SUVI) aboard the \textit{ Geostationary Operational Environmental Satellite 16} (GOES-16) operated by the United States National Oceanic and Atmospheric Administration (NOAA) \cite{suvibg, suvibg2}. SDO, SOHO, and SUVI share many of the same wavelength passbands, but SUVI has a wider field of view allowing for classification of events further off the Sun. SWAP has the same field of view as SUVI but only one passband that SUVI also has. SUVI has a slightly higher spatial resolution than SWAP. In addition, GOES-16 is the first four commissioned satellites in the t-R series, all of which will have SUVI instruments. Thus, the classification system and database built here will be used for at least 20 years. The classification techniques applied here could be applied, with modification, to any of the aforementioned satellites, including to archived data. 

\subsection{Solar Ultraviolet Imager (SUVI) Data}
SUVI observes in six different wavelength passbands (and their corresponding coronal EUV emission line) using filters and multilayer mirrors sensitive to specfic wavelengths: 9.4 nm (Fe XVIII), 13.1 nm (Fe XXI), 17.1 nm (Fe IX/X), 19.5 nm, (Fe XII), 28.4 nm (Fe XV), and 30.4 nm (He II) Each wavelength is most sensitive to a specific wavelength and solar feature as detailed in Figure \ref{fig:suviwavelength}. This is qualitatively apparent in the first-light images from SUVi in Figure \ref{fig:suviimages}.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[scale=0.4]{suvi-wavelength-log.jpg}
    \caption{{\bf Wavelengths}: This diagram indicates which SUVI wavelengths are most helpful in identifying different space weather events \cite{suviwebsite}.}
    \label{fig:suviwavelength}
 \end{center}
\end{figure}


\begin{figure}[ht]
  \begin{center}
    \includegraphics[scale=0.8]{suvi-images.jpg}
    \caption{{\bf Example SUVI images}: These first-light images taken on January 29, 2017 exemplify SUVI's response to different solar features in different passbands. For example, the large coronal hole (the darker boot-like shape in the lower portion of the Sun) has higher contrast from the background in 19.5 nm than 304 nm \cite{suviwebsite}.}
    \label{fig:suviimages}
 \end{center}
\end{figure}


Each image is composed of 1280x1280 14-bit pixels. The individual pixels are 2.5x2.5 arcseconds on a side resulting in a 53.3 arcminute total field of view horizontally \cite{suvibg2}. The field of view is slightly larger along the diagonal but vignetting in some of the image corners in some passbands renders these regions unusable. Ultimately, SUVI can observe out to 1.6-2.3 solar radii (depending whether on the horizontal or diagonal). 

One image, in a single passband, is taken every 10 seconds. The observation sequence insures that every passband is observed at least once every four minutes. In addition, SUVI captures a short exposure time and a long exposure time image in each passband. During energetic and bright events like flares, the detector would saturate at the longer exposure time needed for high contrast images during times without energetic events. These two exposure times are combined to create high-dynamic-range composite images, which a two to three orders of magnitude increased dyanmic range than a single exposure time. In addition, SUVI has anti-blooming circuitry so that saturation in the long exposure image does not deteriorate quality in the composite image. This compositing will allow more classification of very bright active regions and flares than if another data set were used. Finally, the composite images are aligned so that the Sun is centered in each. 

\subsection{Noise-gater procedures}
The SUVI 94 angstroms channel is riddled with shot noise that degrades the signal quality. This buries the signal and could potentially make it more difficult for the machine learning classifiers to cleanly identify the solar classes. Deforest (2017) proposed a method using localized Fourier transforms to characterize and remove the noise from images with specific application to extreme ultraviolet images. The effectiveness of noise-gating is shown in Figure \ref{fig:noise-gate-example}.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[scale=0.8]{noise-gate.png}
    \caption{{\bf Effectiveness of noise-gating} The upper left is a good image, no cleaning necessary. However, images like the upper right, dominated by shot noise, are typical for the 94 angstrom channel. This image is created by taking the image on the upper left and adding Poisson noise with a signal-to-noise ratio of 2. The algorithm still performs even if it's worse, although some artifacts appear. DeForest's algorithm was applied to create the cleaned image on the bottom left. This can be compared to simply smoothing the image to decrease the noise as in the bottom right, a typical alternative procedure.}
    \label{fig:noise-gate-example}
 \end{center}
\end{figure}

The noise-gating algorithm was developed by \cite{deforest:2017} for use in extreme ultraviolet solar images, medical imaging, and other applications. The term noise-gating stems from the approach used in audio noise reduction and functions similarly: local adaptive filters in Fourier domain distinguish between coherent spatial structure and background noise. 

Beginning with an image sequence, described as a mapping from coordinates in the horizontal and vertical spatial dimensions and a time dimension, i.e. $\mathbb{Z}^3 \rightarrow \mathbb{R}$, the noise is modeled as:
\[Im(x,y,t) = Im_0(x,y,t) + N_a(x,y,t) + N_s(x,y,t) + N_{\text{other}}(x,y,t) \]
The observed image sequence $Im(x,y,t)$ is a combination of $Im_0$, the idealized, noise-free image; $N_a$, background noise that is independent of the signal of $Im_0$; $N_s$, shot noise that is dependent on the signal of $Im_0$; and $N_{\text{other}}$, all other noise sources. For this application, $N_a$ and $N_{\text{other}}$ are ignored because $N_s$ dominates them.  $N_s$ is then approximated with a normal distribution $G(x,y,t)$ and instrumental constant $\alpha$:
\[N_s(x,y,t) \approx \alpha G(x,y,t) \sqrt{\text{Im}_0(x,y,t)} \]
This is transformed into Fourier space into
\[|N_s'(k_x, k_y, \omega) \approx \beta(k_x, k_y, \omega) \sum_{x,y,t}\sqrt{\text{Im}(x,y,t)}\]
The image is broken into many small neighborhoods $\text{Im}_i (x,y,t)$. Since the image is dominated by shot noise, which should be fairly uniform in Fourier space, the noise distribution is approximated by: \[\beta_{\text{approx}}(k_x,k_y,\omega) = \text{median}_i \left(\frac{|\text{Im}_i'(k_x,k_y,\omega)|}{\overline{\text{Im}_i}} \right)\]. Then, the threshold to filter out noise is set for each image section independently by:
\[T_i(k_x, k_y, \omega) = 3 N_i'(k_x, k_y, \omega)\] with a gate filter:
\[F_{i, \text{gate}}' (k_x, k_y, \omega) = \begin{cases} 0 & \text{if Im}_i'(k_x, k_y, \omega) < T_i(k_x, k_y, \omega) \\ 1 & \text{otherwise} \end{cases}\]. 
This results in a cleaned image $Im_i'(k_x, k_y, k_\omega) F_i'(k_x, k_y, k_z)$ which can be mapped back into image space. The Fourier space is apodized by a Hanning windowo to avoid any vignetting. The full details can be found in the original paper \cite{deforest:2017}. 

\subsection{H-alpha imagery} 
As discussed in Section \ref{sec:halphainclusion}, it was impossible to differentiate between all labeled classes using only ultraviolet imagery. Thus, \halpha images were included. These images were gathered using the Virtual Solar Observatory \cite{vso}, an online compilation of images of the Sun from various sources, using SunPy \cite{sunpy, sunpyweb}, a Python toolkit for solar physics. In particular, \halpha images from the Global Oscillation Network Group (GONG) were used since they have continuous time coverage because of their seven globally distributed observing sites in Learmonth, Australia; Udaipur, India; El Teide, Spain; Cerro Tololo, Chile; Tuscon, Arizona, United States; Big Bear, California, United States; and Mauna Loa, Hawaii, United States. Each image is 2048x2048 pixels with each pixel being one arcsecond on a side. These are rescaled to SUVI using a linear transformation that scales the 1 arcsecond per pixel \halpha down to SUVI's plate scale of 2.5 arcseconds per pixel. The image was shifted so that the Sun remained centered.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[scale=0.1]{example-halpha.jpg}
    \caption{{\bf Example \halpha image}: This image is a typical \halpha view of the Sun from the GONG network.}
    \label{fig:halphaimage}
 \end{center}
\end{figure}


\section{Labeled Imagery} \label{sec:labeling}
Since this study is using supervised machine learning, labeled images of the Sun are needed. There does not exist a verified sample of human labeled solar events for more than one category of event. The closest system is the Heliophysics Event Knowledgebase (HEK) \cite{hurlburt:2012} which combines data mining and computer vision with data visualization techniques to create a database of labeled events. These are sometimes verified by humans but can be problematic, especially with multiple entries per event to sift through. Each event is given one of their designated labels as shown in Table \ref{tab:heklabels}.  

\begin{table}[ht!]
  \centering
  \begin{tabular}{|p{4cm} p{10cm}|}
    \hline
    Event Class & Description \\ \hline
    Active Region & Solar Active Region\\
    Coronal Mass Ejection & Ejection of material from the solar corona \\
    Coronal Dimming & A large-scale reduction in EUV emission \\
    Coronal Jet & A jet-like object observed in the low corona \\
    Coronal Wave & EIT or Morton waves spanning a large fraction of the solar disk \\
    Emerging Flux & Regions of new magnetic flux in the solar photosphere \\
    Filament & Solar Filament or Prominence \\
    Filament Eruption & A sudden launching of a filament into the corona  \\
    Filament Activation & A sudden change in a filament without launching \\
    Flare & Solar Flare \\
    Loop & Magnetic loops typically traced out using coronal imagery \\
    Oscillation &  A region with oscillating coronal field lines \\
    Sigmoid & S-shaped regions seen in soft X rays; indicator for flares \\
    Spray & Surge Sudden or sustained intrusion of chromospheric material well into the corona \\
    Sunspot & Sunspots on the solar disk \\
    Plage &  Bright areas associated with active regions\\
    Other & Something that could not be classified – good candidate for further research \\
    Nothing & Reported Used to indicate that the particular data were examined, but had nothing noteworthy to the observer \\
    \hline             
  \end{tabular}
  \caption{List of event labels for HEK \cite{hurlburt:2012}.}
  \label{tab:heklabels}
\end{table}

For this project, a small curated set of solar events was produced using three solar physics experts. They used an abbreviated set of solar event categories as detailed in Table \ref{tab:mylabels}.  

\begin{table}[ht!]
  \centering
  \begin{tabular}{|p{4cm} p{10cm}|}
    \hline
    Event Class & Description \\ \hline
    Bright Region & Solar Active Region\\
    Coronal Hole & Dimmer region in EUV where magnetic field lines are open\\
    Filament & Solar Filament \\
    Flare & Solar Flare \\
    Prominence & Solar prominence \\
    Limb & Edge of solar disk in EUV \\
    Structured outer space & region off the disk with structure \\
    Unstructured outer space & region off the disk with no structure \\
    Unlabeled & region where no label was given or confidence was especially low \\
    Quiet Sun & region on disk with no particularly interesting structures \\
    \hline             
  \end{tabular}
  \caption{List of event labels for curated data gathered in this study.}
  \label{tab:mylabels}
\end{table}

Twenty-seven image groupings of the SUVI six-band imagery were used for the labeled data as shown in Table \ref{tab:labelingtimes}. Each group consists of one image from each SUVI band, an \halpha image, and any needed derived images used as features in the machine learning. These images were selected because they were spread over the entirety of SUVI's operational lifetime and capture a variety of solar phenomena. 

\begin{table}[ht!]
  \centering
  \begin{tabular}{|c c c c|}
    \hline
    Group Number & Date & Time & Span of time (seconds) \\
    \hline
    0 & 2017-04-01 & 00:02:19 & 123 \\
    1 & 2017-04-15 & 00:01:49 & 150 \\
    2 & 2017-05-15 & 00:01:20 & 220 \\
    3 & 2017-05-20 & 00:02:07 & 121 \\
    4 & 2017-06-01 & 00:03:09 & 210 \\
    5 & 2017-06-15 & 00:02:57 & 120 \\
    6 & 2017-06-19 & 06:02:09 & 220 \\
    7 & 2017-07-01 & 00:02:18 & 120 \\
    8 & 2017-07-15 & 00:02:07 & 120 \\
    9 & 2017-07-28 & 05:02:17 & 122 \\
    10 & 2017-08-01 & 00:02:36 & 120 \\
    11 & 2017-08-20 & 00:01:07 & 160 \\
    12 & 2017-09-01 & 00:01:59 & 150 \\
    13 & 2017-09-08 & 00:01:58 & 180 \\
    14 & 2017-09-15 & 00:02:31 & 130 \\
    15 & 2017-10-01 & 12:01:56 & 150 \\
    16 & 2017-10-15 & 00:02:02 & 200 \\
    17 & 2017-11-02 & 00:02:07 & 210 \\
    18 & 2017-11-15 & 00:01:47 & 190 \\
    19 & 2017-11-30 & 00:01:51 & 150 \\
    20 & 2017-12-15 & 00:02:31 & 220 \\
    21 & 2018-01-01 & 00:03:04 & 210 \\
    22 & 2018-01-15 & 00:02:57 & 130 \\
    23 & 2018-02-01 & 00:02:54 & 200 \\
    24 & 2018-02-15 & 00:01:35 & 180 \\
    25 & 2018-03-01 & 00:01:57 & 220 \\
    26 & 2018-03-03 & 00:02:15 & 200 \\
    \hline
  \end{tabular}
  \caption{Times for images used in the labeling image set with the number of seconds between the first and last image in the grouping. }
  \label{tab:labelingtimes}
\end{table}

Four annotators were asked however only one (Annotator B) provided a complete annotation of every image. One (Annotator A) provided a nearly complete sample. Another (Annotator C) provided only an image. The last withdrew from the project. 

\subsection{Annotating Software}
In general, a solar annotation tool requires the following features:
\begin{itemize}
\item automatic solar coordinate alignment 
\item an interface to select a labeling class
\item utility to indicate which pixels should be labeled
\item ability to edit already annotated images
\item navigation tools to move throughout the image
\end{itemize}

To aid annotators, the author created a very simple annotation tool in Tkinter that met these requirements and more. This tool is very much a work in progress and has iteratively been improved based on user feedback. There are currently plans to move the annotation tool into a web interface to avoid a cumbersome install process. (This is a response to users having problems installing necessary dependencies because of the wide variety of machine operating systems and environments.) 

A user of the tool must first configure a reference database. A reference database is an HDF5 file that coordinates the image paths and groups. This database contains meta-data about when the database was compiled, what channels it includes, and the master path to the images. Its main purpose is to coordinate image groupings. For every image grouping, there is an entry which contains a name and paths to each passband image. This structuring allows users to classify based on a shared data repository instead of having individual copies. The database can automatically be generated from a directory containing SUVI images with a provided script. In the case of this thesis, the reference database and passband images were compiled and given to the users to minimize their work. 

With this database set up, the annotator calls the software from the commandline by specifying where the reference database is located and where to save labeled images. To make this easier, the users were given a parameterless version of this call which was configured to run based on the zipped reference database and images given to them. 

Upon startup, the software will look at its configuration file and determine how many times each image should be classified. As discussed in Section \ref{sec:consistency}, each annotator was asked to label every group twice to determine how consistent they were. If there are still images left to label, the software loads one using the reference database. Using meta-data in the images, it solves for where the Sun should be located in each image. It is configured with an option to automatically classify the entire disk of the Sun as quiet Sun, the limb of the Sun, and everything else as unstructured outer space. The location of these classes is known and do not need classification. The annotator can then draw on top of these labels by dragging their mouse. A lasso tool is used to determine the region enclosed by their drawing and given their selected label.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[scale=0.25]{labeling-initial}
    \caption{{\bf Startup of annotation software:} The region on the left shows a preview image of the Sun. Initially, it is configured to show a three-color composite of the Sun, with channels scaled to show most features. The right region shows the labeling of the Sun.}
    \label{fig:label-initial}
 \end{center}
\end{figure}

In a single static image, it is difficult to label all the features because they are on different spatial, brightness, and spectral scales. Pan and zoom tools were included to allow the user to easily navigate about an image. In the configure panel, the annotator can adjust the left, preview solar image. They can choose to either see an RGB image that combines three different passbands or look at a single passband in a greyscale image. If they select the RGB they can assign the colors to their desired passband as well as what scaling factor to use. The scaling factor is included because the brightness values between different passbands differ. Without the adjustable scaling factor, the brightest passband will always dominate and more subtle detail cannot be seen. A single color option is included because some features may only be definitively visible in one passband, for example filaments in \halpha. 


\subsubsection{Iterated improvements}
While this software is not complicated, it was not the first approach. In the very first approach, the image was automatically segmented into super-pixels using the Simple Linear Iterative Clustering (SLIC) segmentation algorithm \cite{SLIC}. Drawing boundaries is more time consuming than simply clicking an region of the image. SLIC was used in the hopes of producing more labeled images. However, it systematically over/under segmented specific classes, making their boundaries consistently too large or small, and did not consistently segment filaments. It instead lumped the structure into a larger superpixel with other classifications. 

This approach would render the annotated database full of biases that would be difficult to correct. Effectively, the machine learning approaches used here would approximate the segmentation algorithm instead a gold-standard human definition. Thus, SLIC was abandoned and annotators denoted boundaries with a set of points that were individually clicked. Eventually, the current drawing approach was reached. It most effectively simulates the act of hand-drawing labeled maps by space weather forecasters used at NOAA today. 

\subsection{Analysis of labeled data}
Since this is the first digitally collected dataset of human solar annotations for multiple classes, it brings the opportunity to establish baseline metrics for human performance on the task of solar image segmentation. 

\subsubsection{Self-consistency of annotators} \label{sec:consistency}

Each annotator was asked to label each image group twice. They were not informed when they were relabeling but instead shown the images in a random order. This allows for a check on human self-consistency. If the task were trivial, then the annotators would be expected to have perfect consistency. However, this is not the case. 

\begin{figure}[ht]
  \begin{center}
    \includegraphics[scale=0.1]{g20171015000202-0}
    \includegraphics[scale=0.35]{g20171015000202}
    \includegraphics[scale=0.1]{g20171015000202-1}
    \caption{{\bf Mistake for annotator} Left and right are annotations for Group 16, at center, by Annotator A. As you can see the annotator appears to have forgotten to label bright regions in one case. Such errors could be identified manually and cleaned from the labeled set. } 
    \label{fig:annotatorconsistency}
 \end{center}
\end{figure}

\begin{figure}[ht]
  \begin{center}
    \includegraphics[scale=0.1]{g20171102000206-0}
    \includegraphics[scale=0.35]{g20171102000206}
    \includegraphics[scale=0.1]{g20171102000206-1}
    \caption{{\bf Typical inconsistency for annotator} Left and right are annotations for Group 17, at center, by Annotator A. In this case, the annotator makes a typical kind of disagreement in the extent of boundaries. It is unclear which boundary is correct so both must be included in the labeled set.} 
    \label{fig:annotatorconsistency2}
 \end{center}
\end{figure}

The agreement can be quantified using Cohen's kappa or Fleiss's kappa \cite{landiskoch}. Cohen's kappa can be used when there are two raters or in this case two annotations of one solar image group by the same annotator. Fleiss's kappa is used when more than two raters exist or the raters are not deliberately chosen but selected from a population at random. 

Cohen's kappa is defined as 
\[\kappa = \frac{p_o - p_e}{1-p_e} \] where $p_o$ is the relative agreement among raters and $p_e$ is the hypothetical chance agreement. $p_o$ is thus analogous to accuracy and in this case is interpreted as the number of pixels the annotator labeled the same between sessions. $p_e$ provides context for $p_o$ is calculated in the binary label class as the sum of the random chance of labeling true $p_t$ and the random chance of labeling false $p_f$. In turn, $p_t$ is the percentage of pixels labeled with a given class out of the total for each trial and $p_f$ is similarly defined. With more categories, it is the sum of chance of labeling a category as something other than agreed between sessions. Cohen's kappa can be interpreted as shown in Table \ref{tab:cohens} \cite{cohens}.

\begin{table}[ht!]
  \centering
  \begin{tabular}{|c c c|}
  \hline    
$\kappa$ range &  Agreement interpretation & \% of data that are reliable \\
\hline \hline
0-.20	& None &	0-4\% \\
.21–.39	& Minimal &	4-15\% \\
.40–.59 & Weak & 15-35\% \\
.60–.79	& Moderate & 35-63\% \\
.80–.90 & Strong &64-81\% \\
$>.90$  & Almost Perfect	& 82-100\% \\
\hline
  \end{tabular}
  \caption{Interpretation of Cohen's kappa coefficient for agreement \cite{cohens}}
  \label{tab:cohens}
\end{table}

Fleiss's kappa is defined as \[ \kappa = \frac{\overline{P} -    \overline{P_e}}{1-\overline{P_e}} \] where $\overline{P} - \overline{P_e}$ quantifies actual observed agreement above chance and $1 - \overline{P_e}$ quantifies attainable agreement above chance. These terms are calculated where $N$ is the number of pixels, $n$ is the number of ratings per pixel, and $k$ is the number of classes each pixel can be assigned. Each pixel is index $i = 1, 2, ..., N$ and each class is indexed $j = 1, 2, ..., k$. Then, $n_{ij}$ is the number of annotators who labeled the $i$-th pixel with class $j$. 

\begin{align*}
p_j &= \frac{1}{Nn} \sum_{i=1}^{N} n_{ij} \\
P_i &= \frac{1}{n(n-1)} \left(\sum_{j=1}^{k} n_{ij}^2 - n_{ij} \right) \\
\overline{P} &= \frac{1}{N} \sum_{i=1}^N P_i \\
\overline{P_e} &= \sum_{j=1}^k p_j^2
\end{align*}

Landis and Koch (1977) proposed the scale in Table \ref{tab:fleiss} for Fleiss's kappa \cite{landiskoch}. 

\begin{table}[ht!]
  \centering
  \begin{tabular}{|c c|}
  \hline    
$\kappa$ range &  Interpretation \\
\hline \hline
$<$ 0 & No agreement \\ 
0.0 - 0.19 & Poor agreement \\ 
0.20 - 0.39 & Fair agreement \\
0.40 - 0.59 & Moderate agreement \\
0.60 - 0.79 & Substantial agreement \\ 
0.80 - 1.00 & Almost perfect agreement \\
\hline
  \end{tabular}
  \caption{Interpretation of Fleiss's kappa coefficient for agreement}
  \label{tab:fleiss}
\end{table}

Since each annotator labeled twice, Cohen's kappa was used here as shown in Table \ref{tab:annotatorconsistency}. 

\begin{table}[ht!]
\centering
 \begin{tabular}{|c |c c c c c c c c c c | c|} 
 \hline
\textbf{A} & BR & CH & EOS & FIL & FLA & limb & PR & QS & SOS & UL & Sum \\ \hline
BR & 115923 & 0 & 4051 & 550 & 351 & 2363 & 1051 & 25379 & 0 & 0 & 149668\\
CH & 25 & 138006 & 222 & 4526 & 0 & 12361 & 0 & 44810 & 0 & 0 & 199950\\
EOS & 5758 & 11 & 6829470 & 0 & 0 & 0 & 2369 & 0 & 0 & 0 & 6837608\\
FIL & 815 & 6400 & 183 & 8415 & 0 & 548 & 1412 & 4105 & 0 & 0 & 21878\\
FLA & 567 & 0 & 0 & 0 & 231 & 19 & 0 & 0 & 0 & 0 & 817\\
limb & 3585 & 6258 & 0 & 100 & 0 & 229637 & 2441 & 334 & 0 & 0 & 242355\\
PR & 98 & 0 & 6627 & 0 & 0 & 6311 & 13417 & 4 & 0 & 0 & 26457\\
QS & 58507 & 47494 & 0 & 9850 & 14 & 0 & 0 & 2229489 & 0 & 0 & 2345354\\
SOS & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
UL & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 6312 & 0 & 0 & 6313\\ \hline
Sum & 185279 & 198169 & 6840553 & 23441 & 596 & 251239 & 20690 & 2310433 & 0 & 0 & \\
 \hline
 \end{tabular}
 \caption{Annotator A's consistency: This table details Annotator A's labelings for each class. The rows indicate one session and the columns another. Thus, column CH and row FIL with a value of 6400 indicate that in one session Annotator A labeled 6400 pixels as coronal hole and in the other session as filament. The classes have been abbreviated as follows: bright region is BR, coronal hole is CH, empty outer space is EOS, filament is FIL, flare is FLA, limb is limb, prominence is PR, quiet Sun is QS, structured outer space is SOS, and unlabeled is UL. As seen here, Annotator A did not use the structured outer space label and did leave some pixels unlabeled.}
 \label{tab:annotatora}
\end{table}

\begin{table}[ht!]
\centering
 \begin{tabular}{|c |c c c c c c c c c c | c|} 
 \hline
\textbf{B} & BR & CH & EOS & FIL & FLA & limb & PR & QS & SOS & UL & Sum \\ \hline
BR & 80107 & 605 & 3094 & 280 & 94 & 2474 & 141 & 41576 & 0 & 0 & 128371\\
CH & 227 & 319404 & 2613 & 0 & 0 & 19069 & 0 & 58172 & 0 & 0 & 399485\\
EOS & 1954 & 447 & 10256024 & 0 & 0 & 0 & 8498 & 0 & 150 & 0 & 10267073\\
FIL & 3101 & 0 & 0 & 12366 & 0 & 8 & 186 & 13238 & 0 & 0 & 28899\\
FLA & 570 & 0 & 0 & 0 & 573 & 0 & 0 & 31 & 0 & 0 & 1174\\
limb & 1012 & 15889 & 0 & 0 & 0 & 331215 & 10019 & 0 & 0 & 0 & 358135\\
PR & 0 & 92 & 3677 & 0 & 0 & 5229 & 29701 & 273 & 0 & 0 & 38972\\
QS & 30045 & 73408 & 0 & 7160 & 0 & 0 & 518 & 3411629 & 0 & 0 & 3522760\\
SOS & 0 & 0 & 141 & 0 & 0 & 0 & 0 & 0 & 590 & 0 & 731\\
UL & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
Sum & 117016 & 409845 & 10265549 & 19806 & 667 & 357995 & 49063 & 3524919 & 740 & 0 &\\
 \hline
 \end{tabular}
 \caption{Annotator B's consistency: This table details Annotator B's labelings in the same format as Table \ref{tab:annotatora}.}
 \label{tab:annotatorb}
\end{table}

\begin{table}[ht!]
\centering
 \begin{tabular}{|c c c|} 
 \hline
 Class & Annotator A & Annotator B \\
   \hline\hline
Empty outer space & 0.996 & 0.997 \\
Structure outer space & N/A & 0.802 \\
Bright region & 0.687 & 0.650 \\
Filament & 0.370 & 0.507 \\
Prominence & 0.568 & 0.674 \\
Coronal hole & 0.687 & 0.783 \\ 
Quiet Sun & 0.945 & 0.958 \\
Limb & 0.929 & 0.923 \\
Flare & 0.327 & 0.622 \\ \hline
Overall & 0.941 &  0.956\\
 \hline
 \end{tabular}
 \caption{Annotator consistency: For each annotator and image, Cohen's kappa for each class is shown. Annotator B is more consistent for each class. The overall agreement is skewed by the large number of outer space pixels. Annotator A has no agreement score for structured outer space because the label was disregarded.}
 \label{tab:annotatorconsistency}
\end{table}

From Table \ref{tab:annotatorconsistency}, it is clear that labeling filaments is very difficult. Oddly, flares, which were anticipated to be simple were inconsistent as well. However, by looking at Tables \ref{tab:annotatora} and \ref{tab:annotatorb} it's clear that the inconsistency is not in  identifying them but drawing the appropriate boundary. Since there are so few pixels, bleeding over into the surrounding bright region or being too limited in the boundary has a huge skew. Classes that are fairly automatic like empty outer space and limb, where suggestion is provided in the annotating software, have high agreement. 

\subsubsection{Agreement among annotators} \label{sec:agreement}
Further, there is no universally agreed upon definition for each labeling class. Some cases are obvious, for example outer space. However, there are times where it is unclear which label to apply, for example a filament versus a coronal hole, or where exactly the boundary of the label should be drawn, for example which portions consist of active regions. The labeled database gives data-driven insight on where solar physics experts disagree. The disagreement may be amplified if experts from different research institutions or very different research focus are selected. 

In this labeling, Annotator A did not use the structured outer space class. However, it is a rare class, and they chose to skip the image group that Annotator B labeled substantial structured outer space. 
\begin{figure}[ht]
  \begin{center}
    \includegraphics[scale=0.35]{g20170619060208}
    \includegraphics[scale=0.1]{g20170619060208-0b}
    \caption{{\bf Structured outer space discrepancy} At left is the 30.4 nm image for Group 6. Very faintly in the upper left, some structure is visible in outer space as a CME moves outward. This is only seen in the 30.4 passband. Thus, if Annotator A had chosen to label this image they may or may not have missed this class.} 
    \label{fig:sos}
 \end{center}
\end{figure}

A much more typical example of disagreement is shown in Figure \ref{fig:annotatordifference}. 
\begin{figure}[ht]
  \begin{center}
  	\includegraphics[scale=0.1]{g20170415000148-a}
    \includegraphics[scale=0.35]{g20170415000148}
    \includegraphics[scale=0.1]{g20170415000148-b}
    \caption{{\bf Inter-Annotator disagreement} Group 1 provides a key example for disagreement among annotators.Annotator A is shown at left while Annotator B is shown at right. In the center, is the three-color image of the Sun at that time. Annotator A identified smaller coronal holes than Annotator B and included a filament Annotator B did not. This filament is not seen in \halpha but does look rather strong in the EUV three-color. In general, they identifies the same features but with different boundaries.} 
    \label{fig:annotatordifference}
 \end{center}
\end{figure}

This disagreement can be quantified with Cohen's kappa in Table \ref{tab:annotatoragreement}. 
\begin{table}[ht!]
\centering
 \begin{tabular}{|c c|} 
 \hline
 Class & Inter-annotator agreement\\
   \hline\hline
Empty outer space & 0.992 \\
Structured outer space & 0.0 \\
Bright region & 0.471\\
Filament & 0.380 \\
Prominence & 0.432 \\
Coronal Hole & 0.703 \\
Quiet Sun & 0.935 \\
Limb & 0.902 \\
Flare & 0.366 \\
 \hline
 \end{tabular}
 \caption{Inter-annotator agreeement: For each class the agreement between annotators on what pixels are declared that class is shown. Annotator A did not use the structured outer space class. Coronal hole is really the only non-automatic class that is reliable.}
 \label{tab:annotatoragreement}
\end{table}

A third annotator submitted only a handful of images, however they provide an interesting case study in Section \ref{sec:casestudy}.

\subsection{Case Study} \label{sec:casestudy}
    
\begin{figure}[ht]
  \begin{center}
  	\includegraphics[scale=0.4]{casestudycomparison}
    \caption{{\bf An interesting case study from Group 13} For this group, Annotator A and Annotator B submitted double labeling and Annotator C also submitted a labeling. This group comes from 2017-09-08 during a solar flare. All labelings include the annotated flare in the bottom right of the image. However, there are many other differences discussed in Section \ref{sec:casestudy}.}
    \label{fig:annotatordifference}
 \end{center}
\end{figure}

Some of the annotator disagreement noted in Section \ref{sec:agreement} and Table \ref{tab:annotatoragreement} for bright regions stems from the size of bright regions labeled.     Annotator A tends to draw a larger boundary including the cascades of loops. Annotator B was a bit more conservative and only included the brighter regions of the cascades. Annotator C was the most conservative and only included the bright core. Along those same lines, Annotator C left a gap between the flare and surrounding bright region. The upper left bright regions were only sometimes separated even though a clear gap divides them. It appears Annotator C was also uncertain about the southernly coronal hole and chose to leave it unlabeled. However, they did identify another coronal hole that was not labeled by any others. The southern filament was only identified half the time. Annotators A and C agreed that the upper left dark region was a coronal hole while Annotator B decided it was filament. Upon further examination there is no filament visible in \halpha here so it likely is coronal hole. This may be why Annotator B also left it as quiet Sun. Prominences were very inconsistently labeled. This likely stems from them only being visible in 30.4 nm with special attention. Thus, a quick labeling may overlook some. Annotator A identified a spicule as a prominence in trial one. This potentially argues for another category for spicules or other "extra" features. 

Overall, there are a few types of disagreements:
\begin{itemize}
\item Uncertainty in data for labeling: coronal holes and filaments are often unclear. 
\item Boundary: It is unclear where the edge of a feature should be drawn and this creates disagreement. 
\item Omission: Sometimes, one annotator completely skips a clearly identifiable feature another annotator sees or even worse completely skips labeling a class by mistake. 
\end{itemize}

\subsection{Labeled database}
Ideally, the labeled database would be compiled with input from all annotators. However, only Annotators A and B consistently submitted labeled images. Therefore, there is no third vote to break ties when they have disagreements. Picking a label at random for each pixel when there is a disagreement would result in a very pixelated image, for example when it is unclear whether a feature is a filament or coronal hole the feature would be a mess of pixels switching between coronal hole and filament with quiet Sun on boundary disagreements. This would create a poor gold standard database.

Instead, when there are multiple annotators or trials for an image, one is selected at random as the gold standard. This incorporates definitions from all annotators and should spread out and hopefully eliminate systematic biases by preferencing one annotator's definition. 


\chapter{Classifiers} \label{ch:classifiers}

In best conditions, a deep learning approach such as a convolutional neural network would be used because in other computer vision applications they have greatly outperformed other machine learning techniques. However, they require large amounts of training data. Therefore, this chapter introduces the machine learning techniques used. Instead of treating the images as a collection of individual pixels as described in the related work (Chapter \ref{ch:relatedwork}) or in a convolutional sense, these classifiers use neighborhoods of multichannel pixels. The neighborhood of neighborhood of $(x_0, y_0)$ is $\{(x_i, y_i) | x_0 - w \le x_i \le x_0 + w, y_0 - h \le y_i \le y_0 + h\}$  where $w$ and $h$ are the number of pixels on either side of the reference pixel $(x_0, y_0)$ used in the horizontal and vertical dimension respectively. Each multichannel pixel is a $m \times 1$-dimension column vector where each entry corresponds to a spectral passband or derived channel. The derived channels are ratios of spectral passbands or spatial information but could be extended to any type of feature. The goal is to assign each multichannel pixel a label from Table \ref{tab:mylabels}. 

The implementations of the 
\section{Naive Bayesian Maximum Likelihood}

This approach mimics the maximum likelihood estimation of \cite{rigler:2012} described in Section \ref{sec:riglermaxlikelihood}. It is modified for pixel neighborhoods. If $w=h=0$, this model is identical to \cite{rigler:2012}. 

Like before, every pixel is classfied into exactly one class using multispectral ultraviolet images, \halpha, and other derived channels as features. A pixel is a member of a pixel neighborhood that is used in classifying. Therefore, a pixel at $(i,j)$ is the reference pixel to the neighborhood $\{(i, j) | x_0 - j \le x_i \le x_0 + j, y_0 - k \le y_i \le y_0 + k\}$ where each neighborhood pixel has $c$ channels and can be described as a vector:
\[x_{(i,j)} = \begin{bmatrix} x_1 & x_2 & \hdots & x_c \end{bmatrix}^T \]
Thus, the neighborhood of pixels is generalized as a $2w + 1 \times 2h + 1 \times c$ dimensional matrix, the algamation of all neighborhood pixels. 
For each $x_{(i,j)}$ pixel a label $w_k$ from the set of classes $W$. This approach employs Bayes' Theorem:
\[ P(w_k | x_{(i,j)}) = \frac{P(x_{(i,j)} | w_k) P(w_k)}{P(x_{(i,j)})} \]
Since $P(x_{(i,j)})$ is not a function of the label classification, it can be ignored.
\[ P(w_j | x_{(i,j)}) \propto P(x_{(i,j)}) P(w_{k}) \].
Following \cite{rigler:2012}: 
\[ P(w_k | x_{(i,j)}) \propto P(x_{(i,j)} | w_k) \]
This approach is the maximum likelihood solution to this problem.

Each class is modeled as a multivariate normal distribution, i.e. for each potential label there is a archetypal example pixel and all pixels with that label should be distributed normally about it. This is overly constraining if any given class has multiple distinct modes with respect to the selected data. However, this can be solved adding more subclasses for feathres. Alternatively, this section could be replaced with a Gaussian mixture approach. The multivariate distribution for class $w_k$ is characterized by a mean vector $\mu_k$ and covariance matrix $C_k$ which are calculated as:
\[ \mu_k = \frac{\sum_{x \in W_k} x}{|W_k|} \]
where $W_k$ is the collection of pixel neighborhoods where the reference pixel has label $w_k$. Similarly,
\[ C_k = \frac{\sum_{x \in W_k} \begin{bmatrix} x - \mu_k \end{bmatrix} \times \begin{bmatrix} x - \mu_k \end{bmatrix}^T}{|W_k|} \].
These mean vectors and covariance matrices characterize the class. Given this characterization for class $w_k$ one can calculate the conditional probability of a pixel $x_{(i,j)}$ having label $w_k$:
\[ P(x_{(i,j)} | w_k) = \frac{1}{\sqrt{(2 \pi)^h} \sqrt{|C_k|}} \mathrm{exp} \left( \frac{-1}{2} \times \left(x_{(i,j)} - \mu_k \right)^T \times C_k^{-1} \times \left(x_{(i,j)} - \mu_k \right) \right) \]
Thus, the label that maximizes this probability is assigned for the reference pixel.

No smoothing is done in this version as in Rigler et al. (2012) \cite{rigler:2012}. 

\section{Random Forest}
The random forest is an ensemble of decision tree classifiers and are used in a variety of Earth remote sensing applications \cite{puissant2014object, salas2016multispectral, clark2016mapping, kulkarni2017multispectral, lowe:2015}. Solar image segmentation is very similar to remote Earth sensing, just with a different subject matter and often fewer passbands. In this application, an ensemble of $D$ decision trees with maximum depth $Q$ are used to form the random forest \todo{change to optimal}. The empircally best depth and number is determined in Section \ref{sec:randomfresthyperparameter}. The random forest is given the same types of input as the maximum likelihood classifier: the EUV channels, the \halpha channel, and derived channels (spatial, line ratios). The random forest implementation in Scikit-Learn is used \cite{scikit-learn}. 

\begin{figure}[ht]
  \begin{center}
    \includegraphics[scale=0.8]{rand-forest.jpg}
    \caption{{\bf An example random forest construction}: This helpful graphic shows the general layout of how a random forest works. Multiple decision trees are evaluated to determine a majority class vote \cite{randforestgraphic}}
 \end{center}
\end{figure}

Another benefit of the random forest is its ability to indicate what features are most important for classification. This is briefly explored in Section \ref{sec:rforestfeatures}. 

\section{Feed-forward Neural Network}
A simple fully-connected feed forward network is employed here consisting of three densely connected hidden layers with an input layer with similar neighborhood inputs and an output layer for classes. The hidden layers had 16, 16, and 8 nodes \todo{change to optimal} each with relu activation. They were trained for \todo{this many} epochs with a batch size of \todo{this many}. 

\section{Convolutional Neural Network}
A convolutional neural network was attempted with the expectation of poor results because of insufficient training. The system was based on the Fully Convolutional Neural Network appraoch by Long, Shelhamer, \& Darrell (2015) \cite{fcnn}, a method to take arbitrarily sized image input and create similarly sized image output with classification labels. The specific implementation used was a modification of Sarath Shekkizar's Tensorflow implementation \cite{fcnntensorflow}. This approach was intialized with VGG-16 pretrained model, although it's unclear if this training helps for solar image segmentation as well. 

\chapter{Experiments and Results} \label{ch:experiments}

This chapter describes various experiments performed on the data and the conclusions derived while working toward the optimal machine learning solar image classifier, most importantly it explores feature selection. It begins by introducing the methods of evaluation in Section \ref{sec:evaluationmethods} and then proceeds through the experiments. 

\section{Evaluation approaches}\label{sec:evaluationmethods}
The database of annotated images was broken into two categories: a training set and a testing set. Fourteen images were used in training and Thirteen were used for testing. These were randomly selected so that no observational time was biased. The classifiers in the experiments are trained solely on the images in the training set and only exposed to the testing set for evaluation. 

\subsection{Qualitative}
For each experiment, the generated thematic maps from the trained classifiers are qualitatively compared to the testing images. Several qualities are inspected:
\begin{itemize}
\item Are all classes expected in this image present? If not, what classes are missing or overly present?
\item How do the boundaries of each region compare to the testing image? 
\item Is the classification noisy with neighboring pixels varying? 
\item Is the limb correctly aligned? 
\item Are there any other pecuiliarities? 
\end{itemize}

\subsection{Standard machine learning metrics}
In addition to qualitative inspection, the testing set and classifier generated results are compared quantitatively. 

\subsection{Confusion matrix}
\begin{figure}[ht]
  \begin{center}
    includegraphics[scale=0.8]{placeholder.jpg}
    \caption{{\bf An example confusion matrix}: A confusion matrix is a method of showing accuracy of classifying each label. On the vertical axis the correct label is shown while on the horizontal axis the annotator's label is shown \todo{check that this is correct orientation}. Then, looking at the cell defined shows the percentage of labels in that region. Ideally, the diagonal should be 1 indicating every feature was labeled with the correct label always. }
    \label{fig:exampleconfusionmatrix}
 \end{center}
\end{figure}


\section{Baseline results}

\section{Normalization of data}
\todo{copy from notes}

\section{Spatial features}
\todo{copy from notes}

\section{\halpha inclusion}\label{sec:halphainclusion}
As seen in Figure \ref{fig:halphamotivation}, filaments and coronal holes cannot be differentiated using spectral information in the EUV channels of SUVI. Spatial approaches may discern between them because filaments are long, skinny strands while coronal holes are typically more round and extended. Alternatively, \halpha spectral information easily differentiates between coronal holes and filaments as shown in Figure \ref{fig:halphause}. 

\begin{figure}[ht]
  \begin{center}
    \includegraphics[scale=0.8]{placeholder.jpg}
    \caption{{\bf Degeneracy of filament and coronal hole spectrally with EUV images} This histogram compares 19.5 nm pixel values for coronal holes, top, and filaments, bottom. They nearly overlap completely making it difficult for classification methods to key on sufficient themes for classification. All other EUV channels suffered from complete overlap. \halpha images differentiate filaments and coronal holes as shown in Figure \ref{fig:halphause}.} 
    \label{fig:halphamotivation}
 \end{center}
\end{figure}

\begin{figure}[ht]
  \begin{center}
    \includegraphics[scale=0.8]{placeholder.jpg}
    \caption{{\bf Coronal hole and filament in \halpha} In \halpha images, filaments are dark strand structures while coronal holes have no apparent signature. This defining difference complements the degneracy in EUV images and boosts classifier performance.}
    \label{fig:halphause}
 \end{center}
\end{figure}

In this experiment, the suite of classifiers are first trained with only EUV channels. Then, they are trained with both EUV and \halpha channels. As shown in Figure \ref{fig:halpharesult} the \todo{insert results discussion here}. 

\begin{figure}[ht]
  \begin{center}
    \includegraphics[scale=0.8]{placeholder.jpg}
    \caption{{\bf \halpha inclusion experiment results}. \todo{add relevant caption based on results}}
    \label{fig:halpharesult}
 \end{center}
\end{figure}

\section{Hyperparameter selection}

\subsection{Random forest} \label{sec:randomforesthyperparameter}

\subsection{Neural network} \label{sec:neuralnetworkhyperparameter}

\section{Random forest feature importance} \label{sec:rforestfeatures}

\section{Comparison to existing classifiers}

\chapter{Applications} \label{ch:application}
Solar image segmentation is helpful for space weather forecasting but is not the end goal for research. Solar physicists can utilize it as a tool in research applications. To aid in finding relevant data, a database of observations with key statistical features can be compiled from thematic maps as described in \ref{sec:database}. Alternatively, the segmentation can be used directly as a boundary as illustrated in an exploration of fractal dimension and class properties in Section \ref{sec:fractal}. 

\section{Database building} \label{sec:database}
\todo{copy from notes}

\section{Fractal dimension and class properties} \label{sec:fractal}
\todo{copy from fractals paper}

\chapter{Conclusion} \label{ch:conclusion}

\section{Future work}
There are many avenues for future inquiry stemming from this project. A few are presented here. 

\subsection{Stability over solar cycles}
The Sun goes through 11-year cycles where behavior changes drastically. During the "solar minimum" phase, where the Sun is now, coronal holes are much more prominent and extend further down to the solar equator, there are fewer flares and bright regions. As solar maximum approaches the distribution and characteristics of the classes will change. This may require new training data. It also may require the addition of a solar cycle feature that links data to when in a solar cycle it was taken. One way to study this is by looking at satellite with a longer history, such as SDO, or waiting until SUVI has a long history of observations. 

\subsection{Converting digitized synoptic charts}
Since obtaining annotated images was among the most difficult portions of this project, it would be nice to utilize as much existing annotation as possible. Space weather forescasters at NOAA create thematic maps daily by hand and have done so since 1972. These are digitized into a PDF format. 

\begin{figure}[ht]
  \begin{center}
    \includegraphics[scale=0.8]{synoptic}
    \caption{{\bf Daily synoptic drawing} Here are four synoptic charts drawn at NOAA ranging from 1974 in the upper left to 2018 in the bottom right. Even more recent synoptic charts vary in style because different forecasters have slightly different styles.}
    \label{fig:synoptic}
 \end{center}
\end{figure}

Despite their accessibility, they are difficult to use as a training database and propose a project all to themself. Figure \ref{fig:synoptic} illustrates the cariety of drawing styles. While they are drawn on the same template (earlier templates are slightly different but anything within satellite observation interest would be consistent), the annotations vary and present a challenge for computer vision recognition. The color of red for prominences and flares can change. In the bottom right, the neutral magnetic lines do not extend to the solar limb, likely because of an uncertainty in their location, while in other figures they do. Coronal holes, indicated by bounaries with lines perpendicular to the boundary, are complicated since hatchings are drawn at variable widths and determining the interior of the the coronal hole relies upon that. Accompanying annotations are sometimes drawn next to features and other times drawn further away and linked by an arrow. Finally, these thematic maps do not detail features off the disk (except sometimes prominences) and do not include all the features (filaments) discussed in this thesis. If these challenges were overcome, the level of detail and care in the drawings makes them ideal for the labeled database. 

In the meantime, a more realistic goal is for forecasters to make a daily drawing using the annotation software. This is being pursued through a more accessible online tool with a funding proposal in the process of submission. 

\subsection{Prediction}
Potentially the most interesting and beneficial avenue for development is transitioning the classification paradigm to a prediction paradigm. 
\section{Summary}


%%%%%%%%References %%%%%%%%%%
\bibliographystyle{acm}
\bibliography{references}
%%%%%%%% End References %%%%%%
\end{document}
